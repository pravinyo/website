[ { "title": "Effortless Postgres-S3 Data Flow (Spring Boot)", "url": "/posts/effortless-postgres-s3-data-flow-spring-boot/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment", "date": "2025-04-23 00:00:00 +0530", "snippet": "OverviewThis guide demonstrates how to leverage PostgreSQL’s native AWS S3 integration to directly export and import table data between your database and Amazon S3 - all through SQL commands executed from a Spring Boot application.flowchart TD A[Spring Boot Application] --&gt;|JDBC| B[(PostgreSQL Database)] B --&gt;|aws_s3 Extension| C[(Amazon S3 Bucket)] C --&gt;|Imported Data| B B --&gt;|Exported Data| CPrerequisites1. Database Requirements PostgreSQL 12+ (Amazon RDS or self-managed) aws_s3 and aws_commons extensions installed Proper IAM permissions for S3 access2. Application Requirements Spring Boot 2.7+ Spring Data JDBC or JPA PostgreSQL JDBC driverSetup Instructions1. Enable Required PostgreSQL Extensions-- For self-managed PostgreSQL:CREATE EXTENSION IF NOT EXISTS aws_commons;CREATE EXTENSION IF NOT EXISTS aws_s3;-- Verify installationSELECT * FROM pg_available_extensions WHERE name LIKE 'aws%';2. Configure AWS CredentialsOption A: IAM Role (Recommended for RDS)For RDS PostgreSQL, attach an IAM role with these permissions:{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:GetObject\", \"s3:DeleteObject\" ], \"Resource\": \"arn:aws:s3:::your-bucket-name/*\" } ]}Option B: Access Keys (For non-RDS)SELECT aws_commons.set_aws_credentials( 'your-access-key-id', 'your-secret-access-key', 'us-east-1');sequenceDiagram Admin-&gt;&gt;PostgreSQL: CREATE EXTENSION aws_commons Admin-&gt;&gt;PostgreSQL: CREATE EXTENSION aws_s3 PostgreSQL--&gt;&gt;Admin: Extensions created Admin-&gt;&gt;PostgreSQL: Configure AWS credentials PostgreSQL--&gt;&gt;AWS: Validate permissions AWS--&gt;&gt;PostgreSQL: Access grantedExporting Data to S3SQL Command StructureSELECT aws_s3.query_export_to_s3( 'SELECT * FROM your_table', -- Query to export aws_commons.create_s3_uri( -- S3 destination 'your-bucket', 'path/to/file.csv', 'aws-region' ), options := 'format csv, header' -- Export options);Supported Export Formats Format Options Example Notes CSV 'format csv, header' Most common format Text 'format text' Tab-delimited Binary 'format binary' PostgreSQL binary format flowchart LR SB[Spring Boot] --&gt;|1. Execute SQL| PG[PostgreSQL] PG --&gt;|2. Generate CSV| PG PG --&gt;|3. Upload| S3[S3 Bucket] S3 --&gt;|4. Confirm| PG PG --&gt;|5. Return Status| SBImporting Data from S3SQL Command StructureSELECT aws_s3.table_import_from_s3( 'target_table', -- Target table name 'col1,col2,col3', -- Optional column list '(format csv, header)', -- Import options aws_commons.create_s3_uri( -- S3 source 'your-bucket', 'path/to/file.csv', 'aws-region' ));sequenceDiagram Spring Boot-&gt;&gt;PostgreSQL: Import request PostgreSQL-&gt;&gt;S3: Get file S3--&gt;&gt;PostgreSQL: File data PostgreSQL-&gt;&gt;PostgreSQL: Parse and insert PostgreSQL--&gt;&gt;Spring Boot: Import resultSpring Boot Implementation1. Database Configurationspring.datasource.url=jdbc:postgresql://localhost:5432/yourdbspring.datasource.username=userspring.datasource.password=passwordspring.datasource.driver-class-name=org.postgresql.Driverspring.datasource.hikari.maximum-pool-size=10spring.jpa.hibernate.ddl-auto=nonespring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect2. S3 Export/Import Repository and Service@Repositorypublic interface S3DataRepository extends Repository&lt;Object, Long&gt; { @Modifying @Query(value = \"\"\" SELECT aws_s3.query_export_to_s3( :query, aws_commons.create_s3_uri(:bucket, :key, :region), :options ) \"\"\", nativeQuery = true) void exportToS3( @Param(\"query\") String query, @Param(\"bucket\") String bucket, @Param(\"key\") String key, @Param(\"region\") String region, @Param(\"options\") String options ); @Modifying @Query(value = \"\"\" SELECT aws_s3.table_import_from_s3( :table, :columns, :options, aws_commons.create_s3_uri(:bucket, :key, :region) ) \"\"\", nativeQuery = true) void importFromS3( @Param(\"table\") String table, @Param(\"columns\") String columns, @Param(\"options\") String options, @Param(\"bucket\") String bucket, @Param(\"key\") String key, @Param(\"region\") String region );}@Service@RequiredArgsConstructorpublic class S3DataService { private final S3DataRepository s3DataRepository; @Transactional public void exportToS3(String query, String bucket, String key, String region, String formatOptions) { s3DataRepository.exportToS3(query, bucket, key, region, formatOptions); } @Transactional public void importFromS3(String table, String columns, String bucket, String key, String region, String formatOptions) { s3DataRepository.importFromS3(table, columns, formatOptions, bucket, key, region); }}3. REST Controller@RestController@RequestMapping(\"/api/data\")@RequiredArgsConstructorpublic class DataTransferController { private final S3DataService s3DataService; @PostMapping(\"/export\") public ResponseEntity&lt;String&gt; exportData( @RequestParam String query, @RequestParam String bucket, @RequestParam String key, @RequestParam(defaultValue = \"us-east-1\") String region, @RequestParam(defaultValue = \"format csv, header\") String options) { s3DataService.exportToS3(query, bucket, key, region, options); return ResponseEntity.ok(\"Data exported successfully to s3://%s/%s\".formatted(bucket, key)); } @PostMapping(\"/import\") public ResponseEntity&lt;String&gt; importData( @RequestParam String table, @RequestParam(required = false) String columns, @RequestParam String bucket, @RequestParam String key, @RequestParam(defaultValue = \"us-east-1\") String region, @RequestParam(defaultValue = \"(format csv, header)\") String options) { s3DataService.importFromS3(table, columns, bucket, key, region, options); return ResponseEntity.ok(\"Data imported successfully from s3://%s/%s\".formatted(bucket, key)); }}Advanced Usage1. Handling Large Exports-- Export in chunksSELECT aws_s3.query_export_to_s3( 'SELECT * FROM large_table WHERE id BETWEEN 1 AND 100000', aws_commons.create_s3_uri('bucket', 'chunk1.csv', 'region'), 'format csv');2. Custom Delimiters-- Using pipe delimiterSELECT aws_s3.query_export_to_s3( 'SELECT * FROM employees', aws_commons.create_s3_uri('bucket', 'file.psv', 'region'), 'format csv, delimiter ''|'', header');⚠️ Troubleshooting Issue Solution Permission denied Verify IAM role or access keys Extension not found Install aws_s3 extension Invalid S3 URI Check bucket/region spelling Malformed CSV Verify format options Performance Considerations Network Throughput: S3 transfers are network-bound File Size: Optimal chunk size 100MB-1GB Database Load: Schedule large exports during off-peak hoursSecurity Best Practices Use IAM roles instead of access keys when possible Restrict S3 bucket access with bucket policies Enable S3 server-side encryption Use VPC endpoints for private network accessAdditional Resources AWS Documentation on PostgreSQL S3 Extension PostgreSQL Foreign Data Wrappers" }, { "title": "Scalability for Dummies: A Comprehensive Guide to Building Scalable Web Services", "url": "/posts/scalabilities-for-dummies/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment", "date": "2025-03-04 00:00:00 +0530", "snippet": "Scalability is a critical aspect of modern web services, especially as user bases grow and traffic increases. In this blog series, we’ll break down the key concepts of scalability into digestible parts, starting with the basics and gradually diving into more advanced topics. Whether you’re a beginner or an experienced developer, this guide will help you understand how to design and implement scalable systems.Part 1: Clones - The Foundation of ScalabilityIntroduction to ScalabilityWhen building a web service, one of the first challenges you’ll face is handling increasing traffic. The key to scalability lies in distributing the load efficiently across multiple servers. In this section, we’ll explore how to achieve this using clones—identical copies of your application servers.graph TD A[User] --&gt; B[Load Balancer] B --&gt; C[Server 1] B --&gt; D[Server 2] B --&gt; E[Server 3] C --&gt; F[Centralized Data Store] D --&gt; F E --&gt; FLoad Balancers and Application ServersIn a scalable web service, public servers are hidden behind a load balancer. The load balancer evenly distributes incoming requests across a cluster of application servers. For example, if a user named Rakesh makes multiple requests, his first request might be handled by Server 2, the second by Server 9, and the third by Server 2 again.Key Point: Rakesh should always receive the same response, regardless of which server handles his request. This leads us to the first golden rule of scalability: Every server must contain the same codebase and must not store user-related data (like sessions or profile pictures) locally.Centralized Data StorageTo ensure consistency across servers, user-related data such as sessions should be stored in a centralized data store accessible to all application servers. This could be an external database or a persistent cache like Redis. Redis, being an in-memory data store, offers better performance compared to traditional databases.Why Centralized Storage? Prevents data inconsistency across servers. Ensures seamless user experience, even if requests are handled by different servers.Modern Deployment with CI/CDDeploying code changes across multiple servers can be tricky. How do you ensure that all servers are running the latest version of your code? This is where modern CI/CD pipelines come in. Tools like GitHub Actions, GitLab CI/CD, or ArgoCD automate the deployment process, ensuring that all servers are updated simultaneously. These tools integrate seamlessly with your version control system and provide powerful automation capabilities for testing, building, and deploying your code.Containerization and OrchestrationInstead of creating traditional server images (AMIs), modern scalable applications leverage containerization with Docker. Containers package your application and its dependencies into a standardized unit that can run consistently across different environments. For orchestrating these containers at scale, Kubernetes has become the industry standard, allowing you to deploy, scale, and manage containerized applications with ease.When you need to deploy a new instance, simply pull the latest container image from a registry like Docker Hub or Amazon ECR, and Kubernetes will handle the deployment across your cluster.Part 2: Database Scalability - Beyond ClonesThe Database BottleneckAfter implementing clones, your web service can handle thousands of concurrent requests. However, as traffic grows, you’ll likely encounter a new bottleneck: the database. Whether you’re using MySQL or another relational database, scaling your database is more complex than simply adding more servers.graph TD A[Database Scaling Approaches] A --&gt; B[SQL Solutions] A --&gt; C[NoSQL Solutions]Path #1: Scaling with Modern SQL SolutionsIf you choose to stick with SQL databases, you’ll need to implement strategies like: Master-Slave Replication: Write operations go to the master, while read operations are distributed across slaves. Sharding: Splitting your database into smaller, more manageable pieces. Denormalization: Reducing the number of joins by duplicating data.Modern solutions like Amazon Aurora, Vitess (used by YouTube and GitHub), or CockroachDB can help manage these complexities by providing built-in sharding, replication, and high availability features while maintaining SQL compatibility.Path #2: Embracing NoSQLA more radical approach is to switch to a NoSQL database like MongoDB, DynamoDB, or Cassandra. NoSQL databases are designed for scalability and can handle large datasets more efficiently. By eliminating joins and denormalizing your data from the start, you can simplify your database architecture and improve performance.Key Point: Even with NoSQL, you’ll eventually need to introduce a cache to handle increasing database requests.Part 3: Caching - Speeding Up Your ApplicationWhy Caching MattersAs your database grows, fetching data can become slow, leading to poor user experience. The solution is caching—storing frequently accessed data in memory for quick retrieval.sequenceDiagram participant User participant Server participant Cache participant Database User-&gt;&gt;Server: Request Data Server-&gt;&gt;Cache: Check Cache alt Cache Hit Cache--&gt;&gt;Server: Return Cached Data Server--&gt;&gt;User: Respond with Data else Cache Miss Server-&gt;&gt;Database: Fetch Data Database--&gt;&gt;Server: Return Data Server-&gt;&gt;Cache: Store in Cache Server--&gt;&gt;User: Respond with Data endIn-Memory CachingAlways use in-memory caches like Redis or managed services such as AWS ElastiCache or Redis Enterprise. Avoid file-based caching, as it complicates server cloning and auto-scaling.Caching PatternsThere are two primary caching patterns: Cached Database Queries: Store the result of database queries in the cache. The query itself is hashed and used as the cache key. While this approach is common, it has limitations, especially when it comes to cache expiration. Cached Objects: Store entire objects or datasets in the cache. This approach is more efficient and easier to manage. For example, instead of caching individual database queries, cache the entire “Product” object, including prices, descriptions, and reviews. Benefits of Cached Objects: Simplifies cache management. Enables asynchronous processing. Reduces database load.Multi-Level CachingModern applications often implement multi-level caching strategies: Application-level caching: Libraries like Caffeine (Java) or quick.cache (Node.js) Distributed caching: Redis or Memcached CDN caching: Cloudflare, Fastly, or AWS CloudFront for static assets and API responsesPart 4: Asynchronism - Handling Time-Consuming TasksThe Problem with Synchronous ProcessingImagine walking into a bakery and being told to wait two hours for your bread. This is similar to what happens when your web service handles time-consuming tasks synchronously. Users are forced to wait, leading to a poor experience.sequenceDiagram participant User participant Frontend participant Queue participant Worker participant Database User-&gt;&gt;Frontend: Initiate Long-Running Task Frontend-&gt;&gt;Queue: Add Task to Queue Frontend--&gt;&gt;User: Task Queued Notification Queue-&gt;&gt;Worker: Assign Task Worker-&gt;&gt;Database: Process Task Worker--&gt;&gt;Frontend: Task Completed Frontend--&gt;&gt;User: Task Completion NotificationAsync #1: Pre-Computing DataOne way to handle this is by pre-computing data. For example, a bakery might bake bread at night and sell it in the morning. Similarly, you can pre-render dynamic content as static HTML files and serve them to users. This approach is highly scalable and can be combined with Content Delivery Networks (CDNs) like AWS CloudFront or Cloudflare.Async #2: Modern Message Queues and Event StreamingFor tasks that can’t be pre-computed, use message queues or event streaming platforms. When a user initiates a time-consuming task, the frontend adds the task to a queue and immediately notifies the user that the task is in progress. Workers process the queue and notify the frontend when the task is complete.Modern Tools for Asynchronism: Apache Kafka: A high-throughput distributed streaming platform. AWS SQS/SNS: Managed queue and notification services. Google Pub/Sub: A scalable, real-time messaging service. Serverless Functions: AWS Lambda, Google Cloud Functions, or Azure Functions for event-driven processing.Benefits of Asynchronism Improves user experience by reducing wait times. Makes your backend infinitely scalable. Simplifies the handling of complex, time-consuming tasks.ConclusionScalability is a journey, not a destination. By implementing containerized applications, optimizing your database with modern solutions, leveraging multi-level caching, and embracing event-driven architectures, you can build web services that handle millions of users with ease. Stay tuned for the next parts of this series, where we’ll dive deeper into advanced scalability techniques.Happy scaling!" }, { "title": "Understanding Apache Camel: A Comprehensive Guide to Integration Patterns and System Management", "url": "/posts/apache-camel-application-integration/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment", "date": "2025-02-08 00:00:00 +0530", "snippet": "Table of Contents Introduction to Apache Camel Understanding Camel Routes Essential Integration Patterns Real-World Integration Examples Advanced System Management Health CheckIntroductionApache Camel is like a universal translator and message router for different software systems. Think of it as a postal service for your applications - it picks up messages from one place, maybe transforms them, and delivers them to another place.flowchart LR subgraph \"Sources\" Files[Files in Folder] REST[Web Service] Email[Email] SQueue[Message Queue] end subgraph \"Apache Camel\" Router[Message Router] end subgraph \"Destinations\" DB[(Database)] Queue[Message Queue] API[Another Web Service] end Files --&gt; Router REST --&gt; Router Email --&gt; Router SQueue --&gt; Router Router --&gt; DB Router --&gt; Queue Router --&gt; API style Router fill:#f96,stroke:#333Understanding Camel RoutesA Camel route is like a set of instructions telling Camel how to move messages from point A to point B. Here’s a simple example:// This route moves files from an 'orders' folder to a 'processed' folderfrom(\"file:orders\") // 1. Watch the 'orders' folder for new files .log(\"Found file: ${file:name}\") // 2. Log the filename .to(\"file:processed\"); // 3. Move the file to 'processed' folder// This route reads files and sends important ones to a different placefrom(\"file:inbox\") // 1. Watch the 'inbox' folder .choice() // 2. Make a decision based on file content .when(simple(\"${file:name} contains 'urgent'\")) // 3. If filename has 'urgent' .to(\"direct:urgent\") // 4. Send to urgent processor .otherwise() // 5. If not urgent .to(\"direct:normal\"); // 6. Send to normal processorLet’s break down what this code does: The first route is like having someone watch a folder called “orders” for new files, and when they find one, they log its name and move it to a “processed” folder. The second route shows how to make decisions - it’s like a mail sorter who looks at each envelope and sends urgent mail to one place and regular mail to another.Essential Integration Patterns1. Content-Based RouterRoutes messages based on their content:from(\"file:incoming-orders\") .choice() .when(simple(\"${body.orderType} == 'RUSH'\")) .to(\"direct:rush-orders\") .otherwise() .to(\"direct:regular-orders\") .end();This pattern is like a traffic controller that: Examines the content of each message (the orderType) Routes rush orders one way Routes regular orders another way2. Filter PatternFilters messages based on criteria:flowchart LR Input[Input Messages] --&gt; Filter{Contains 'important'?} Filter --&gt;|Yes| Output[Important Queue] Filter --&gt;|No| Discard[Discarded] style Filter fill:#f96,stroke:#333from(\"direct:start\") .filter(simple(\"${body} contains 'important'\")) .to(\"direct:important-messages\");This pattern works like a sieve that: Examines each message Only lets through messages matching certain criteria Discards or ignores non-matching messages3. Transformer PatternConverts messages between formats:flowchart LR Input[JSON Input] --&gt; Transform[Transform Process] Transform --&gt; Output[XML Output] subgraph \"Transform Process\" M[Marshal] --&gt; U[Unmarshal] end style Transform fill:#69f,stroke:#333from(\"direct:start\") .marshal().json() .unmarshal().xml() .to(\"direct:xml-processor\");This pattern acts like a format converter that: Takes input in one format (like JSON) Transforms it to another format (like XML) Useful when connecting systems that speak different “languages”4. Dead Letter Channel PatternHandles failed messages by moving them to a Dead Letter Queue (DLQ).flowchart TD Start[Message] --&gt; Process{Process Message} Process --&gt;|Success| Success[Success Queue] Process --&gt;|Failure| Retry{Retry?} Retry --&gt;|Yes| Process Retry --&gt;|No| Dead[Dead Letter Queue] style Process fill:#f69,stroke:#333 style Retry fill:#f69,stroke:#333errorHandler(deadLetterChannel(\"direct:dead-letter\") .maximumRedeliveries(3) .redeliveryDelay(1000) .backOffMultiplier(2));from(\"direct:start\") .to(\"direct:possibly-failing-service\") .log(\"Message processed successfully\");This pattern works like a safety net that: Attempts to process a message. If processing fails, retries up to 3 times. Increases the delay between retries. If all retries fail, moves the message to a “dead letter” queue for further investigation.5. Splitter PatternSplits a single message into multiple messages:from(\"direct:start\") .split(body().tokenize(\",\")) .to(\"direct:process-split-message\");This pattern works like a paper cutter that: Takes a single message containing multiple items (e.g., a comma-separated list) Splits it into individual messages for each item Processes each item separately6. Aggregator PatternCombines multiple messages into a single message:from(\"direct:start\") .aggregate(header(\"orderId\"), new GroupedExchangeAggregationStrategy()) .completionSize(5) .to(\"direct:process-aggregated-message\");This pattern works like a collector that: Groups messages based on a common attribute (e.g., orderId) Aggregates them into a single message once a condition is met (e.g., 5 messages received) Processes the aggregated message7. Wire Tap Pattern (Logging Implementation)from(\"file:incoming-orders\") .wireTap(\"direct:audit-log\") .log(\"Received order: ${file:name}\") .to(\"direct:process-order\");from(\"direct:audit-log\") .log(\"Audit: ${file:name} received at ${date:now:yyyy-MM-dd HH:mm:ss}\") .to(\"file:audit-logs\");This pattern works like a phone tap that: Creates a copy of each message Sends the copy to a logging/monitoring system Allows the original message to continue its journey Useful for audit trails and monitoring8. Error handlerfrom(\"direct:start\") .errorHandler(defaultErrorHandler() .maximumRedeliveries(3) .redeliveryDelay(1000)) .to(\"direct:possibly-failing-service\") .log(\"Message processed successfully\");This pattern help implement simple error handling, Like a persistent delivery person who tries multiple times if nobody answers the door If sending a message fails, it will retry up to 3 times Between each retry, it waits for 1 second If all retries fail, it gives up and reports an errorReal-World Integration ExamplesHere’s an example that tries to show different patterns in use.// Order Processing Routefrom(\"file:incoming-orders\") // 1. Watch 'incoming-orders' folder for new files .log(\"Received order: ${file:name}\") // 2. Log when we find a new order // 3. Convert the file content to JSON .unmarshal().json() // 4. Add current timestamp to the order .setHeader(\"ProcessingTime\", simple(\"${date:now:yyyy-MM-dd HH:mm:ss}\")) // 5. Make decisions based on order type .choice() .when(simple(\"${body.orderType} == 'RUSH'\")) .log(\"Rush order detected!\") .to(\"direct:rush-orders\") .otherwise() .log(\"Regular order received\") .to(\"direct:regular-orders\") .end() // 6. Move processed file to archive .to(\"file:processed-orders\");// Rush Order Handlerfrom(\"direct:rush-orders\") .log(\"Processing rush order\") .to(\"direct:notify-shipping\") .to(\"direct:update-inventory\");// Regular Order Handlerfrom(\"direct:regular-orders\") .log(\"Processing regular order\") .delay(simple(\"${random(1000,5000)}\")) // Simulate processing time .to(\"direct:update-inventory\");Let’s break down what this order processing system does: File Monitoring: Watches a folder called “incoming-orders” for new order files When a new file appears, it starts processing it Initial Processing: Logs the filename so we know which order is being processed Converts the file content from JSON format (like a digital order form) Adding Information: Adds a timestamp to track when the order was processed Decision Making: Checks if it’s a rush order or regular order Rush orders get special handling Regular orders follow the normal process Different Handling: Rush orders: Immediately notify shipping and update inventory Regular orders: Process with some delay and update inventory File Management: Moves the processed order file to a different folder for record-keeping This example shows how Camel can: Monitor folders for new files Make decisions based on content Process different types of orders differently Keep track of what’s happening (logging) Archive processed filesIn previous example, different patterns work together to create a robust system: Content-Based Router sorts orders into rush and regular Wire Tap logs all activities for monitoring Transformer converts file contents into processable formats Dead Letter Channel handles any processing failures Filter could be used to only process orders meeting certain criteriaThese patterns are like building blocks that can be combined in different ways to handle various integration scenarios. They help solve common problems in a standardized way, making the system more maintainable and reliable.Kafka to ActiveMQ IntegrationThis example demonstrates consuming messages from Kafka and pushing them to ActiveMQ:import org.apache.camel.builder.RouteBuilder;import org.apache.camel.component.kafka.KafkaConstants;import org.apache.camel.LoggingLevel;public class KafkaToMQRoute extends RouteBuilder { @Override public void configure() throws Exception { // Error Handler Configuration errorHandler(deadLetterChannel(\"activemq:queue:dead.letter\") .maximumRedeliveries(3) .redeliveryDelay(1000) .backOffMultiplier(2) .useExponentialBackOff() .logRetryAttempted(true)); // Main route: Kafka → ActiveMQ from(\"kafka:orders-topic?\" + \"brokers=localhost:9092\" + \"&amp;groupId=orders-group\" + \"&amp;autoOffsetReset=earliest\" + \"&amp;consumersCount=3\" + \"&amp;maxPollRecords=100\") // Add correlation ID for tracking .setHeader(\"correlationId\", simple(\"${random(1000,9999)}\")) .log(LoggingLevel.INFO, \"Received from Kafka: ${body} with key: ${headers[kafka.KEY]}\") .process(exchange -&gt; { // Example message transformation String kafkaMessage = exchange.getIn().getBody(String.class); OrderMessage order = convertToOrderMessage(kafkaMessage); exchange.getIn().setBody(order); }) .setHeader(\"JMSPriority\", simple(\"${headers[kafka.PRIORITY]}\")) // Send to ActiveMQ queue .to(\"activemq:queue:orders?\" + \"timeToLive=86400000\" + \"&amp;deliveryPersistent=true\") .log(LoggingLevel.INFO, \"Successfully sent to ActiveMQ: ${header.correlationId}\"); // Dead Letter Queue Handler from(\"activemq:queue:dead.letter\") .log(LoggingLevel.ERROR, \"Failed to process message: ${body}. Error: ${exception.message}\") .process(handleError) .to(\"activemq:queue:failed.orders\"); } private OrderMessage convertToOrderMessage(String kafkaMessage) { //conversion logic here return new OrderMessage(/* converted message */); }}@Getter@Setter@Builderclass OrderMessage { private String orderId; private String customerInfo; private Double amount;}Let’s break down this example in detail: Main Components: Kafka Consumer: Reads messages from a Kafka topic named “orders-topic” Message Transformer: Converts Kafka messages to ActiveMQ format ActiveMQ Producer: Sends messages to ActiveMQ queue named “orders” Kafka Consumer Configuration: \"kafka:orders-topic?brokers=localhost:9092\" + \"&amp;groupId=orders-group\" + \"&amp;autoOffsetReset=earliest\" + \"&amp;consumersCount=3\" + \"&amp;maxPollRecords=100\" brokers: Kafka server address groupId: Consumer group for load balancing autoOffsetReset: Start reading position consumersCount: Number of parallel consumers maxPollRecords: Batch size for each poll Error Handling: errorHandler(deadLetterChannel(\"activemq:queue:dead.letter\") Retries failed messages 3 times Uses exponential backoff (delays increase between retries) Failed messages go to a dead letter queue Message Processing: Adds correlation ID for tracking Logs incoming messages Transforms message format Sets JMS properties ActiveMQ Configuration: activemq:queue:orders?timeToLive=86400000 Sets message expiry time (24 hours) Uses persistent delivery for reliability Here’s a visualization of the message flow:flowchart LR subgraph Kafka KT[Orders Topic] end subgraph \"Camel Route\" C[Consumer] T[Transform] P[Producer] end subgraph ActiveMQ Q[Orders Queue] DL[Dead Letter Queue] end KT --&gt; C C --&gt; T T --&gt; P P --&gt; Q P -.-&gt; DL style C fill:#f96,stroke:#333 style T fill:#69f,stroke:#333 style P fill:#9f6,stroke:#333To use this route, you would need: Dependencies in your pom.xml:&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-kafka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-activemq&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Configure connection properties in your application.properties:kafka.bootstrap.servers=localhost:9092activemq.broker.url=tcp://localhost:61616Some key features of this implementation: Reliable message delivery with retry mechanism Message tracking with correlation IDs Comprehensive logging Dead letter queue for failed messages Scalable with multiple consumers Message transformation capabilityAdvanced System ManagementSystem outages are common and can impact your services. One common approach is to use circuit breakers and route-level control to manage system outages.Circuit Breaker// Order Processing Route with Circuit Breakerfrom(\"kafka:orders-topic\") .routeId(\"order-processing-route\") .circuitBreaker() .resilience4jConfiguration() .failureRateThreshold(50) // 50% failure rate to open circuit .waitDurationInOpenState(10000) // Wait 10 seconds in open state .end() .to(\"http://order-service/api/orders\") .onFallback() .to(\"kafka:failed-orders\") .end();This code sets up a route in Apache Camel that reads messages from a Kafka topic named orders-topic. It uses a circuit breaker pattern to handle failures gracefully. If the failure rate exceeds 50%, the circuit breaker will open and stop sending requests to the primary endpoint (http://order-service/api/orders) for 10 seconds. During this time, any incoming messages will be sent to a fallback Kafka topic named failed-orders. This helps to prevent overwhelming the order service with requests when it is experiencing issues.Implementing the circuit breaker pattern in a route is quite straightforward as it is provided out of the box by Camel.Let’s take another example to understand how we can apply this in a Camel route.import org.apache.camel.builder.RouteBuilder;import org.apache.camel.model.CircuitBreakerDefinition;import org.apache.camel.builder.DeadLetterChannelBuilder;import org.apache.camel.LoggingLevel;public class MultiRouteWithCircuitBreaker extends RouteBuilder { private static final int THRESHOLD = 5; private static final int HALF_OPEN_AFTER = 10000; @Override public void configure() throws Exception { // Global error handler errorHandler(deadLetterChannel(\"direct:errorHandler\") .maximumRedeliveries(3) .redeliveryDelay(1000) .useExponentialBackOff() .logRetryAttempted(true)); // Route Control and Monitoring from(\"timer:routeMonitor?period=60000\") .routeId(\"monitor-route\") .process(exchange -&gt; { checkDestinationHealth(exchange); }) .choice() .when(header(\"systemStatus\").isEqualTo(\"DOWN\")) .process(exchange -&gt; { // Stop affected routes getContext().getRouteController().stopRoute(\"order-processing-route\"); getContext().getRouteController().stopRoute(\"payment-processing-route\"); }) .otherwise() .process(exchange -&gt; { // Start routes if they're stopped if (!getContext().getRouteController().getRouteStatus(\"order-processing-route\").isStarted()) { getContext().getRouteController().startRoute(\"order-processing-route\"); } if (!getContext().getRouteController().getRouteStatus(\"payment-processing-route\").isStarted()) { getContext().getRouteController().startRoute(\"payment-processing-route\"); } }); // Order Processing Route with Circuit Breaker from(\"kafka:orders-topic\") .routeId(\"order-processing-route\") .circuitBreaker() .resilience4jConfiguration() .failureRateThreshold(50) .waitDurationInOpenState(10000) .minimumNumberOfCalls(10) // Minimum calls before calculating failure rate .end() .log(\"Processing order: ${body}\") .to(\"http://order-service/api/orders\") .to(\"direct:payment-processor\") .onFallback() .log(LoggingLevel.ERROR, \"Circuit Breaker triggered for order processing\") .to(\"kafka:failed-orders\") .end(); // Payment Processing Route with Circuit Breaker from(\"direct:payment-processor\") .routeId(\"payment-processing-route\") .circuitBreaker() .resilience4jConfiguration() .failureRateThreshold(50) .waitDurationInOpenState(10000) .end() .log(\"Processing payment for order\") .to(\"http://payment-service/api/process\") .onFallback() .log(LoggingLevel.ERROR, \"Circuit Breaker triggered for payment processing\") .to(\"kafka:failed-payments\") .end(); // Error Handler Route from(\"direct:errorHandler\") .routeId(\"error-handler-route\") .log(LoggingLevel.ERROR, \"Error processing message: ${exception.message}\") .choice() .when(header(\"routeId\").isEqualTo(\"order-processing-route\")) .to(\"kafka:failed-orders\") .when(header(\"routeId\").isEqualTo(\"payment-processing-route\")) .to(\"kafka:failed-payments\") .otherwise() .to(\"kafka:dead-letter-queue\"); // Recovery Route - Processes failed messages when system recovers from(\"kafka:failed-orders\") .routeId(\"recovery-route\") .circuitBreaker() .resilience4jConfiguration() .failureRateThreshold(50) .waitDurationInOpenState(30000) .end() .log(\"Attempting to recover failed order\") .to(\"http://order-service/api/orders\") .onFallback() .log(LoggingLevel.ERROR, \"Recovery still not possible\") .end(); }}Let’s visualize the route structure and circuit breaker flow:flowchart TD subgraph \"Route Monitor\" M[Monitor Timer] --&gt; HC{Health Check} HC --&gt;|Healthy| SR[Start Routes] HC --&gt;|Unhealthy| SP[Stop Routes] end subgraph \"Order Processing\" KO[Kafka Orders] --&gt; CB1{Circuit Breaker} CB1 --&gt;|Closed| OP[Process Order] CB1 --&gt;|Open| FB1[Fallback - Failed Orders] OP --&gt; PP[Payment Processing] end subgraph \"Payment Processing\" PP --&gt; CB2{Circuit Breaker} CB2 --&gt;|Closed| PS[Process Payment] CB2 --&gt;|Open| FB2[Fallback - Failed Payments] end subgraph \"Recovery\" FB1 --&gt; RR[Recovery Route] FB2 --&gt; RR RR --&gt; CB3{Circuit Breaker} CB3 --&gt;|Closed| RPR[Reprocess] CB3 --&gt;|Open| FB3[Delay Recovery] end style CB1 fill:#f96,stroke:#333 style CB2 fill:#f96,stroke:#333 style CB3 fill:#f96,stroke:#333 style M fill:#69f,stroke:#333Key features of this implementation: Route Monitoring: Periodic health checks of destination systems Automatic route stopping/starting based on system health Configurable monitoring intervals Circuit Breaker Pattern: Uses Resilience4j for circuit breaker implementation Configurable failure thresholds and recovery times Separate circuit breakers for different routes Fallback mechanisms for handling failures Multiple Routes: Order processing route Payment processing route Error handling route Recovery route Error Handling: Dead letter channel for failed messages Different error queues for different types of failures Retry mechanism with exponential backoff Recovery Mechanism: Separate route for processing failed messages Circuit breaker protection for recovery attempts Longer wait times during recovery To use this in your application: Add dependencies to pom.xml:&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-resilience4j&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-kafka&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Configure in application.properties:# Circuit Breaker Configurationcamel.circuitbreaker.threshold=5camel.circuitbreaker.half-open-after=10000# Kafka Configurationkafka.bootstrap.servers=localhost:9092# Health Check Configurationhealth.check.interval=60000Health Check (Optional)In the previous example, we saw how based on a health check we can stop and start routes. The timer will invoke the health check route at fixed intervals. Let’s zoom into the implementation of the health check and see how it can be implemented to consider various different systems.import org.apache.camel.Exchange;import org.apache.camel.builder.RouteBuilder;import org.apache.http.client.config.RequestConfig;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClientBuilder;import org.apache.http.client.methods.HttpGet;import javax.sql.DataSource;import java.sql.Connection;import java.util.HashMap;import java.util.Map;import java.net.Socket;import lombok.extern.slf4j.Slf4j;@Slf4jpublic class HealthCheckRoute extends RouteBuilder { private static final int TIMEOUT = 5000; private static final String ORDER_SERVICE_URL = \"http://order-service/health\"; private static final String PAYMENT_SERVICE_URL = \"http://payment-service/health\"; private static final String MQ_HOST = \"localhost\"; private static final int MQ_PORT = 61616; private final DataSource dataSource; public HealthCheckRoute(DataSource dataSource) { this.dataSource = dataSource; } @Override public void configure() throws Exception { from(\"timer:routeMonitor?period=60000\") .routeId(\"health-check-route\") .process(this::checkDestinationHealth) .choice() .when(header(\"systemStatus\").isEqualTo(\"DOWN\")) .log(\"System status is DOWN. Stopping routes...\") .process(exchange -&gt; stopAffectedRoutes(exchange)) .otherwise() .log(\"System status is UP. Starting routes if needed...\") .process(exchange -&gt; startRoutes(exchange)); } private void checkDestinationHealth(Exchange exchange) { Map&lt;String, ServiceHealth&gt; healthResults = new HashMap&lt;&gt;(); // Check each service and store results healthResults.put(\"orderService\", checkHttpService(ORDER_SERVICE_URL)); healthResults.put(\"paymentService\", checkHttpService(PAYMENT_SERVICE_URL)); healthResults.put(\"database\", checkDatabase()); healthResults.put(\"messageQueue\", checkMessageQueue()); // Determine overall system status boolean isSystemHealthy = healthResults.values().stream() .allMatch(ServiceHealth::isHealthy); // Set headers with detailed health information exchange.getIn().setHeader(\"systemStatus\", isSystemHealthy ? \"UP\" : \"DOWN\"); exchange.getIn().setHeader(\"healthResults\", healthResults); // Store specific service statuses for granular route control exchange.getIn().setHeader(\"orderServiceStatus\", healthResults.get(\"orderService\").isHealthy() ? \"UP\" : \"DOWN\"); exchange.getIn().setHeader(\"paymentServiceStatus\", healthResults.get(\"paymentService\").isHealthy() ? \"UP\" : \"DOWN\"); } private ServiceHealth checkHttpService(String url) { try (CloseableHttpClient client = createHttpClient()) { HttpGet request = new HttpGet(url); int responseCode = client.execute(request) .getStatusLine() .getStatusCode(); boolean isHealthy = responseCode &gt;= 200 &amp;&amp; responseCode &lt; 300; return new ServiceHealth( isHealthy, isHealthy ? \"Service responding normally\" : \"Service returned status code: \" + responseCode ); } catch (Exception e) { log.error(\"Error checking HTTP service {}: {}\", url, e.getMessage()); return new ServiceHealth(false, \"Service check failed: \" + e.getMessage()); } } private ServiceHealth checkDatabase() { try (Connection conn = dataSource.getConnection()) { boolean isValid = conn.isValid(TIMEOUT); return new ServiceHealth( isValid, isValid ? \"Database connection is valid\" : \"Database connection test failed\" ); } catch (Exception e) { log.error(\"Database health check failed: {}\", e.getMessage()); return new ServiceHealth(false, \"Database check failed: \" + e.getMessage()); } } private ServiceHealth checkMessageQueue() { try (Socket socket = new Socket(MQ_HOST, MQ_PORT)) { boolean isConnected = socket.isConnected(); return new ServiceHealth( isConnected, isConnected ? \"Message queue is accessible\" : \"Could not connect to message queue\" ); } catch (Exception e) { log.error(\"Message queue health check failed: {}\", e.getMessage()); return new ServiceHealth(false, \"Message queue check failed: \" + e.getMessage()); } } private CloseableHttpClient createHttpClient() { RequestConfig config = RequestConfig.custom() .setConnectTimeout(TIMEOUT) .setConnectionRequestTimeout(TIMEOUT) .setSocketTimeout(TIMEOUT) .build(); return HttpClientBuilder.create() .setDefaultRequestConfig(config) .build(); } private void stopAffectedRoutes(Exchange exchange) { Map&lt;String, ServiceHealth&gt; healthResults = exchange.getIn().getHeader(\"healthResults\", Map.class); try { if (!healthResults.get(\"orderService\").isHealthy()) { getContext().getRouteController().stopRoute(\"order-processing-route\"); log.info(\"Stopped order-processing-route due to health check failure\"); } if (!healthResults.get(\"paymentService\").isHealthy()) { getContext().getRouteController().stopRoute(\"payment-processing-route\"); log.info(\"Stopped payment-processing-route due to health check failure\"); } } catch (Exception e) { log.error(\"Error stopping routes: {}\", e.getMessage()); } } private void startRoutes(Exchange exchange) { Map&lt;String, ServiceHealth&gt; healthResults = exchange.getIn().getHeader(\"healthResults\", Map.class); try { if (healthResults.get(\"orderService\").isHealthy() &amp;&amp; !getContext().getRouteController() .getRouteStatus(\"order-processing-route\").isStarted()) { getContext().getRouteController().startRoute(\"order-processing-route\"); log.info(\"Started order-processing-route after health check recovery\"); } if (healthResults.get(\"paymentService\").isHealthy() &amp;&amp; !getContext().getRouteController() .getRouteStatus(\"payment-processing-route\").isStarted()) { getContext().getRouteController().startRoute(\"payment-processing-route\"); log.info(\"Started payment-processing-route after health check recovery\"); } } catch (Exception e) { log.error(\"Error starting routes: {}\", e.getMessage()); } }}@lombok.Valueclass ServiceHealth { boolean healthy; String message;}Health Check Visualizationflowchart TD Timer[Timer Trigger] --&gt; HC[Health Check Process] subgraph \"Health Checks\" HC --&gt; |Check| HTTP[HTTP Services] HC --&gt; |Check| DB[Database] HC --&gt; |Check| MQ[Message Queue] end subgraph \"Status Evaluation\" HTTP --&gt; Eval[Evaluate Results] DB --&gt; Eval MQ --&gt; Eval Eval --&gt; Status{System Status} end Status --&gt;|DOWN| Stop[Stop Affected Routes] Status --&gt;|UP| Start[Start Routes if Stopped] style HC fill:#f96,stroke:#333 style Status fill:#69f,stroke:#333Key points about the implementation: Health Check Process: Checks multiple services: HTTP endpoints, database, message queue Each check has a timeout (5 seconds) Results are stored in a ServiceHealth object containing: healthy: boolean status message: detailed status message Status Headers: systemStatus: Overall system status (“UP” or “DOWN”) healthResults: Map containing detailed health check results orderServiceStatus: Specific status for order service paymentServiceStatus: Specific status for payment service Route Control: Routes are stopped/started based on their specific service health Order processing route depends on order service health Payment processing route depends on payment service health To use this implementation: Add required dependencies:&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Configure in application.properties:# Health Check Configurationhealth.check.timeout=5000health.check.order.service.url=http://order-service/healthhealth.check.payment.service.url=http://payment-service/healthhealth.check.mq.host=localhosthealth.check.mq.port=61616ConclusionApache Camel provides a powerful framework for implementing enterprise integration patterns. By combining these patterns with robust system management features like circuit breakers and health checks, you can build reliable, scalable integration solutions.Remember to always: Implement proper error handling Monitor system health Use circuit breakers to prevent cascade failures Maintain clear logging and monitoring Consider message persistence and reliability requirementsFor more information, visit the Apache Camel documentation." }, { "title": "Tech Blogs: Become better software Engineer", "url": "/posts/tech-blogs-become-better-software-engineer/", "categories": "Blogging, Article", "tags": "softwareengineering", "date": "2025-01-20 00:00:00 +0530", "snippet": "Tech Company Blogs Netflix Tech Blog Google Developers Blog Amazon Web Services (AWS) Blog Uber Engineering Blog Stripe Engineering Blog Airbnb Engineering Blog Meta Engineering Blog Microsoft Developer Blog Pinterest Engineering Blog LinkedIn Engineering Blog Spotify Engineering Blog Slack Engineering Blog Cloud Engineering Blog by Dropbox Instacart Engineering Blog DoorDash Engineering Blog Shopify Engineering Blog Etsy Engineering Blog (Code as Craft) Robinhood Engineering Blog Datadog Engineering Blog Zendesk Engineering Blog Square Engineering Blog Adobe Tech Blog Cloudflare Blog Canva Engineering Blog Monzo Engineering Blog Asana Engineering Blog Atlassian Engineering Blog Okta Developer BlogCommunity and Open Source Blogs Dev.to ThoughtWorks Insights DigitalOcean Blog Hacker Noon Medium Engineering Open Source Initiative Blog Linux Foundation BlogIndependent and Developer-Centric Blogs Martin Fowler’s Blog High Scalability Coding Horror Joel on Software Troy Hunt’s BlogResearch-Oriented Blogs DeepMind Blog OpenAI Blog NVIDIA Developer Blog Berkeley AI Research (BAIR) Blog Distill Google AI Blog MIT Technology Review Stanford HAI Blog Microsoft Research BlogTech Magazine and News Blogs InfoQ Ars Technica LWN.net (Linux News) Better Programming Dark ReadingSpecialized Engineering Blogs JetBrains Blog Cloud Native Computing Foundation (CNCF) Blog Elastic Blog F5 NGINX Blog Kubernetes Blog Serverless Blog Apache Kafka Blog Postman Engineering Blog PyTorch BlogCloud and Infrastructure Blogs AWS Architecture Blog Linode Developer Blog Microsoft Azure Blog VMware Blogs New Relic Blog CircleCI BlogSecurity-Focused Blogs Krebs on Security Bruce Schneier’s Blog" }, { "title": "Understanding Aspect-Oriented Programming (AOP) in Spring Boot", "url": "/posts/spring-aop/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment", "date": "2024-12-10 00:00:00 +0530", "snippet": "IntroductionAspect-Oriented Programming (AOP) in Spring Boot allows developers to handle cross-cutting concerns (common code) such as logging, authentication, and validation in a modular way. AOP helps keep the main business logic clean by separating these concerns into reusable components called aspects.In this article, we will implement AOP in a Spring Boot application for a Users and Orders API. We will define aspects to: Validate incoming requests. Log invalid requests to the database. Log successful request and response details.As part of auditing, these three steps are common in applications that are expected to keep track of incoming requests and outgoing responses for specific or all important API calls.Key AOP ConceptsBefore diving into implementation, let’s briefly cover some key AOP concepts: Aspect: A class containing cross-cutting concerns. Advice: The action taken by an aspect at a specific point (e.g., before, after, or around method execution). Join Point: A specific point in the execution flow of the application (e.g., method execution). Pointcut: A predicate that matches join points, defining where advice should be applied. Custom Annotation: A user-defined annotation that can be used as a pointcut marker for AOP.Implementing AOP in a Spring Bootflowchart TD A[Client Request] --&gt; B{AOP Proxy} B --&gt;|Around Advice| C{Input Validation} C --&gt;|Valid| D[Business Logic] C --&gt;|Invalid| H[Validation Error] D --&gt; E[Response Processing] E --&gt; F[Logging] F --&gt; G[Client Response] H --&gt; G style B fill:#f9f,stroke:#333,stroke-width:4px style E fill:#bbf,stroke:#333,stroke-width:2pxStep 1: Define a Custom Annotation for Validate And Logging request and response@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface ValidateRequest {}This code defines a custom annotation @ValidateRequest in Java. The @Target(ElementType.METHOD) specifies that this annotation can only be applied to methods.The @Retention(RetentionPolicy.RUNTIME) indicates that the annotation will be available at runtime, allowing it to be accessed through reflection.Step 2: Create Users and Orders API Controllers@RestController@RequestMapping(value = \"/api/orders\", produces = MediaType.APPLICATION_JSON_VALUE)@RequiredArgsConstructorpublic class OrderController { private final OrderService orderService; @GetMapping public ResponseEntity&lt;List&lt;OrderDto&gt;&gt; getAllOrders() { return ResponseEntity.ok(orderService.findAll()); } @PostMapping @ValidateRequest public ResponseEntity&lt;Long&gt; createOrder(@RequestBody final OrderDto orderDTO) { return new ResponseEntity&lt;&gt;(orderService.create(orderDTO), HttpStatus.CREATED); }}@RestController@RequestMapping(value = \"/api/users\", produces = MediaType.APPLICATION_JSON_VALUE)@RequiredArgsConstructorpublic class UserController { private final UserService userService; @GetMapping public ResponseEntity&lt;List&lt;UserDto&gt;&gt; getAllUsers() { return ResponseEntity.ok(userService.findAll()); } @PostMapping @ValidateRequest public ResponseEntity&lt;Long&gt; createUser(@RequestBody final UserDto userDto) { return new ResponseEntity&lt;&gt;(userService.create(userDto), HttpStatus.CREATED); }}Step 3: Define an Aspect for validating and auditing Annotation-Based Pointcuts@Aspect@Component@RequiredArgsConstructorpublic class RequestValidationAndAuditingAspect { private final AuditRepository auditRepository; private final HttpServletRequest requestServlet; private final Validator validator; @Pointcut(\"@annotation(dev.pravin.ecombackend.annotations.ValidateRequest)\") public void validateRequestPointcut() {} @Around(\"validateRequestPointcut() &amp;&amp; args(request,..)\") public Object validateAndAuditRequest(ProceedingJoinPoint joinPoint, Object request) throws Throwable { validateRequest(request); Object response = joinPoint.proceed(); auditDetails(request, response); return response; } private void auditDetails(Object request, Object response) { String correlationId = this.requestServlet.getHeader(\"Correlation-Id\"); String requestDetails = request.toString(); ObjectMapper objectMapper = new ObjectMapper(); String responseJson = objectMapper.writeValueAsString(response) String requestJson = objectMapper.writeValueAsString(request) logAudit(correlationId, requestJson, responseJson); } private void validateRequest(Object request) { Set&lt;ConstraintViolation&lt;Object&gt;&gt; violations = validator.validate(request); if (!violations.isEmpty()) { logBadRequest(request, violations); throw new BadRequestException(\"Invalid request parameters.\"); } } private void logBadRequest(Object request, Set&lt;ConstraintViolation&lt;Object&gt;&gt; violations) { String violationDetails = violations.toString(); String correlationId = this.requestServlet.getHeader(\"Correlation-Id\"); auditRepository.save(AuditLog.builder().status(\"BAD_REQUEST\") .requestDetails(correlationId + \"--\" + request.toString()) .responseDetails(violationDetails) .build()); } private void logAudit(String correlationId, String requestDetails, Object response) { auditRepository.save(AuditLog.builder().status(\"SUCCESS\") .requestDetails(correlationId + \"--\" + requestDetails) .responseDetails(response.toString()) .build()); }}This code defines an aspect RequestValidationAndAuditingAspect for validating and auditing requests and responses in a Spring Boot application. The @Aspect annotation indicates that this class is an aspect. The @Component annotation makes it a Spring-managed bean. The @RequiredArgsConstructor annotation generates a constructor with required arguments.The aspect contains: A pointcut validateRequestPointcut that matches methods annotated with @ValidateRequest. An around advice validateAndAuditRequest that: Validates the request using validateRequest. Proceeds with the method execution. Audits the request and response using auditDetails. The auditDetails method logs the request and response details. The validateRequest method validates the request and logs bad requests if there are validation errors. The logBadRequest and logAudit methods save audit logs to the database.Step 4: Create a Database Entity@Builder@Entity@AllArgsConstructorpublic class AuditLog { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String status; private String requestDetails; private String responseDetails;}Step 5: Define the Repository for Logging Requests@Repositorypublic interface AuditRepository extends JpaRepository&lt;AuditLog, Long&gt; {}let’s visualize the AOP concept and how aspects interact with business logic:stateDiagram-v2 [*] --&gt; RequestReceived RequestReceived --&gt; Validation: @ValidateRequest Validation --&gt; MethodExecution: Valid Validation --&gt; ErrorLogging: Invalid MethodExecution --&gt; ResponseProcessing: Success MethodExecution --&gt; ErrorLogging: Exception ResponseProcessing --&gt; AuditLogging ErrorLogging --&gt; AuditLogging AuditLogging --&gt; [*] note right of Validation Checks: - Required fields - Data format - Business rules end note note right of AuditLogging Stores: - Request details - Response/Error - Correlation ID - Timestamp end noteExplanation of AOP Usage Join Point: Any execution of a method in OrderController and UserController. Pointcut: Defined by @Pointcut(\"@annotation(dev.pravin.ecombackend.annotations.ValidateRequest)\") to apply advice for annotated methods in OrderController and UserController.Types of AdviceBefore Advice Description: This advice runs before the method execution. It is used to perform actions such as logging or security checks before the actual method logic is executed. Example: Validates the token in the API request for all RestController methods.After Advice Description: This advice runs after the method execution, regardless of its outcome. It is useful for cleanup actions or logging that should happen whether the method succeeds or fails. Example: Logs a message after the method execution.AfterReturning Advice Description: This advice runs after the method successfully returns a result. It can be used to log the return value or perform actions based on the result. Example: Tracks successful API calls as part of observability.AfterThrowing Advice Description: This advice runs if the method throws an exception. It is useful for logging errors or performing actions in response to an error. Example: Tracks failed API calls and logs the HTTP status code.Around Advice Description: This advice runs before and after the method execution. It can control whether the method executes at all, and can modify the return value or throw an exception. Example: Validates the request and audits the request and response.AOP Advice Execution SequencesequenceDiagram participant C as Client participant BA as Before Advice participant M as Method participant AA as After Advice participant AR as AfterReturning participant AT as AfterThrowing C-&gt;&gt;BA: Request Note over BA: @Before executes BA-&gt;&gt;M: Continue if validation passes alt Successful Execution M-&gt;&gt;AR: Method completes AR-&gt;&gt;AA: Success path Note over AR: @AfterReturning executes else Exception Thrown M-&gt;&gt;AT: Method throws exception AT-&gt;&gt;AA: Error path Note over AT: @AfterThrowing executes end Note over AA: @After executes AA-&gt;&gt;C: ResponseMore Examples for Different Types of AdviceBefore AdviceThis advice runs before the method execution. Let’s take a simple example that validates the token in the API request for all RestController methods.@Service@RequiredArgsConstructorpublic class TokenValidationService { private final HttpServletRequest requestServlet; public boolean validateToken(String authToken) { if (authToken == null || !isValid(authToken)) { return false; } return true; } private boolean isValid(String token) { // token validation logic return true; // Return true if valid, false otherwise }}@Aspect@Component@RequiredArgsConstructorpublic class TokenValidationAspect { private final TokenValidationService tokenValidationService; @Pointcut(\"within(@org.springframework.web.bind.annotation.RestController *)\") public void controllerMethods() {} @Before(\"controllerMethods()\") public void validateToken(JoinPoint joinPoint) { if (!tokenValidationService.validateToken()) { throw new UnauthorizedException(\"Invalid or missing authentication token.\"); } }}After AdviceThis advice runs after the method execution, regardless of its outcome.@After(\"validateRequestPointcut()\")public void logAfter(JoinPoint joinPoint) { System.out.println(\"After method: \" + joinPoint.getSignature().getName());}AfterReturning And AfterThrowing AdviceAfterReturning advice runs after the method successfully returns a result, and AfterThrowing advice runs if the method throws an exception. Let’s take an example of tracking the API call as part of observability.&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-core&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;&lt;/dependency&gt;@Aspect@Component@RequiredArgsConstructorpublic class ApiHealthAspect { private final MeterRegistry meterRegistry; @Pointcut(\"within(@org.springframework.web.bind.annotation.RestController *)\") public void controllerMethods() {} @AfterReturning(pointcut = \"controllerMethods()\", returning = \"result\") public void logAfterReturning(JoinPoint joinPoint, Object result) { String apiName = joinPoint.getSignature().getName(); meterRegistry.counter(\"api.success\", \"api\", apiName).increment(); } @AfterThrowing(pointcut = \"controllerMethods()\", throwing = \"error\") public void logAfterThrowing(JoinPoint joinPoint, Throwable error) { String apiName = joinPoint.getSignature().getName(); meterRegistry.counter(\"api.failure\", \"api\", apiName, \"status\", getStatus(error)).increment(); } private String getStatus(Throwable error) { if (error instanceof BadRequestException) { return \"4xx\"; } else { return \"5xx\"; } }}Comparison: Annotation-Based vs. Regular Expression-Based Pointcuts Criteria Annotation-Based (@ValidateRequest) Regex-Based (execution(..)) Granularity More flexible, can be applied selectively Matches all methods matching the pattern Code Intrusiveness Requires modifying methods with annotations No modifications needed in the target methods Maintainability Easier to manage since it explicitly marks important methods Might require frequent updates if method signatures change Performance Slightly better as only annotated methods are intercepted Can be less efficient if applied broadly Use Case Best for selective logging where explicit annotation is preferred Best for applying to multiple methods with a common pattern Aspect Ordering with @OrderIn Spring AOP, you can control the order in which aspects are applied using the @Order annotation. This is useful when you have multiple aspects that should be applied in a specific sequence.Example code@Aspect@Component@Order(1)public class FirstAspect { @Before(\"execution(* dev.pravin.ecombackend.service.*.*(..))\") public void beforeAdvice(JoinPoint joinPoint) { System.out.println(\"First Aspect - Before method: \" + joinPoint.getSignature().getName()); }}@Aspect@Component@Order(2)public class SecondAspect { @Before(\"execution(* dev.pravin.ecombackend.service.*.*(..))\") public void beforeAdvice(JoinPoint joinPoint) { System.out.println(\"Second Aspect - Before method: \" + joinPoint.getSignature().getName()); }}In this example, FirstAspect will be applied before SecondAspect due to the @Order annotation.AOP order SequencesequenceDiagram participant C as Client participant S as Security Aspect&lt;br&gt;@Order(1) participant V as Validation Aspect&lt;br&gt;@Order(2) participant L as Logging Aspect&lt;br&gt;@Order(3) participant M as Method C-&gt;&gt;S: Request Note over S: Security Check S-&gt;&gt;V: If authorized Note over V: Validate Input V-&gt;&gt;L: If valid Note over L: Log Request L-&gt;&gt;M: Execute method M-&gt;&gt;L: Response Note over L: Log Response L-&gt;&gt;V: Continue V-&gt;&gt;S: Continue S-&gt;&gt;C: Final ResponseWhen to Use Which? Use annotation-based pointcuts when you need explicit control over which methods should be intercepted. Use regex-based execution pointcuts when you want to apply AOP to multiple methods following a common pattern without modifying the source code.Best Practices and Design PatternsDesign Patterns Commonly Used with AOP Proxy Pattern: AOP uses the proxy pattern to create a proxy object that wraps the target object and intercepts method calls. Decorator Pattern: Similar to the proxy pattern, the decorator pattern adds additional behavior to objects dynamically. Chain of Responsibility: Multiple aspects can be applied in a chain, where each aspect handles a specific concern.Anti-Patterns to Avoid Overusing Aspects: Avoid using aspects for every cross-cutting concern. Use them judiciously to prevent code complexity. Tight Coupling: Ensure that aspects are loosely coupled with the business logic. Avoid direct dependencies between aspects and business classes. Performance Impact: Be mindful of the performance impact of aspects, especially if they are applied to frequently called methods.Guidelines for Aspect Design Single Responsibility: Each aspect should handle a single cross-cutting concern to promote modularity and maintainability. Reusability: Design aspects to be reusable across different parts of the application. Granularity: Use fine-grained pointcuts to apply aspects selectively and avoid broad application that can impact performance. Testing: Thoroughly test aspects to ensure they work as expected and do not introduce unintended side effects.ConclusionIn this article, we demonstrated how Aspect-Oriented Programming (AOP) in Spring Boot can handle cross-cutting concerns like request validation and auditing. By using custom annotations, pointcuts, and aspects, AOP keeps the business logic clean while modularizing tasks such as validation and logging.We also compared annotation-based and regex-based pointcuts. Annotation-based pointcuts offer precise control for selective interception, while regex-based pointcuts can apply AOP broadly.Overall, AOP helps improve code maintainability and separation of concerns in Spring Boot applications." }, { "title": "Kafka Schema Registry and JSON Schema: A Comprehensive Guide", "url": "/posts/kafka-schema-registry-and-json-schema-a-comprehensive-guide/", "categories": "Blogging, Article", "tags": "backenddevelopment, design", "date": "2024-11-08 00:00:00 +0530", "snippet": "Kafka Schema Registry and JSON Schema: A Comprehensive GuideIntroductionWhat is Kafka?Kafka is a distributed streaming platform designed for high throughput and low latency, widely used for: Event-driven architectures Real-time analytics Data pipelinesKafka Schema Registry: Core ConceptKafka Schema Registry is a centralized service that: Manages and validates message schemas in Kafka topics Provides schema versioning Ensures compatibility between producers and consumersThe Role of JSON SchemaJSON Schema is a declarative language for: Defining JSON data structures Enforcing data validation rules Ensuring consistent data across distributed systemsSchema Evolution and CompatibilityTypes of Schema CompatibilityCompatibility Types Explained Backward Compatibility: New schema versions can be read by consumers using older schemas Allows adding new fields Existing consumers can still process messages Forward Compatibility: Old schema versions can consume data from new schemas Allows removing fields carefully New consumers can process older message formats Full Compatibility: Combines backward and forward compatibility Provides maximum flexibility in schema evolution Schema Evolution ScenariosScenario 1: Making a Field NullableOne common schema evolution approach is making a field nullable. This allows the field to either contain a value or be absent (null).Initial Schema (Version 1):{ \"type\": \"object\", \"properties\": { \"user_id\": { \"type\": \"integer\" }, \"name\": { \"type\": \"string\" }, \"email\": { \"type\": \"string\" } }, \"required\": [\"user_id\", \"name\", \"email\"]}Evolved Schema (Version 2):{ \"type\": \"object\", \"properties\": { \"user_id\": { \"type\": \"integer\" }, \"name\": { \"type\": \"string\" }, \"email\": { \"type\": [\"string\", \"null\"] } }, \"required\": [\"user_id\", \"name\"]}In this evolution: The email field becomes optional email can now be a string or null Maintains compatibility with existing consumersScenario 2: Using Default ValuesDefault values provide a powerful mechanism for maintaining compatibility during schema changes, especially when making breaking changes like converting a nullable field to a required field.Schema with Default Value:{ \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"fieldA\", \"type\": \"string\", \"default\": \"default_value\" } ]}Benefits of default values: Prevent serialization errors Maintain backward and forward compatibility Ensure smooth data flow during schema changesData Flow in Kafka with Schema RegistryData Flow Steps Producer Side: Retrieves or registers schema Serializes message using schema Sends message with schema ID to Kafka topic Kafka Topic: Stores messages with schema ID references Supports multiple schema versions Consumer Side: Reads message with schema ID Retrieves corresponding schema Deserializes message using schema Practical ImplementationDocker Compose Setupversion: '3'services: zookeeper: image: confluentinc/cp-zookeeper:latest container_name: zookeeper environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 ports: - \"2181:2181\" schema-registry: image: confluentinc/cp-schema-registry:7.8.0 hostname: schema-registry depends_on: - kafka-broker-1 ports: - \"8081:8081\" environment: SCHEMA_REGISTRY_HOST_NAME: schema-registry SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181' SCHEMA_REGISTRY_LISTENERS: http://schema-registry:8081 SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka-broker-1:9092,PLAINTEXT_INTERNAL://localhost:19092 SCHEMA_REGISTRY_DEBUG: 'true' kafka-broker-1: image: confluentinc/cp-kafka:latest hostname: kafka-broker-1 ports: - \"19092:19092\" depends_on: - zookeeper environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181' KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-1:9092,PLAINTEXT_INTERNAL://localhost:19092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1Schema Registry API# Register a new schemacurl --request POST \\ --url http://localhost:8081/subjects/user-registration-test-value/versions \\ --header 'Content-Type: application/vnd.schemaregistry.v1+json' \\ --data '{\t\"schemaType\": \"JSON\",\t\"schema\": \"{ \\\"$schema\\\": \\\"http://json-schema.org/draft-07/schema#\\\", \\\"title\\\": \\\"User\\\", \\\"description\\\": \\\"Schema representing a user\\\", \\\"type\\\": \\\"object\\\", \\\"additionalProperties\\\": false, \\\"properties\\\": { \\\"name\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"Name of person.\\\" }, \\\"email\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"email of person.\\\" }, \\\"userId\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"user id in the system\\\" } }, \\\"required\\\": [\\\"name\\\", \\\"userId\\\", \\\"email\\\"] }\"}'# Fetch Schemacurl --request GET \\--url http://localhost:8081/subjects/user-registration-test-value/versions/latest# Delete Schemacurl --request DELETE \\ --url http://localhost:8081/subjects/user-registration-test-valueJava Integration ExamplesConfigurationplugins {\tid 'java'\tid 'org.springframework.boot' version '3.4.0'\tid 'io.spring.dependency-management' version '1.1.6'}group = 'dev.pravin'version = '0.0.1-SNAPSHOT'java {\ttoolchain {\t\tlanguageVersion = JavaLanguageVersion.of(21)\t}}repositories {\tmavenCentral()\tmaven {\t\turl \"https://packages.confluent.io/maven\"\t}}dependencies {\timplementation 'org.springframework.boot:spring-boot-starter'\timplementation 'org.springframework.boot:spring-boot-starter-web'\timplementation 'org.springframework.kafka:spring-kafka'\timplementation 'io.confluent:kafka-json-schema-serializer:7.8.0'\timplementation 'io.confluent:kafka-schema-registry-client:7.8.0'\timplementation 'org.projectlombok:lombok:1.18.36'\tannotationProcessor 'org.projectlombok:lombok:1.18.36'\ttestImplementation 'org.springframework.boot:spring-boot-starter-test'\ttestImplementation 'org.springframework.kafka:spring-kafka-test'\ttestRuntimeOnly 'org.junit.platform:junit-platform-launcher'}tasks.named('test') {\tuseJUnitPlatform()}spring.application.name=schema registry demoserver.port=8100spring.kafka.properties.use.latest.version=true@EnableKafka@Configurationpublic class KafkaConfig { public static final String BROKER_URL = \"localhost:19092\"; public static final String SCHEMA_REGISTRY_URL = \"http://localhost:8081\"; public static final String SPRING_KAFKA_ADMIN_CLIENT = \"spring-kafka-admin-client\"; public static final String GROUP_ID = \"group_id_23\"; @Bean public ProducerFactory&lt;String, UserRegistrationTest&gt; producerFactory() { Map&lt;String, Object&gt; config = new HashMap&lt;&gt;(); config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BROKER_URL); config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaJsonSchemaSerializer.class); config.put(\"schema.registry.url\", SCHEMA_REGISTRY_URL); return new DefaultKafkaProducerFactory&lt;&gt;(config); } @Bean public KafkaTemplate&lt;String, UserRegistrationTest&gt; kafkaTemplate() { return new KafkaTemplate&lt;&gt;(producerFactory()); } @Bean public SchemaRegistryClient schemaRegistryClient() throws RestClientException, IOException { CachedSchemaRegistryClient client = new CachedSchemaRegistryClient(SCHEMA_REGISTRY_URL, 1); String jsonString = \"\"\" { \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"User\", \"description\": \"Schema representing a user\", \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"Name of person.\" }, \"email\": { \"type\": \"string\", \"description\": \"email of person.\" }, \"userId\": { \"type\": \"string\", \"description\": \"user id in the system\" } }, \"required\": [\"name\", \"userId\", \"email\"] } \"\"\"; client.register(\"user-registration-test-value\", new JsonSchema(jsonString)); return client; } @Bean public ConsumerFactory&lt;String, UserRegistrationTest&gt; consumerFactory(SchemaRegistryClient schemaRegistryClient) { Map&lt;String, Object&gt; config = new HashMap&lt;&gt;(); config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BROKER_URL); config.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID); config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaJsonSchemaDeserializer.class); config.put(\"schema.registry.url\", SCHEMA_REGISTRY_URL); config.put(\"specific.json.reader\", true); return new DefaultKafkaConsumerFactory&lt;&gt;(config, new StringDeserializer(), new KafkaJsonSchemaDeserializer&lt;&gt;(schemaRegistryClient)); } @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, UserRegistrationTest&gt; kafkaListenerContainerFactory(SchemaRegistryClient schemaRegistryClient) { ConcurrentKafkaListenerContainerFactory&lt;String, UserRegistrationTest&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;(); factory.setConsumerFactory(consumerFactory(schemaRegistryClient)); return factory; }}Producer Example@Data@Builder@Getterpublic class UserRegistrationTest { private String userId; private String email; private String name;}@Service@RequiredArgsConstructorpublic class KafkaProducer { private final SchemaRegistryClient schemaRegistryClient; private final KafkaTemplate&lt;String, UserRegistrationTest&gt; kafkaTemplate; public void sendMessage(UserRegistrationTest userRegistrationTest) throws RestClientException, IOException { schemaRegistryClient.reset(); int version = schemaRegistryClient.getLatestSchemaMetadata(TOPIC+\"-value\").getVersion(); System.out.println(\"Version: \" + version); kafkaTemplate.send(TOPIC, userRegistrationTest); System.out.println(\"Message sent: \" + userRegistrationTest); }}@RestController@RequiredArgsConstructor@RequestMapping(\"/kafka\")public class KafkaController { private final KafkaProducer kafkaProducer; @PostMapping(\"/publish\") public String publishMessage(@RequestBody UserRegistrationTest userRegistrationTest) throws RestClientException, IOException { kafkaProducer.sendMessage(userRegistrationTest); return \"Message published: \" + userRegistrationTest.toString(); }}Consumer Example@Service@RequiredArgsConstructorpublic class KafkaConsumer { @KafkaListener(topics = TOPIC) public void consume(String message) { System.out.println(\"Message received: \" + message); }}Publish message to Kafka topiccurl --request POST \\ --url http://localhost:8100/kafka/publish \\ --header 'Content-Type: application/json' \\ --header 'User-Agent: insomnia/10.2.0' \\ --data '{\t\"userId\": \"pravin\",\t\"email\": \"pravin@pravin.dev\"}'Best Practices and ConsiderationsSchema Evolution Guidelines Minimize breaking changes Use default values for new required fields Test compatibility before deploying Monitor schema changesPotential Challenges Managing complex schema evolutions Ensuring backward/forward compatibility Performance overhead of schema validationConclusionKafka Schema Registry with JSON Schema provides a robust solution for managing data structures in distributed systems. By carefully implementing schema evolution strategies, developers can create flexible, maintainable, and scalable data streaming architectures.Key Takeaways Centralize schema management Implement careful evolution strategies Utilize default values Continuously test and validate schemasSample CodeDownload the code" }, { "title": "Understanding Spring @Transactional Annotation: A Comprehensive Guide", "url": "/posts/spring-transactional-annotation/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, spring", "date": "2024-10-01 00:00:00 +0530", "snippet": "Spring’s @Transactional annotation is a powerful tool for managing transactions in Java applications. It simplifies transaction management by allowing developers to define transactional behavior declaratively, eliminating the need for boilerplate code. In this blog post, we’ll explore how @Transactional works, its key attributes, common pitfalls, and best practices to help you master transaction management in Spring.Table of Contents How @Transactional Works Key Attributes of @Transactional Propagation Isolation Rollback Behavior Read-Only Transactions Timeout Common Pitfalls Self-Invocation Checked Exceptions Visibility Issues Best Practices ConclusionHow @Transactional WorksWhen a method is annotated with @Transactional, Spring creates a proxy that wraps the method call. This proxy manages the transaction boundaries: A new transaction is started before method execution (if required). The transaction is committed if the method executes successfully. The transaction is rolled back if an exception occurs (based on the rollback rules).Here’s a visual representation of how @Transactional works:sequenceDiagram participant Client participant Proxy participant TargetMethod participant TransactionManager Client-&gt;&gt;Proxy: Calls @Transactional method Proxy-&gt;&gt;TransactionManager: Begin Transaction TransactionManager--&gt;&gt;Proxy: Transaction Started Proxy-&gt;&gt;TargetMethod: Invoke Method TargetMethod--&gt;&gt;Proxy: Method Execution alt Successful Execution Proxy-&gt;&gt;TransactionManager: Commit Transaction TransactionManager--&gt;&gt;Proxy: Transaction Committed else Exception Occurs Proxy-&gt;&gt;TransactionManager: Rollback Transaction TransactionManager--&gt;&gt;Proxy: Transaction Rolled Back end Proxy--&gt;&gt;Client: Return Result or ExceptionKey Attributes of @TransactionalPropagationPropagation defines how transactions behave when a method is called within another transaction. It determines whether a new transaction should be created, an existing one should be used, or no transaction should be used at all.Propagation Types at a Glance Propagation Type Short Description REQUIRED (default) Uses an existing transaction if available; otherwise, starts a new one. REQUIRES_NEW Suspends the current transaction and starts a new one. MANDATORY Throws an exception if no existing transaction is found. SUPPORTS Runs within a transaction if one exists; otherwise, runs non-transactionally. NOT_SUPPORTED Runs the method outside of any transaction, suspending an existing one if necessary. NEVER Throws an exception if an active transaction exists. NESTED Executes within a nested transaction if an existing transaction is found. Detailed Breakdown with Examples REQUIRED Uses an existing transaction if available; otherwise, starts a new one. Example: @Transactional(propagation = Propagation.REQUIRED)public void methodA() { methodB(); // Reuses the same transaction if called within methodA} Explanation: If methodA is called without an existing transaction, a new transaction is started. If methodB is called within methodA, it reuses the same transaction. REQUIRES_NEW Suspends the current transaction and starts a new one. Example: @Transactional(propagation = Propagation.REQUIRES_NEW)public void methodB() { // Business logic in a new transaction} Explanation: If methodB is called within an existing transaction, the current transaction is suspended, and a new transaction is started for methodB. After methodB completes, the original transaction resumes. MANDATORY Throws an exception if no existing transaction is found. Example: @Transactional(propagation = Propagation.MANDATORY)public void methodC() { // Business logic that requires an existing transaction} Explanation: If methodC is called without an existing transaction, a TransactionRequiredException is thrown. SUPPORTS Runs within a transaction if one exists; otherwise, runs non-transactionally. Example: @Transactional(propagation = Propagation.SUPPORTS)public void methodD() { // Business logic that can run with or without a transaction} Explanation: If methodD is called within a transaction, it runs within that transaction. If no transaction exists, it runs without one. NOT_SUPPORTED Runs the method outside of any transaction, suspending an existing one if necessary. Example: @Transactional(propagation = Propagation.NOT_SUPPORTED)public void methodE() { // Business logic that should not run within a transaction} Explanation: If methodE is called within a transaction, the transaction is suspended, and the method runs without a transaction. After the method completes, the original transaction resumes. NEVER Throws an exception if an active transaction exists. Example: @Transactional(propagation = Propagation.NEVER)public void methodF() { // Business logic that must not run within a transaction} Explanation: If methodF is called within an active transaction, an IllegalTransactionStateException is thrown. NESTED Executes within a nested transaction if an existing transaction is found. Example: @Transactional(propagation = Propagation.NESTED)public void methodG() { // Business logic that runs in a nested transaction} Explanation: If methodG is called within an existing transaction, a nested transaction is created. If the outer transaction fails, the nested transaction is rolled back. If the nested transaction fails, the outer transaction can decide whether to commit or rollback. IsolationDefines how data modifications in one transaction are visible to others. Isolation Level Description DEFAULT Uses the database default isolation level. READ_UNCOMMITTED Allows dirty reads (reading uncommitted changes from other transactions). READ_COMMITTED Prevents dirty reads; a transaction sees only committed changes. REPEATABLE_READ Prevents non-repeatable reads; data read within a transaction remains consistent. SERIALIZABLE Ensures complete isolation but may cause performance overhead. Example:@Transactional(isolation = Isolation.REPEATABLE_READ)public void updateAccountBalance() { // Business logic to update balance}Rollback BehaviorSpecifies conditions under which transactions should be rolled back. Rollback Rule Description rollbackFor Defines exceptions that trigger rollback (e.g., rollbackFor = Exception.class). noRollbackFor Defines exceptions that should not trigger rollback. Example:@Transactional(rollbackFor = SQLException.class)public void saveData() throws SQLException { // Database operations that should roll back on SQL exception}Read-Only TransactionsIndicates that a transaction will not perform write operations, allowing optimizations.Example:@Transactional(readOnly = true)public List&lt;User&gt; getUsers() { return userRepository.findAll();}TimeoutSpecifies the maximum time (in seconds) a transaction can run before rollback.Example:@Transactional(timeout = 5)public void processLongRunningTask() { // Business logic}Common PitfallsSelf-InvocationTransactional methods calling each other within the same class do not trigger transactional behavior due to proxy-based AOP. This happens because the proxy is bypassed during self-invocation.Solution: Use AopContext.currentProxy() or refactor the code to call the method from another bean.@Servicepublic class MyService { public void outerMethod() { ((MyService) AopContext.currentProxy()).innerMethod(); } @Transactional public void innerMethod() { // Business logic }}Checked ExceptionsBy default, transactions roll back only on unchecked exceptions (RuntimeException or Error). Checked exceptions do not trigger rollback unless explicitly configured.Solution: Use the rollbackFor attribute to specify checked exceptions.@Transactional(rollbackFor = CustomCheckedException.class)public void myMethod() throws CustomCheckedException { // Business logic}Visibility IssuesThe method must be public to ensure Spring’s proxy mechanism applies correctly. Non-public methods (e.g., private, protected) will not be transactional.Solution: Always declare transactional methods as public.Best Practices Use @Transactional at the Service Layer: Apply @Transactional to service methods rather than DAO or repository methods. This ensures that business logic is executed within a single transaction. Avoid Long-Running Transactions: Use the timeout attribute to prevent transactions from holding database resources for too long. Leverage Read-Only Transactions: Mark read-only methods with readOnly = true to optimize database performance. Test Transactional Behavior: Write unit and integration tests to verify that transactions behave as expected, especially for rollback scenarios. Monitor Transaction Performance: Use tools like Spring Boot Actuator or database monitoring tools to track transaction performance and identify bottlenecks. ConclusionSpring’s @Transactional annotation provides a powerful way to manage transactions declaratively. By understanding attributes like propagation, isolation, rollback, read-only, and timeout, developers can fine-tune transaction management to optimize performance and consistency.Key Takeaways Use propagation to control how transactions interact. Choose the right isolation level for data consistency. Define rollback rules for specific exceptions. Mark read-only methods with readOnly = true for performance benefits. Be cautious of self-invocation and proxy limitations. Test and monitor transactional behavior to ensure reliability." }, { "title": "Comprehensive Guide to Caching in Spring Boot with Redis", "url": "/posts/caching-in-springboot-with-redis/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment", "date": "2024-09-04 00:00:00 +0530", "snippet": "IntroductionCaching is a crucial optimization technique that enhances application performance by reducing database load and response time. Spring Boot provides built-in support for caching via the Spring Cache Abstraction, which allows developers to integrate various cache providers, including Redis.In this article, we will cover: Caching in Spring Boot – key interfaces, annotations, and underlying design patterns. Configuration required to enable caching. Different eviction policies supported by Redis in Spring Boot. Default eviction policy used by Redis. Key generation strategy and its default behavior. How cached data is stored in Redis and how to query it via terminal. Data structures supported by Redis and how to use them in Spring. When to use different Redis data types for caching. Real-world use cases and best practices for caching in Spring Boot with Redis.1. Caching in Spring Boot: Interfaces, Annotations &amp; Design PatternSpring Boot provides the Spring Cache Abstraction, which enables caching without binding the application to a specific caching solution.Key Interfaces Used in Spring Boot Caching CacheManager – Manages different cache regions. Cache – Represents an individual cache region. CacheResolver – Determines which cache should be used. CacheWriter – Defines how cache entries are written.Common Caching Annotations and Their Meaning Annotation Purpose @Cacheable(value = \"products\", key = \"#id\") Caches method output. If key exists, returns cached data. @CachePut(value = \"products\", key = \"#id\") Updates cache and ensures method execution. @CacheEvict(value = \"products\", key = \"#id\") Removes a specific cache entry. @Caching Groups multiple caching operations. @CacheConfig Specifies common cache settings at the class level. @EnableCaching Enables Spring’s caching mechanism. Behind the Scenes: Design Pattern UsedSpring Boot’s caching abstraction follows the Proxy Design Pattern. When a method annotated with @Cacheable is called, Spring first checks if the result exists in the cache. If it does, the method execution is skipped, and the cached result is returned. Otherwise, the method executes, and the result is cached.graph TD; A[Method Call] --&gt;|Check Cache| B{Cache Available?}; B -- Yes --&gt; C[Return Cached Data]; B -- No --&gt; D[Execute Method]; D --&gt; E[Store Result in Cache]; E --&gt; C;2. Enabling Caching in Spring BootTo enable caching in a Spring Boot application, follow these steps:Step 1: Add Dependencies (Spring + Redis)Maven (pom.xml):&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;application.properties:spring.redis.host=localhostspring.redis.port=6379Step 2: Enable Caching in Configuration Class@Configuration@EnableCachingpublic class CacheConfig { @Bean @Primary public RedisCacheConfiguration defaultCacheConfig() { StringRedisSerializer keySerializer = new StringRedisSerializer(); Jackson2JsonRedisSerializer&lt;Object&gt; valueSerializer = new Jackson2JsonRedisSerializer&lt;&gt;(Object.class); return RedisCacheConfiguration.defaultCacheConfig() .entryTtl(Duration.ofMinutes(2)) // Set cache expiration .serializeKeysWith(RedisSerializationContext.SerializationPair.fromSerializer(keySerializer)) .serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(valueSerializer)); }}3. Different Eviction Policies in Spring Boot with RedisEviction policies determine how data is removed when memory is full. Spring Boot allows configuring different strategies: Eviction Policy Description LRU (Least Recently Used) Removes least accessed items when memory is full. LFU (Least Frequently Used) Removes least frequently used items first. FIFO (First In, First Out) Evicts oldest stored items first. TTL-Based Eviction Removes data after a set time (Time-To-Live). 4. Default Eviction Policy in RedisBy default, Redis uses noeviction for eviction. To check the policy:127.0.0.1:6379&gt; CONFIG GET maxmemory-policy1) \"maxmemory-policy\"2) \"noeviction\"To change the policy:127.0.0.1:6379&gt; CONFIG SET maxmemory-policy allkeys-lruOK127.0.0.1:6379&gt; CONFIG GET maxmemory-policy1) \"maxmemory-policy\"2) \"allkeys-lru\"5. Key Generation Strategy in Spring BootSpring Boot uses a Simple Key Generator by default, which constructs keys as follows: Single argument: Uses the argument value as the key. Multiple arguments: Uses an array containing all arguments. No arguments: Uses a default key SimpleKey.EMPTY.Example: Custom Key Generator@Beanpublic KeyGenerator customKeyGenerator() { return (target, method, params) -&gt; method.getName() + \"::\" + Arrays.toString(params);}6. Using String and Hash Data Types in Redis with SpringRedis provides multiple data structures for caching, with String and Hash being two of the most commonly used. String is often used for simple key-value caching, while Hash is beneficial for storing structured objects and updating specific fields efficiently.Using String Data Type with AnnotationsThe @Cacheable annotation in Spring enables automatic caching of method return values. The following example demonstrates how to cache product details using the String data type in Redis:Product Service Implementation@Servicepublic class ProductService { private final ProductRepository productRepository; public ProductService(final ProductRepository productRepository) { this.productRepository = productRepository; } @Cacheable(value = \"productCache\", key = \"#id\") public ProductDto get(final Long id) { return productRepository.findById(id) .map(product -&gt; mapToDTO(product, new ProductDto())) .orElseThrow(NotFoundException::new); }}Product Controller@CrossOrigin(origins = \"*\", maxAge = 3600)@RestController@RequestMapping(value = \"/api/products\", produces = MediaType.APPLICATION_JSON_VALUE)public class ProductController { private final ProductService productService; public ProductController(ProductService productService) { this.productService = productService; } @GetMapping(\"/{id}\") public ResponseEntity&lt;ProductDto&gt; getProduct(@PathVariable final Long id) { return ResponseEntity.ok(productService.get(id)); }}Adding a Product to the Databasecurl --request POST \\ --url http://localhost:8080/api/products \\ --header 'Content-Type: application/json' \\ --header 'User-Agent: insomnia/10.2.0' \\ --data '{\t\"id\" : 1,\t\"name\": \"iphone\",\t\"category\": \"phone\",\t\"availableItems\": 12,\t\"price\": \"50000\",\t\"description\": \"2025 model\",\t\"manufacturer\": \"Apple Inc\"}'Retrieving a ProductWhen fetching a product, the service first checks Redis. If the entry is found in the cache, it is returned directly; otherwise, the database is queried, and the result is stored in the cache before returning it.curl --request GET --url http://localhost:8080/api/products/1Checking Redis Cache127.0.0.1:6379&gt; GET productCache::1\"{\\\"id\\\":1,\\\"name\\\":\\\"iphone\\\",\\\"category\\\":\\\"phone\\\",\\\"price\\\":50000.0,\\\"description\\\":\\\"2025 model\\\",\\\"manufacturer\\\":\\\"Apple Inc\\\",\\\"availableItems\\\":12}\"After Expiry127.0.0.1:6379&gt; GET productCache::1(nil)Using Hash Data Type ProgrammaticallyThe Hash data type in Redis is useful for efficiently storing structured objects. It allows updating specific fields without replacing the entire object, making it ideal for frequently updated records.Setting Up RedisTemplate BeanModify CacheConfig.java to include a Redis template: @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory connectionFactory) { RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(connectionFactory); // JSON Serializer Jackson2JsonRedisSerializer&lt;Object&gt; jsonSerializer = new Jackson2JsonRedisSerializer&lt;&gt;(Object.class); // Set Key and HashKey as String template.setKeySerializer(new StringRedisSerializer()); template.setHashKeySerializer(new StringRedisSerializer()); // Value and HashValue as JSON template.setValueSerializer(jsonSerializer); template.setHashValueSerializer(jsonSerializer); template.afterPropertiesSet(); return template; }Modifying ProductService to Store Data as Hash@Servicepublic class ProductService { private final RedisTemplate&lt;String, Object&gt; redisTemplate; private final ObjectMapper objectMapper; private final ProductRepository productRepository; public ProductService(RedisTemplate&lt;String, Object&gt; redisTemplate, ObjectMapper objectMapper, ProductRepository productRepository) { this.redisTemplate = redisTemplate; this.objectMapper = objectMapper; this.productRepository = productRepository; } public ProductDto getModified(final Long id) { HashOperations&lt;String, Object, Object&gt; hashOps = redisTemplate.opsForHash(); Map&lt;Object, Object&gt; productMap = hashOps.entries(\"product:\" + id); if (productMap.isEmpty()) { ProductDto productDto = productRepository.findById(id) .map(product -&gt; mapToDTO(product, new ProductDto())) .orElseThrow(NotFoundException::new); cacheProduct(productDto); return productDto; } return objectMapper.convertValue(productMap, ProductDto.class); } public Long create(final ProductDto productDTO) { final Product product = new Product(); mapToEntity(productDTO, product); Product saved = productRepository.save(product); cacheProduct(productDTO); return saved.getId(); } private void cacheProduct(ProductDto product) { HashOperations&lt;String, Object, Object&gt; hashOps = redisTemplate.opsForHash(); Map&lt;String, Object&gt; productMap = objectMapper.convertValue(product, Map.class); hashOps.putAll(\"product:\" + product.getId(), productMap); }}Updating Stock Availability in Cache Using Order ServiceTo keep stock availability updated, an OrderService modifies only the availableItems field in the Redis cache without rewriting the entire product object.@Servicepublic class OrderService { private final RedisTemplate&lt;String, Object&gt; redisTemplate; private final OrderRepository orderRepository; private final ProductRepository productRepository; public OrderService(RedisTemplate&lt;String, Object&gt; redisTemplate, final OrderRepository orderRepository, final UserRepository userRepository, final ProductRepository productRepository) { this.redisTemplate = redisTemplate; this.orderRepository = orderRepository; this.productRepository = productRepository; } public OrderDto get(final Long id) { return orderRepository.findById(id) .map(order -&gt; mapToDTO(order, new OrderDto())) .orElseThrow(NotFoundException::new); } public Long create(final OrderDto orderDTO) { final Order order = new Order(); mapToEntity(orderDTO, order); Order saved = orderRepository.save(order); saved.getLineItems().forEach(this::updateProductAvailability); return saved.getId(); } private void updateProductAvailability(LineItem lineItem) { Long productId = lineItem.getProduct().getId(); Integer quantity = lineItem.getQuantity(); HashOperations&lt;String, Object, Object&gt; hashOps = redisTemplate.opsForHash(); Long newStock = hashOps.increment(\"product:\" + productId, \"availableItems\", -quantity); hashOps.put(\"product:\" + productId, \"lastUpdate\", System.currentTimeMillis()); if (newStock &lt; 0) { hashOps.put(\"product:\" + productId, \"availableItems\", 0); // Reset to zero throw new IllegalArgumentException(\"Stock is insufficient!\"); } }}Final Redis StructureAfter Adding a Product127.0.0.1:6379&gt; HGETALL product:1 1) \"id\" 2) \"1\" 3) \"name\" 4) \"\\\"iphone\\\"\" 5) \"category\" 6) \"\\\"phone\\\"\" 7) \"price\" 8) \"50000.0\" 9) \"description\"10) \"\\\"2025 model\\\"\"11) \"manufacturer\"12) \"\\\"Apple Inc\\\"\"13) \"availableItems\"14) \"12\"15) \"lineItems\"16) \"\"17) \"dateCreated\"18) \"\"19) \"lastUpdated\"20) \"\"127.0.0.1:6379&gt;After Placing an OrderOnce we place order for this product, we are saving in the DB and also updating cache for available count quit as only specific field is updated.API curl:curl --request POST \\ --url http://localhost:8080/api/orders \\ --header 'Content-Type: application/json' \\ --header 'Correlation-Id: ALPHA-123' \\ --header 'User-Agent: insomnia/10.2.0' \\ --data '{\t\"id\" : 1,\t\"userId\": 1,\t\"lineItems\": [\t\t{\t\t\t\"productId\": 1,\t\t\t\"quantity\": 4\t\t}\t]}'Redis Terminal127.0.0.1:6379&gt; HGETALL product:1 1) \"id\" 2) \"1\" 3) \"name\" 4) \"\\\"iphone\\\"\" 5) \"category\" 6) \"\\\"phone\\\"\" 7) \"price\" 8) \"50000.0\" 9) \"description\"10) \"\\\"2025 model\\\"\"11) \"manufacturer\"12) \"\\\"Apple Inc\\\"\"13) \"availableItems\"14) \"8\"15) \"lineItems\"16) \"\"17) \"dateCreated\"18) \"\"19) \"lastUpdated\"20) \"\"21) \"lastUpdate\"22) \"1741976096813\"127.0.0.1:6379&gt;7. Supported Redis Data Structures &amp; Usage in Springgraph TD; A[\"Spring\"] --&gt;|\"Simple Key-Value\"| B[\"@Cacheable\"]; A --&gt;|\"Hash Map\"| C[\"RedisTemplate.opsForHash\"]; A --&gt;|\"List\"| D[\"RedisTemplate.opsForList\"]; A --&gt;|\"Set\"| E[\"RedisTemplate.opsForSet\"]; A --&gt;|\"Sorted Set (ZSet)\"| F[\"RedisTemplate.opsForZSet\"];8. When to Use Which Data Type?String Data Type Use Case: Simple key-value pairs where the value is a single object or a serialized object. Example: Caching user sessions, configuration settings, or individual product details.Hash Data Type Use Case: Storing objects with multiple fields, where each field can be accessed individually. Example: Caching user profiles, where each user has multiple attributes like name, email, and address.List Data Type Use Case: Storing ordered collections of items, where the order of insertion is important. Example: Caching recent activities or logs.Set Data Type Use Case: Storing unique items where the order does not matter. Example: Caching unique tags or categories.Sorted Set (ZSet) Data Type Use Case: Storing ordered collections of items with associated scores, where items can be retrieved based on their scores. Example: Caching leaderboard data or ranked items.9. Real-World Use Cases and Best PracticesReal-World Use Cases E-commerce Applications: Product Catalog: Cache product details to reduce database load during high traffic. Shopping Cart: Use Redis to store temporary shopping cart data. Social Media Platforms: User Feeds: Cache user feeds to improve read performance. Notifications: Store and retrieve user notifications efficiently. Gaming Applications: Leaderboards: Use Redis Sorted Sets to maintain and display leaderboard data. Session Management: Cache user sessions to ensure quick access and scalability. Best Practices Cache Invalidation: Implement proper cache invalidation strategies to ensure data consistency. Use @CacheEvict to remove stale data from the cache. @Servicepublic class ProductService { @CacheEvict(value = \"productCache\", key = \"#id\") public void evictProductById(Long id) { // This method will remove the cached product with the given id }} Cache Expiration: Set appropriate TTL (Time-To-Live) values to automatically expire stale data. Use Redis’ TTL-based eviction policy to manage cache expiration. @Beanpublic RedisCacheManager cacheManager(RedisConnectionFactory factory) { RedisCacheConfiguration config = RedisCacheConfiguration.defaultCacheConfig() .entryTtl(Duration.ofMinutes(10)); // Set TTL to 10 minutes return RedisCacheManager.builder(factory) .cacheDefaults(config) .build();} Monitoring and Logging: Monitor cache hit and miss ratios to optimize cache usage. Log cache operations to diagnose and troubleshoot issues. @Servicepublic class ProductService { private static final Logger logger = LoggerFactory.getLogger(ProductService.class); @Cacheable(value = \"productCache\", key = \"#id\") public Product getProductById(Long id) { logger.info(\"Fetching product from database with id: {}\", id); // Simulate database call return new Product(id, \"Laptop\", 1200); }} Scalability: Use Redis clustering to distribute cache data across multiple nodes for scalability. Implement cache sharding to handle large datasets efficiently. @Beanpublic RedisConnectionFactory redisConnectionFactory() { RedisClusterConfiguration clusterConfig = new RedisClusterConfiguration(); clusterConfig.clusterNode(\"127.0.0.1\", 6379); clusterConfig.clusterNode(\"127.0.0.1\", 6380); return new JedisConnectionFactory(clusterConfig);} ConclusionCaching is a powerful technique to enhance the performance of Spring Boot applications. By leveraging Redis as a cache provider, developers can achieve significant improvements in response times and database load. This guide has covered the essential aspects of caching in Spring Boot with Redis, including configuration, eviction policies, key generation, and data structures. By following best practices and understanding real-world use cases, you can effectively implement caching in your applications to deliver a seamless user experience." }, { "title": "How to Implement Rate Limiting in Spring Boot APIs Using Aspect-Oriented Programming (AOP)", "url": "/posts/rate-limiting-using-spring-aop/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment", "date": "2024-08-01 00:00:00 +0530", "snippet": "IntroductionAspect-Oriented Programming (AOP) is a powerful technique in Spring Boot for separating cross-cutting concerns from the main application logic. One common use case of AOP is implementing rate limiting in APIs, where you restrict the number of requests a client can make within a certain period. In this article, we’ll explore how to leverage AOP to implement rate limiting in Spring Boot APIs, ensuring optimal performance and resource utilization.Table of Contents Understanding Aspect-Oriented Programming (AOP) Implementing Rate Limiting with AOP in Spring Boot Example: Rate Limiting in a Spring Boot API Conclusion1. Understanding Aspect-Oriented Programming (AOP)Aspect-Oriented Programming is a programming paradigm that aims to modularize cross-cutting concerns in software development. Cross-cutting concerns are aspects of a program that affect multiple modules and are difficult to modularize using traditional approaches. Examples include logging, security, and transaction management.AOP introduces the concept of aspects, which encapsulate cross-cutting concerns. Aspects are modular units that can be applied across different parts of the application without modifying the core logic. AOP frameworks, such as Spring AOP, provide mechanisms for defining aspects and applying them to specific join points in the application’s execution flow.Mermaid Diagram: AOP Overviewgraph TD A[Application Logic] --&gt; B[Logging] A --&gt; C[Security] A --&gt; D[Rate Limiting] B --&gt; E[Aspect] C --&gt; E D --&gt; EIn the diagram above, the application logic is separated from cross-cutting concerns like logging, security, and rate limiting. These concerns are encapsulated in aspects, which are applied to the application logic at specific join points.2. Implementing Rate Limiting with AOP in Spring BootRate limiting is a common requirement in web APIs to prevent abuse and ensure fair usage of resources. With AOP in Spring Boot, we can implement rate limiting by intercepting method invocations and enforcing restrictions on the number of requests allowed within a certain time frame.To implement rate limiting with AOP in Spring Boot, we typically follow these steps: Define a custom annotation to mark methods that should be rate-limited. Create an aspect class that intercepts method invocations annotated with the custom annotation. Use a rate limiter component to track and enforce rate limits. Handle rate limit exceeded scenarios gracefully, such as by throwing a custom exception.Rate Limiting FlowsequenceDiagram participant Client participant Controller participant Aspect participant RateLimiter Client-&gt;&gt;Controller: Request Controller-&gt;&gt;Aspect: Intercept Method Aspect-&gt;&gt;RateLimiter: Check Rate Limit RateLimiter--&gt;&gt;Aspect: Allow/Deny alt Rate Limit Not Exceeded Aspect-&gt;&gt;Controller: Proceed Controller--&gt;&gt;Client: Response else Rate Limit Exceeded Aspect-&gt;&gt;Client: Throw RateLimitException endIn the diagram above, the client makes a request to the controller. The aspect intercepts the method call and checks the rate limit using the rate limiter component. If the rate limit is not exceeded, the request proceeds to the controller. If the rate limit is exceeded, the aspect throws a RateLimitException.3. Example: Rate Limiting in a Spring Boot APIImplementing rate limiting in a Spring Boot API can be achieved using various techniques. One common approach is to use Spring AOP (Aspect-Oriented Programming) to intercept incoming requests and enforce rate limits.Step 1 - Define Rate Limiting ConfigurationAdd application properties and create a configuration class where you define the rate limit parameters such as the number of requests allowed and the time period.rate.limit.requests=10rate.limit.seconds=60@Component@ConfigurationProperties(prefix = \"rate.limit\")@Getter@Setterpublic class RateLimitConfig { private int requests; private int seconds;}Step 2 - Define RateLimited AnnotationCreate a custom annotation to mark the methods that should be rate-limited.@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface RateLimited { }Step 3 — Create a Rate Limiting AspectImplement an aspect using Spring AOP to intercept method calls and enforce rate limits.@Aspect@Component@RequiredArgsConstructorpublic class RateLimitAspect { private final RateLimiter rateLimiter; @Around(\"@annotation(RateLimited)\") public Object enforceRateLimit(ProceedingJoinPoint joinPoint) throws Throwable { String key = getIpAddress(); if (!rateLimiter.tryAcquire(key)) { throw new RateLimitException(\"Rate limit exceeded\"); } return joinPoint.proceed(); } private String getIpAddress() { ServletRequestAttributes requestAttributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = requestAttributes.getRequest(); return request.getRemoteAddr(); }}Step 4 — Implement Rate LimiterCreate a rate limiter component to manage rate limits using a token bucket algorithm or any other suitable algorithm.@Component@RequiredArgsConstructorpublic class RateLimiter { private final RateLimitConfig rateLimitConfig; private final Map&lt;String, RateLimitedSemaphore&gt; semaphores = new ConcurrentHashMap&lt;&gt;(); public boolean tryAcquire(String key) { long currentTime = System.currentTimeMillis(); long startTime = getStartTime(currentTime); cleanupExpiredEntries(startTime); return isRequestAllowed(key, currentTime); } private long getStartTime(long currentTime) { return currentTime - rateLimitConfig.getSeconds() * 1000L; } private boolean isRequestAllowed(String key, long currentTime) { RateLimitedSemaphore semaphore = getRateLimitedSemaphore(key, currentTime); boolean acquired = semaphore.tryAcquire(); if (acquired) { semaphore.setLastAcquireTime(currentTime); } return acquired; } private RateLimitedSemaphore getRateLimitedSemaphore(String key, long currentTime) { return semaphores.computeIfAbsent(key, k -&gt; { RateLimitedSemaphore newSemaphore = new RateLimitedSemaphore(rateLimitConfig.getRequests()); newSemaphore.setLastAcquireTime(currentTime); return newSemaphore; }); } private void cleanupExpiredEntries(long startTime) { Iterator&lt;Map.Entry&lt;String, RateLimitedSemaphore&gt;&gt; iterator = semaphores.entrySet().iterator(); while (iterator.hasNext()) { Map.Entry&lt;String, RateLimitedSemaphore&gt; entry = iterator.next(); RateLimitedSemaphore semaphore = entry.getValue(); if (semaphore.getLastAcquireTime() &lt; startTime) { iterator.remove(); } } } @Setter @Getter private class RateLimitedSemaphore extends Semaphore { private volatile long lastAcquireTime; public RateLimitedSemaphore(int permits) { super(permits); } }}Step 5 - Define Exception and HandlingDefine a custom exception for rate limit exceeded scenarios and handle it globally.Custom Exception:public class RateLimitException extends RuntimeException { public RateLimitException(String message) { super(message); }}Error Response:@Getter@Builder@AllArgsConstructorpublic class ErrorResponse { private String errorCode; private String message;}Global Exception handler (another usecase of AOP):@RestControllerAdvice@RequestMapping(produces = \"application/json\")public class GlobalExceptionAdvice { @ExceptionHandler(RateLimitException.class) public ResponseEntity&lt;ErrorResponse&gt; handleRateLimitException(Exception ex) { ErrorResponse errorResponse = ErrorResponse.builder() .errorCode(\"ERROR-01\") .message(ex.getMessage()) .build(); return ResponseEntity.status(HttpStatus.TOO_MANY_REQUESTS).body(errorResponse); } @ExceptionHandler(Exception.class) public ResponseEntity&lt;ErrorResponse&gt; handleGenericException(Exception ex) { ErrorResponse errorResponse = ErrorResponse.builder() .message(ex.getMessage()) .errorCode(\"\") .build(); return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(errorResponse); }}Step 6 — Annotate Controller MethodsAnnotate the controller methods that should be rate-limited with @RateLimited.@RestController@RequiredArgsConstructorpublic class FinanceController { private final FinanceClient financeClient; @RateLimited @GetMapping(\"/finance/historical-data\") public FinanceHistoricalData getHistoricalData( @RequestParam String userId, @RequestParam long from, @RequestParam long to) { return financeClient.getHistoricalData(userId, from, to); }}In this example, we define a custom annotation @RateLimited to mark methods that should be rate-limited. We then create an aspect RateLimitAspect that intercepts method invocations annotated with @RateLimited. Inside the aspect, we enforce rate limits using a RateLimiter component.4. ConclusionIn this article, we’ve explored how to implement rate limiting in Spring Boot APIs using Aspect-Oriented Programming (AOP). By separating cross-cutting concerns such as rate limiting from the core application logic, we can ensure better modularity, maintainability, and scalability of our applications. AOP provides a powerful mechanism for addressing such concerns, allowing developers to focus on building robust and efficient APIs.By following the steps outlined in this article and leveraging AOP capabilities in Spring Boot, developers can easily implement rate limiting and other cross-cutting concerns in their applications, leading to more resilient and high-performing APIs.Overall Flowgraph TD A[Client] --&gt; B[API Request] B --&gt; C{Rate Limiting Aspect} C --&gt;|Rate Limit Check| D[RateLimiter] D --&gt;|Limit Not Exceeded| E[Controller Method] D --&gt;|Limit Exceeded| F[Throw RateLimitException] E --&gt; G[Process Request] G --&gt; H[Response to Client] F --&gt; I[Error Response to Client]In the final diagram, the overall flow of the rate limiting process is illustrated. The client makes a request to the controller, which is intercepted by the aspect. The aspect checks the rate limit using the rate limiter and either allows the request to proceed or throws a RateLimitException.#spring-boot #rate-limit #spring-aop" }, { "title": "Part 5: Database Engineering Fundamentals: Write Amplification Problem in PostgreSQL", "url": "/database-engineering-fundamental-part-5/write-amplification-problem-in-postgresql/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-05 11:00:00 +0530", "snippet": "Write Amplification Problem in PostgreSQLWrite Amplification Problem in PostgreSQLThe Write Amplification Problem (WAP) typically arises in systems using Solid State Drives (SSDs) or other storage media that employ techniques like wear leveling and garbage collection. Although PostgreSQL itself doesn’t directly create write amplification, the nature of SSDs and their interaction with PostgreSQL’s I/O behavior can lead to this phenomenon.Here’s a breakdown of how the write amplification problem can manifest and how it relates to PostgreSQL:1. Understanding Write Amplification in SSDs Wear leveling is a technique used by SSDs to evenly distribute writes across the drive, preventing individual memory cells from wearing out too quickly. However, to achieve this, SSDs may need to write data to a new location before erasing or modifying the original location. Write amplification occurs when the amount of data written to the storage device is greater than the amount of actual data that needs to be stored. For example, if PostgreSQL writes 4 KB of data to a block, and the SSD needs to rewrite an entire 64 KB page (including old and new data) due to wear leveling, the actual write to the SSD is 64 KB instead of just 4 KB. This results in a write amplification factor of 16 (64 KB / 4 KB).2. PostgreSQL’s I/O CharacteristicsPostgreSQL interacts with the underlying disk in a variety of ways that can exacerbate write amplification: WAL (Write-Ahead Logging): PostgreSQL uses a WAL to ensure data integrity. Every time a modification occurs in the database, it is first written to the WAL before the actual data files are modified. This process helps to ensure durability but also means multiple writes are required for a single transaction: One write to the WAL. Another write to the data files (e.g., heap files, index files). Autovacuum: PostgreSQL has an autovacuum process to reclaim space from dead tuples (deleted or updated rows). This can result in additional write operations, as the system may need to rewrite pages that were previously modified (due to MVCC, or Multi-Version Concurrency Control). Index Maintenance: Indexes in PostgreSQL require periodic updates to maintain their structure. When data is updated, corresponding changes must be made to the associated indexes, resulting in additional writes. Checkpoints: PostgreSQL writes data from memory to disk during checkpoints. While this ensures that the data is safely written to disk, it can lead to a burst of writes when many dirty pages accumulate in memory.3. How Write Amplification Occurs in PostgreSQL When a transaction modifies a row in a table, the actual change is not just written to the table’s data file. First, it’s recorded in the WAL (Write-Ahead Log). Then, due to PostgreSQL’s MVCC, the old version of the row might remain on disk until it’s cleaned up by autovacuum, and the new version will be written to a different place. In some cases, the page containing modified rows might need to be rewritten, potentially requiring larger I/O operations (e.g., rewriting a 64 KB data page). The garbage collection mechanisms in SSDs then kick in, and the system may need to rewrite or even copy the entire page, even if only a small part of it changed. If your PostgreSQL instance has high transaction rates, frequent index updates, or a large number of small updates, the cumulative effect can cause significant write amplification, where a small database change causes many more writes to be sent to disk than necessary.4. Mitigating Write Amplification in PostgreSQLWhile PostgreSQL itself can’t directly eliminate write amplification, there are several strategies to reduce its impact: Tune Autovacuum: Properly tuning the autovacuum parameters can help reduce unnecessary rewrites of data pages and indexes. This reduces the number of dead tuples, which need to be cleaned up and rewritten. Use Larger Disk Pages: SSDs with larger block sizes (e.g., 16 KB or 64 KB) can reduce the relative write amplification. While PostgreSQL typically uses 8 KB pages internally, it can still benefit from SSDs with larger page sizes. Optimize Checkpoint Settings: Tuning the frequency of checkpoints in PostgreSQL can reduce the burst of writes that occur during each checkpoint. For example, increasing the checkpoint_timeout or reducing checkpoint_completion_target can help distribute writes more evenly. Use SSDs with Higher Endurance: Some SSDs are designed to handle higher write intensities, so using high-endurance SSDs can mitigate the effects of write amplification over time. Database Partitioning: By partitioning tables, PostgreSQL can limit the number of updates to any single partition. This can reduce the frequency of large I/O operations and improve performance.ConclusionIn summary, write amplification in PostgreSQL arises due to its interaction with SSD storage, particularly with mechanisms like WAL, MVCC, autovacuum, and checkpointing. While PostgreSQL itself does not directly create write amplification, its disk I/O patterns can cause SSDs to perform more writes than strictly necessary. Managing database write operations through proper configuration and using SSDs optimized for high write endurance can help mitigate this problem.Back to Parent Page" }, { "title": "Part 5: Database Engineering Fundamentals: Why Uber Engineering Switched from Postgres to MySQL", "url": "/database-engineering-fundamental-part-5/why-uber-engineering-switched-from-postgres-to-mysql/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-05 10:00:00 +0530", "snippet": "Why Uber Engineering Switched from Postgres to MySQL26 July 2016 / GlobalIntroductionThe early architecture of Uber consisted of a monolithic backend application written in Python that used Postgres for data persistence. Since that time, the architecture of Uber has changed significantly, to a model of microservices and new data platforms. Specifically, in many of the cases where we previously used Postgres, we now use Schemaless, a novel database sharding layer built on top of MySQL. In this article, we’ll explore some of the drawbacks we found with Postgres and explain the decision to build Schemaless and other backend services on top of MySQL.The Architecture of PostgresWe encountered many Postgres limitations: Inefficient architecture for writes Inefficient data replication Issues with table corruption Poor replica MVCC support Difficulty upgrading to newer releasesWe’ll look at all of these limitations through an analysis of Postgres’s representation of table and index data on disk, especially when compared to the way MySQL represents the same data with its InnoDB storage engine. Note that the analysis that we present here is primarily based on our experience with the somewhat old Postgres 9.2 release series. To our knowledge, the internal architecture that we discuss in this article has not changed significantly in newer Postgres releases, and the basic design of the on-disk representation in 9.2 hasn’t changed significantly since at least the Postgres 8.3 release (now nearly 10 years old).On-Disk FormatA relational database must perform a few key tasks: Provide insert/update/delete capabilities Provide capabilities for making schema changes Implement a multiversion concurrency control (MVCC) mechanism so that different connections have a transactional view of the data they work withConsidering how all of these features will work together is an essential part of designing how a database represents data on disk.One of the core design aspects of Postgres is immutable row data. These immutable rows are called “tuples” in Postgres parlance. These tuples are uniquely identified by what Postgres calls a ctid. A ctid conceptually represents the on-disk location (i.e., physical disk offset) for a tuple. Multiple ctids can potentially describe a single row (e.g., when multiple versions of the row exist for MVCC purposes, or when old versions of a row have not yet been reclaimed by the autovacuum process). A collection of organized tuples form a table. Tables themselves have indexes, which are organized as data structures (typically B-trees) that map index fields to a ctid payload.Typically, these ctids are transparent to users, but knowing how they work helps you understand the on-disk structure of Postgres tables. To see the current ctid for a row, you can add “ctid” to the column list in a WHERE clause:`uber@[local] uber=&gt; SELECT ctid, * FROM my_table LIMIT 1;-[ RECORD 1 ]——–+—————————— ctid                  (0,1) …other fields here…`To explain the details of the layout, let’s consider an example of a simple users table. For each user, we have an auto-incrementing user ID primary key, the user’s first and last name, and the user’s birth year. We also define a compound secondary index on the user’s full name (first and last name) and another secondary index on the user’s birth year. The DDL to create such a table might be like this:`CREATE TABLE users (   id SERIAL,   first TEXT,   last TEXT,   birth_year INTEGER,   PRIMARY KEY (id)); CREATE INDEX ix_users_first_last ON users (first, last); CREATE INDEX ix_users_birth_year ON users (birth_year);`Note the three indexes in this definition: the primary key index plus the two secondary indexes we defined.For the examples in this article we’ll start with the following data in our table, which consists of a selection of influential historical mathematicians: id first last birth_year 1 Blaise Pascal 1623 2 Gottfried Leibniz 1646 3 Emmy Noether 1882 4 Muhammad al-Khwārizmī 780 5 Alan Turing 1912 6 Srinivasa Ramanujan 1887 7 Ada Lovelace 1815 8 Henri Poincaré 1854 As described earlier, each of these rows implicitly has a unique, opaque ctid. Therefore, we can think of the internal representation of the table like this: ctid id first last birth_year A 1 Blaise Pascal 1623 B 2 Gottfried Leibniz 1646 C 3 Emmy Noether 1882 D 4 Muhammad al-Khwārizmī 780 E 5 Alan Turing 1912 F 6 Srinivasa Ramanujan 1887 G 7 Ada Lovelace 1815 H 8 Henri Poincaré 1854 The primary key index, which maps ids to ctids, is defined like this: id ctid 1 A 2 B 3 C 4 D 5 E 6 F 7 G 8 H The B-tree is defined on the id field, and each node in the B-tree holds the ctid value. Note that in this case, the order of the fields in the B-tree happens to be the same as the order in the table due to the use of an auto-incrementing id, but this doesn’t necessarily need to be the case.The secondary indexes look similar; the main difference is that the fields are stored in a different order, as the B-tree must be organized lexicographically. The (first, last) index starts with first names toward the top of the alphabet: first last ctid Ada Lovelace G Alan Turing E Blaise Pascal A Emmy Noether C Gottfried Leibniz B Henri Poincaré H Muhammad al-Khwārizmī D Srinivasa Ramanujan F Likewise, the birth_year index is clustered in ascending order, like this: birth_year ctid 780 D 1623 A 1646 B 1815 G 1854 H 1887 F 1882 C 1912 E As you can see, in both of these cases the ctid field in the respective secondary index is not increasing lexicographically, unlike in the case of an auto-incrementing primary key.Suppose we need to update a record in this table. For instance, let’s say we’re updating the birth year field for another estimate of al-Khwārizmī’s year of birth, 770 CE. As we mentioned earlier, row tuples are immutable. Therefore, to update the record, we add a new tuple to the table. This new tuple has a new opaque ctid, which we’ll call I . Postgres needs to be able to distinguish the new, active tuple at I from the old tuple at D. Internally, Postgres stores within each tuple a version field and pointer to the previous tuple (if there is one). Accordingly, the new structure of the table looks like this: ctid prev id first last birth_year A null 1 Blaise Pascal 1623 B null 2 Gottfried Leibniz 1646 C null 3 Emmy Noether 1882 D null 4 Muhammad al-Khwārizmī 780 E null 5 Alan Turing 1912 F null 6 Srinivasa Ramanujan 1887 G null 7 Ada Lovelace 1815 H null 8 Henri Poincaré 1854 I D 4 Muhammad al-Khwārizmī 770 As long as two versions of the al-Khwārizmī row exist, the indexes must hold entries for both rows. For brevity, we omit the primary key index and show only the secondary indexes here, which look like this: first last ctid Ada Lovelace G Alan Turing E Blaise Pascal A Emmy Noether C Gottfried Leibniz B Henri Poincaré H Muhammad al-Khwārizmī D Muhammad al-Khwārizmī I Srinivasa Ramanujan F birth_year ctid 770 I 780 D 1623 A 1646 B 1815 G 1854 H 1887 F 1882 C 1912 E We’ve represented the old version in red and the new row version in green. Under the hood, Postgres uses another field holding the row version to determine which tuple is most recent. This added field lets the database determine which row tuple to serve to a transaction that may not be allowed to see the latest row version.With Postgres, the primary index and secondary indexes all point directly to the on-disk tuple offsets. When a tuple location changes, all indexes must be updated.ReplicationWhen we insert a new row into a table, Postgres needs to replicate it if streaming replication is enabled. For crash recovery purposes, the database already maintains a write-ahead log (WAL) and uses it to implement two-phase commit. The database must maintain this WAL even when streaming replication is not enabled because the WAL allows the atomicity and durability aspects of ACID.We can understand the WAL by considering what happens if the database crashes unexpectedly, like during a sudden power loss. The WAL represents a ledger of the changes the database plans to make to the on-disk contents of tables and indexes. When the Postgres daemon first starts up, the process compares the data in this ledger with the actual data on disk. If the ledger contains data that isn’t reflected on disk, the database corrects any tuple or index data to reflect the data indicated by the WAL. It then rolls back any data that appears in the WAL but is from a partially applied transaction (meaning that the transaction was never committed).Postgres implements streaming replication by sending the WAL on the master database to replicas. Each replica database effectively acts as if it’s in crash recovery, constantly applying WAL updates just as it would if it were starting up after a crash. The only difference between streaming replication and actual crash recovery is that replicas in “hot standby” mode serve read queries while applying the streaming WAL, whereas a Postgres database that’s actually in crash recovery mode typically refuses to serve any queries until the database instance finishes the crash recovery process.Because the WAL is actually designed for crash recovery purposes, it contains low-level information about the on-disk updates. The content of the WAL is at the level of the actual on-disk representation of row tuples and their disk offsets (i.e., the row ctids). If you pause a Postgres master and replica when the replica is fully caught up, the actual on-disk content on the replica exactly matches what’s on the master byte for byte. Therefore, tools like rsync can fix a corrupted replica if it gets out of date with the master.Consequences of Postgres’s DesignPostgres’s design resulted in inefficiencies and difficulties for our data at Uber.Write AmplificationThe first problem with Postgres’s design is known in other contexts as write amplification. Typically, write amplification refers to a problem with writing data to SSD disks: a small logical update (say, writing a few bytes) becomes a much larger, costlier update when translated to the physical layer. The same issue arises in Postgres. In our previous example when we made the small logical update to the birth year for al-Khwārizmī, we had to issue at least four physical updates: Write the new row tuple to the tablespace Update the primary key index to add a record for the new tuple Update the (first, last) index to add a record for the new tuple Update the birth_year index to add a record for the new tuple Write the new row tuple to the tablespace Update the primary key index to add a record for the new tuple Update the (first, last) index to add a record for the new tuple Update the birth_year index to add a record for the new tuple In fact, these four updates only reflect the writes made to the main tablespace; each of these writes needs to be reflected in the WAL as well, so the total number of writes on disk is even larger.What’s noteworthy here are updates 2 and 3. When we updated the birth year for al-Khwārizmī, we didn’t actually change his primary key, nor did we change his first and last name. However, these indexes still must be updated with the creation of a new row tuple in the database for the row record. For tables with a large number of secondary indexes, these superfluous steps can cause enormous inefficiencies. For instance, if we have a table with a dozen indexes defined on it, an update to a field that is only covered by a single index must be propagated into all 12 indexes to reflect the ctid for the new row.ReplicationThis write amplification issue naturally translates into the replication layer as well because replication occurs at the level of on-disk changes. Instead of replicating a small logical record, such as “Change the birth year for ctid D to now be 770,” the database instead writes out WAL entries for all four of the writes we just described, and all four of these WAL entries propagate over the network. Thus, the write amplification problem also translates into a replication amplification problem, and the Postgres replication data stream quickly becomes extremely verbose, potentially occupying a large amount of bandwidth.In cases where Postgres replication happens purely within a single data center, the replication bandwidth may not be a problem. Modern networking equipment and switches can handle a large amount of bandwidth, and many hosting providers offer free or cheap intra–data center bandwidth. However, when replication must happen between data centers, issues can quickly escalate. For instance, Uber originally used physical servers in a colocation space on the West Coast. For disaster recovery purposes, we added servers in a second East Coast colocation space. In this design we had a master Postgres instance (plus replicas) in our western data center and a set of replicas in the eastern one.Cascading replication limits the inter–data center bandwidth requirements to the amount of replication required between just the master and a single replica, even if there are many replicas in the second data center. However, the verbosity of the Postgres replication protocol can still cause an overwhelming amount of data for a database that uses a lot of indexes. Purchasing very high bandwidth cross-country links is expensive, and even in cases where money is not an issue it’s simply not possible to get a cross-country networking link with the same bandwidth as a local interconnect. This bandwidth problem also caused issues for us with WAL archival. In addition to sending all of the WAL updates from West Coast to East Coast, we archived all WALs to a file storage web service, both for extra assurance that we could restore data in the event of a disaster and so that archived WALs could bring up new replicas from database snapshots. During peak traffic early on, our bandwidth to the storage web service simply wasn’t fast enough to keep up with the rate at which WALs were being written to it.Data CorruptionDuring a routine master database promotion to increase database capacity, we ran into a Postgres 9.2 bug. Replicas followed timeline switches incorrectly, causing some of them to misapply some WAL records. Because of this bug, some records that should have been marked as inactive by the versioning mechanism weren’t actually marked inactive.The following query illustrates how this bug would affect our users table example:SELECT * FROM users WHERE id = 4;This query would return two records: the original al-Khwārizmī row with the 780 CE birth year, plus the new al-Khwārizmī row with the 770 CE birth year. If we were to add ctid to the WHERE list, we would see different ctid values for the two returned records, as one would expect for two distinct row tuples.This problem was extremely vexing for a few reasons. To start, we couldn’t easily tell how many rows this problem affected. The duplicated results returned from the database caused application logic to fail in a number of cases. We ended up adding defensive programming statements to detect the situation for tables known to have this problem. Because the bug affected all of the servers, the corrupted rows were different on different replica instances, meaning that on one replica row X might be bad and row Y would be good, but on another replica row X might be good and row Y might be bad. In fact, we were unsure about the number of replicas with corrupted data and about whether the problem had affected the master.From what we could tell, the problem only manifested on a few rows per database, but we were extremely worried that, because replication happens at the physical level, we could end up completely corrupting our database indexes. An essential aspect of B-trees are that they must be periodically rebalanced, and these rebalancing operations can completely change the structure of the tree as sub-trees are moved to new on-disk locations. If the wrong data is moved, this can cause large parts of the tree to become completely invalid.In the end, we were able to track down the actual bug and use it to determine that the newly promoted master did not have any corrupted rows. We fixed the corruption issue on the replicas by resyncing all of them from a new snapshot of the master, a laborious process; we only had enough capacity to take a few replicas out of the load balancing pool at a time.The bug we ran into only affected certain releases of Postgres 9.2 and has been fixed for a long time now. However, we still find it worrisome that this class of bug can happen at all. A new version of Postgres could be released at any time that has a bug of this nature, and because of the way replication works, this issue has the potential to spread into all of the databases in a replication hierarchy.Replica MVCCPostgres does not have true replica MVCC support. The fact that replicas apply WAL updates results in them having a copy of on-disk data identical to the master at any given point in time. This design poses a problem for Uber.Postgres needs to maintain a copy of old row versions for MVCC. If a streaming replica has an open transaction, updates to the database are blocked if they affect rows held open by the transaction. In this situation, Postgres pauses the WAL application thread until the transaction has ended. This is problematic if the transaction takes a long amount of time, since the replica can severely lag behind the master. Therefore, Postgres applies a timeout in such situations: if a transaction blocks the WAL application for a set amount of time, Postgres kills that transaction.This design means that replicas can routinely lag seconds behind master, and therefore it is easy to write code that results in killed transactions. This problem might not be apparent to application developers writing code that obscures where transactions start and end. For instance, say a developer has some code that has to email a receipt to a user. Depending on how it’s written, the code may implicitly have a database transaction that’s held open until after the email finishes sending. While it’s always bad form to let your code hold open database transactions while performing unrelated blocking I/O, the reality is that most engineers are not database experts and may not always understand this problem, especially when using an ORM that obscures low-level details like open transactions.Postgres UpgradesBecause replication records work at the physical level, it’s not possible to replicate data between different general availability releases of Postgres. A master database running Postgres 9.3 cannot replicate to a replica running Postgres 9.2, nor can a master running 9.2 replicate to a replica running Postgres 9.3.We followed these steps to upgrade from one Postgres GA release to another: Shut down the master database. Run a command called pg_upgrade on the master, which updates the master data in place. This can easily take many hours for a large database, and no traffic can be served from the master while this process takes place. Start the master again. Create a new snapshot of the master. This step completely copies all data from the master, so it also takes many hours for a large database. Wipe each replica and restore the new snapshot from the master to the replica. Bring each replica back into the replication hierarchy. Wait for the replica to fully catch up to all updates applied by the master while the replica was being restored.We started out with Postgres 9.1 and successfully completed the upgrade process to move to Postgres 9.2. However, the process took so many hours that we couldn’t afford to do the process again. By the time Postgres 9.3 came out, Uber’s growth increased our dataset substantially, so the upgrade would have been even lengthier. For this reason, our legacy Postgres instances run Postgres 9.2 to this day, even though the current Postgres GA release is 9.5.If you are running Postgres 9.4 or later, you could use something like pglogical, which implements a logical replication layer for Postgres. Using pglogical, you can replicate data among different Postgres releases, meaning that it’s possible to do an upgrade such as 9.4 to 9.5 without incurring significant downtime. This capability is still problematic because it’s not integrated into the Postgres mainline tree, and pglogical is still not an option for people running on older Postgres releases.The Architecture of MySQLIn addition to explaining some of Postgres’s limitations, we also explain why MySQL is an important tool for newer Uber Engineering storage projects, such as Schemaless. In many cases, we found MySQL more favorable for our uses. To understand the differences, we examine MySQL’s architecture and how it contrasts with that of Postgres. We specifically analyze how MySQL works with the InnoDB storage engine. Not only do we use InnoDB at Uber; it’s perhaps the most popular MySQL storage engine.InnoDB On-Disk RepresentationLike Postgres, InnoDB supports advanced features like MVCC and mutable data. An exhaustive discussion of InnoDB’s on-disk format is outside the scope of this article; instead, we’ll focus on its core differences from Postgres.The most important architectural difference is that while Postgres directly maps index records to on-disk locations, InnoDB maintains a secondary structure. Instead of holding a pointer to the on-disk row location (like the ctid does in Postgres), InnoDB secondary index records hold a pointer to the primary key value. Thus, a secondary index in MySQL associates index keys with primary keys: first last id (primary key) Ada Lovelace 7 Alan Turing 5 Blaise Pascal 1 Emmy Noether 3 Gottfried Leibniz 2 Henri Poincaré 8 Muhammad al-Khwārizmī 4 Srinivasa Ramanujan 6 In order to perform an index lookup on the (first, last) index, we actually need to do two lookups. The first lookup searches the table and finds the primary key for a record. Once the primary key is found, a second lookup searches the primary key index to find the on-disk location for the row.This design means that InnoDB is at a slight disadvantage to Postgres when doing a secondary key lookup, since two indexes must be searched with InnoDB compared to just one for Postgres. However, because the data is normalized, row updates only need to update index records that are actually changed by the row update. Additionally, InnoDB typically does row updates in place. If old transactions need to reference a row for the purposes of MVCC MySQL copies the old row into a special area called the rollback segment.Let’s follow what happens when we update al-Khwārizmī’s birth year. If there is space, the birth year field in the row with id 4 is updated in place (in fact, this update always happens in place, as the birth year is an integer that occupies a fixed amount of space). The birth year index is also updated in place to reflect the new date. The old row data is copied to the rollback segment. The primary key index does not need to be updated, nor does the (first, last) name index. If we have a large number of indexes on this table, we still only have to update the indexes that actually index over the birth_year field. So say we have indexes over fields like signup_date, last_login_time, etc. We don’t need to update these indexes, whereas Postgres would have to.This design also makes vacuuming and compaction more efficient. All of the rows that are eligible to be vacuumed are available directly in the rollback segment. By comparison, the Postgres autovacuum process has to do full table scans to identify deleted rows.MySQL uses an extra layer of indirection: secondary index records point to primary index records, and the primary index itself holds the on-disk row locations. If a row offset changes, only the primary index needs to be updated.ReplicationMySQL supports multiple different replication modes: Statement-based replication replicates logical SQL statements (e.g., it would literally replicate literal statements such as: UPDATE users SET birth_year=770 WHERE id = 4) Row-based replication replicates altered row records Mixed replication mixes these two modesThere are various tradeoffs to these modes. Statement-based replication is usually the most compact but can require replicas to apply expensive statements to update small amounts of data. On the other hand, row-based replication, akin to the Postgres WAL replication, is more verbose but results in more predictable and efficient updates on the replicas.In MySQL, only the primary index has a pointer to the on-disk offsets of rows. This has an important consequence when it comes to replication. The MySQL replication stream only needs to contain information about logical updates to rows. The replication updates are of the variety “Change the timestamp for row X from T_1 to T_2.” Replicas automatically infer any index changes that need to be made as the result of these statements.By contrast, the Postgres replication stream contains physical changes, such as “At disk offset 8,382,491, write bytes XYZ.” With Postgres, every physical change made to the disk needs to be included in the WAL stream. Small logical changes (such as updating a timestamp) necessitate many on-disk changes: Postgres must insert the new tuple and update all indexes to point to that tuple. Thus, many changes will be put into the WAL stream. This design difference means that the MySQL replication binary log is significantly more compact than the PostgreSQL WAL stream.How each replication stream works also has an important consequence on how MVCC works with replicas. Since the MySQL replication stream has logical updates, replicas can have true MVCC semantics; therefore read queries on replicas won’t block the replication stream. By contrast, the Postgres WAL stream contains physical on-disk changes, so Postgres replicas cannot apply replication updates that conflict with read queries, so they can’t implement MVCC.MySQL’s replication architecture means that if bugs do cause table corruption, the problem is unlikely to cause a catastrophic failure. Replication happens at the logical layer, so an operation like rebalancing a B-tree can never cause an index to become corrupted. A typical MySQL replication issue is the case of a statement being skipped (or, less frequently, applied twice). This may cause data to be missing or invalid, but it won’t cause a database outage.Finally, MySQL’s replication architecture makes it trivial to replicate between different MySQL releases. MySQL only increments its version if the replication format changes, which is unusual between various MySQL releases. MySQL’s logical replication format also means that on-disk changes in the storage engine layer do not affect the replication format. The typical way to do a MySQL upgrade is to apply the update to one replica at a time, and once you update all replicas, you promote one of them to become the new master. This can be done with almost zero downtime, and it simplifies keeping MySQL up to date.Other MySQL Design AdvantagesSo far, we’ve focused on the on-disk architecture for Postgres and MySQL. Some other important aspects of MySQL’s architecture cause it to perform significantly better than Postgres, as well.The Buffer PoolFirst, caching works differently in the two databases. Postgres allocates some memory for internal caches, but these caches are typically small compared to the total amount of memory on a machine. To increase performance, Postgres allows the kernel to automatically cache recently accessed disk data via the page cache. For instance, our largest Postgres replicas have 768 GB of memory available, but only about 25 GB of that memory is actually RSS memory faulted in by Postgres processes. This leaves more than 700 GB of memory free to the Linux page cache.The problem with this design is that accessing data via the page cache is actually somewhat expensive compared to accessing RSS memory. To look up data from disk, the Postgres process issues lseek(2) and read(2) system calls to locate the data. Each of these system calls incurs a context switch, which is more expensive than accessing data from main memory. In fact, Postgres isn’t even fully optimized in this regard: Postgres doesn’t make use of the pread(2) system call, which coalesces seek + read operations into a single system call.By comparison, the InnoDB storage engine implements its own LRU in something it calls the InnoDB buffer pool. This is logically similar to the Linux page cache but implemented in userspace. While significantly more complicated than Postgres’s design, the InnoDB buffer pool design has some huge upsides: It makes it possible to implement a custom LRU design. For instance, it’s possible to detect pathological access patterns that would blow out the LRU and prevent them from doing too much damage. It results in fewer context switches. Data accessed via the InnoDB buffer pool doesn’t require any user/kernel context switches. The worst case behavior is the occurrence of a TLB miss, which is relatively cheap and can be minimized by using huge pages. It makes it possible to implement a custom LRU design. For instance, it’s possible to detect pathological access patterns that would blow out the LRU and prevent them from doing too much damage. It results in fewer context switches. Data accessed via the InnoDB buffer pool doesn’t require any user/kernel context switches. The worst case behavior is the occurrence of a TLB miss, which is relatively cheap and can be minimized by using huge pages. Connection HandlingMySQL implements concurrent connections by spawning a thread-per-connection. This is relatively low overhead; each thread has some memory overhead for stack space, plus some memory allocated on the heap for connection-specific buffers. It’s not uncommon to scale MySQL to 10,000 or so concurrent connections, and in fact we are close to this connection count on some of our MySQL instances today.Postgres, however, use a process-per-connection design. This is significantly more expensive than a thread-per-connection design for a number of reasons. Forking a new process occupies more memory than spawning a new thread. Additionally, IPC is much more expensive between processes than between threads. Postgres 9.2 uses System V IPC primitives for IPC instead of lightweight futexes when using threads. Futexes are faster than System V IPC because in the common case where the futex is uncontended, there’s no need to make a context switch.Beside the memory and IPC overhead associated with Postgres’s design, Postgres seems to simply have poor support for handling large connection counts, even when there is sufficient memory available. We’ve had significant problems scaling Postgres past a few hundred active connections. While the documentation is not very specific about why, it does strongly recommend employing an out-of-process connection pooling mechanism to scale to large connection counts with Postgres. Accordingly, using pgbouncer to do connection pooling with Postgres has been generally successful for us. However, we have had occasional application bugs in our backend services that caused them to open more active connections (usually “idle in transaction” connections) than the services ought to be using, and these bugs have caused extended downtimes for us.ConclusionPostgres served us well in the early days of Uber, but we ran into significant problems scaling Postgres with our growth. Today, we have some legacy Postgres instances, but the bulk of our databases are either built on top of MySQL (typically using our Schemaless layer) or, in some specialized cases, NoSQL databases like Cassandra. We are generally quite happy with MySQL, and we may have more blog articles in the future explaining some of its more advanced uses at Uber.Evan Klitzke is a staff software engineer within Uber Engineering‘s core infrastructure group. He is also a database enthusiast and joined Uber as an engineering early bird in September 2012.https://www.uber.com/en-IN/blog/postgres-to-mysql-migration/https://youtu.be/_E43l5EbNI4?si=q9vWT8HjV5NT-M8lBack to Parent Page" }, { "title": "Part 5: Database Engineering Fundamentals: WAL, Redo and undo logs in postgres", "url": "/database-engineering-fundamental-part-5/wal-redo-and-undo-logs-in-postgres/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-05 09:00:00 +0530", "snippet": "WAL, Redo and undo logs in postgresIn PostgreSQL, Write-Ahead Logging (WAL), Redo logs, and Undo logs are crucial components for maintaining data integrity, supporting crash recovery, and providing consistency in database operations. Let’s break these concepts down in detail.1. Write-Ahead Logging (WAL)Write-Ahead Logging (WAL) is a fundamental mechanism in PostgreSQL to ensure data durability, consistency, and crash recovery. The core idea behind WAL is that before any changes are written to the actual data files (e.g., tables, indexes), those changes must first be written to a special log file (the WAL log). This guarantees that even if the system crashes before the changes are fully written to disk, the changes can still be recovered.Key Aspects of WAL: Durability: Once a change is logged in the WAL, PostgreSQL guarantees that it will be applied to the database, even in the event of a crash. This is part of the ACID properties of a transaction (Atomicity, Consistency, Isolation, Durability). Log Structure: The WAL log records all changes made to the database at the level of individual operations. Each WAL entry contains enough information to reconstruct the changes made to a database record (e.g., inserting a row, updating a value, etc.). Sequential Write Optimization: WAL logs are written sequentially to a log file, which makes writes faster because random disk I/O (which is common when modifying data pages) is minimized. PostgreSQL writes data pages to disk asynchronously and in the background, while the WAL log ensures that any change can be recovered. WAL Files: WAL logs are stored in segments, and these segments are named with an identifier based on the timeline of the transaction. Once the WAL log is written to disk, it’s referred to as a WAL “segment.” These segments can be archived for long-term recovery purposes. Log Shipping/Replication: WAL logs are the key to implementing replication in PostgreSQL. Changes recorded in the WAL are streamed to replicas in real-time, allowing replicas to keep up with the primary server and ensure they have the latest state of the database.Basic Workflow: A transaction starts. Before any changes to the database are made (e.g., modifying a table row), the changes are first written to the WAL log. The actual database file is updated in the background, but the WAL log entry ensures that the change can be recovered if necessary. After the changes are written to both the WAL and the data files, PostgreSQL acknowledges that the transaction is complete (commit).2. Redo Logs in PostgreSQLThe redo log is part of the WAL mechanism. It is the sequence of log entries that records changes that need to be re-applied (or “redone”) to the database during recovery in case of a crash.When PostgreSQL starts after a crash, it will replay the redo log to bring the database back to the last consistent state. The redo log entries store information about every committed transaction, which means that if the system crashes after a transaction is committed but before the changes are fully written to the data files, the redo log will ensure the changes are applied to the database files once PostgreSQL restarts.Key Points about Redo Logs: The redo log contains the necessary information to re-apply changes that have been made to the database, i.e., all committed changes. The redo log is sequential, meaning that PostgreSQL can replay the logs in the order they were written to ensure a consistent state. Redo logs are a key part of crash recovery. After a crash, the recovery process replays the logs from the last consistent state to the most recent transaction to bring the system back up.The redo process involves reading the WAL logs that were written since the last checkpoint and applying them to the database files.Example: A transaction inserts a row into a table. The WAL log records that the insert operation has occurred. If the system crashes before the data is written to disk, PostgreSQL can use the redo log to replay that insert operation when it restarts.3. Undo Logs in PostgreSQLIn PostgreSQL, Undo logs (which are also often associated with rollback operations) are not explicitly used in the same way they are in some other database systems. Unlike databases that use undo logs to roll back transactions (like Oracle or MySQL’s InnoDB), PostgreSQL typically uses a multi-version concurrency control (MVCC) system to handle transaction isolation and rollbacks.However, PostgreSQL does handle transaction rollbacks and guarantees that all changes made during a transaction can be undone, but this is achieved through its MVCC mechanism, rather than by explicitly writing undo logs.MVCC Overview: PostgreSQL maintains multiple versions of a row. When a transaction modifies a row, the old version of the row is preserved, allowing other transactions to see the previous state (depending on isolation level). If a transaction is rolled back, the database doesn’t need an undo log. Instead, it simply removes any versions of rows created by the transaction and allows the original versions to be accessed.This means that when a transaction is aborted or rolled back, PostgreSQL undoes the changes by discarding the changes made in the current transaction. Since each version of a row is tracked by the transaction ID and the system handles this with visibility rules, there is no need for traditional undo logs as seen in other databases.Example of MVCC: A transaction updates a row. The update is written as a new version of the row, and the old version is kept. If the transaction commits, the new version of the row becomes visible to other transactions. If the transaction is rolled back, the new version is discarded, and the old version becomes visible again.Recovery ProcessIn PostgreSQL, the recovery process after a crash involves the following steps: Start the Database: PostgreSQL begins by checking the last checkpoint in the WAL logs. A checkpoint is a point at which all data changes are guaranteed to have been written to disk. Replay WAL (Redo Logs): After identifying the last checkpoint, PostgreSQL replays the WAL logs since that checkpoint to ensure that all committed transactions are applied to the database, even if they were not yet written to disk at the time of the crash. Undo Uncommitted Transactions: Any uncommitted transactions at the time of the crash will be discarded (this is handled by the MVCC system, not undo logs). Recovery Complete: Once the redo and undo processes are completed, the database is back in a consistent state.Conclusion WAL ensures durability and crash recovery by logging every change made to the database before it is written to disk. Redo logs (part of WAL) ensure that committed changes can be reapplied during recovery. PostgreSQL does not explicitly use undo logs but uses its MVCC system to handle transaction rollbacks and concurrency control, maintaining old versions of rows instead of needing undo logs.The combination of these mechanisms allows PostgreSQL to guarantee ACID properties, ensuring that data remains consistent and durable even in the event of crashes.Back to Parent Page" }, { "title": "Part 5: Database Engineering Fundamentals: TOAST table in Postgres", "url": "/database-engineering-fundamental-part-5/toast-table-in-postgres/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-05 08:00:00 +0530", "snippet": "TOAST table in PostgresTOAST Table in PostgreSQLTOAST (The Oversized-Attribute Storage Technique) is a mechanism used by PostgreSQL to efficiently store large data types (such as large text fields, byte arrays, or large objects) that do not fit within the regular 8 KB page size for a table. PostgreSQL, by default, uses 8 KB blocks (or pages) for storing table data. If a column’s value exceeds this size, PostgreSQL uses TOAST tables to store the large data outside of the regular table, ensuring that performance isn’t compromised.Why Do We Need TOAST? Limitations of the 8 KB Page Size: PostgreSQL organizes data in fixed-size pages (typically 8 KB). If a single column’s data exceeds this limit, storing it directly in the main table would either waste space or require excessive storage and memory. Efficient Handling of Large Data: Instead of storing large values directly in the main table, PostgreSQL stores them in a TOAST table. This method helps manage large data types, like: TEXT or VARCHAR columns with very long strings. BYTEA columns that store binary data (e.g., images, files). Large Object (LOB) data types. How Does TOAST Work? TOAST Storage: Each table that contains large data types will have an associated TOAST table. These tables are automatically created by PostgreSQL when needed. The TOAST table stores the large values for the main table in a separate storage area. PostgreSQL uses out-of-line storage for values that exceed a certain threshold (about 2 KB by default). Smaller values are stored directly in the regular table page, while larger ones are stored in TOAST tables. Compression: When PostgreSQL detects that a column contains large data, it may use compression (if configured) to reduce the size of the data before storing it in the TOAST table. This helps save storage space. Common compression algorithms used include PGLZ and LZ4. Compression is especially useful for text or data that can be highly compressed (e.g., repetitive strings). Out-of-Line Storage: If the data exceeds a certain threshold (around 2 KB by default), PostgreSQL stores the large column’s data out-of-line, i.e., outside the main table, in the TOAST table. The main table stores a pointer to the location of the data in the TOAST table. This allows PostgreSQL to keep the main table’s pages small, ensuring that regular queries can still operate efficiently, even with large data types present. Chunking: Large values are split into chunks (usually 2 KB each) and stored across multiple pages in the TOAST table. This ensures that PostgreSQL doesn’t need to store excessively large values on a single page, which could be inefficient. When a value is requested, PostgreSQL reconstructs it by reading these chunks from the TOAST table. TOAST Table StructureA TOAST table is not something a user normally interacts with directly. However, you can observe its structure and use it indirectly in PostgreSQL: Automatic Creation: PostgreSQL automatically creates TOAST tables for any regular table that has large data columns. For example, if a table has a TEXT or BYTEA column with large values, PostgreSQL creates a TOAST table behind the scenes. TOAST Tables Naming Convention: The TOAST table is automatically named based on the original table name. For example, if you have a table named my_table, the associated TOAST table might be named pg_toast.pg_toast_&lt;oid_of_my_table&gt;. Here, &lt;oid_of_my_table&gt; refers to the internal object ID of the original table. Columns in TOAST Table:A typical TOAST table has three columns: chunk_id: A unique identifier for each chunk of the large object. chunk_data: The actual data for each chunk (compressed and stored in a binary format). main_table_row_id: A reference (foreign key) to the corresponding row in the main table. TOAST Table ExampleIf you have a table that stores large text data:CREATE TABLE my_table ( id serial primary key, large_text TEXT);If the large_text column contains data larger than 2 KB, PostgreSQL will automatically move the large_text data into a TOAST table, and the my_table will store a reference to the data’s location in the TOAST table. You won’t have to manually interact with the TOAST table—PostgreSQL handles the process for you.Viewing the TOAST TableWhile you usually don’t interact with TOAST tables directly, you can inspect the TOAST table if necessary using PostgreSQL system catalogs. Here’s an example of how you can find a TOAST table associated with a regular table:SELECT t.relname AS toast_tableFROM pg_class t JOIN pg_attribute a ON a.attrelid = t.oidWHERE t.relkind = 'r' -- regular table AND a.attname = 'large_text'; -- The column name that uses TOASTThis query will return the name of the TOAST table for the column large_text.Managing TOAST Disabling TOAST for Specific Columns: PostgreSQL allows you to configure whether certain columns use TOAST or not, by using the storage attribute: CREATE TABLE my_table ( id serial primary key, large_text TEXT STORAGE EXTERNAL ); The STORAGE parameter can be: PLAIN: No TOAST, store directly in the main table. EXTERNAL: Store large data externally (use TOAST). MAIN: Store small values directly on the main table page (default). By default, PostgreSQL uses EXTERNAL for large data types, but you can tweak it if necessary. TOAST Compression: You can adjust compression settings for TOAST using pg_catalog.pglz_compress or other methods for fine-tuning performance. Summary TOAST is PostgreSQL’s internal mechanism to handle large values in TEXT, BYTEA, or other large object data types. Large values are stored outside the main table, typically in a separate TOAST table. PostgreSQL uses compression and chunking to minimize the storage overhead of large values. TOAST improves the performance of database queries by keeping regular table pages smaller and efficient.Thus, TOAST helps PostgreSQL efficiently manage storage and access for large data types without sacrificing performance.Back to Parent Page" }, { "title": "Part 5: Database Engineering Fundamentals: PostgreSQL Process Architecture", "url": "/database-engineering-fundamental-part-5/postgresql-process-architecture/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-05 07:00:00 +0530", "snippet": "PostgreSQL Process ArchitecturePostgreSQL Process ArchitectureCreating a listener on the backend application that accepts connections is simple. You listen on an address-port pair, connection attempts…Hussein NasserFollowCreating a listener on the backend application that accepts connections is simple. You listen on an address-port pair, connection attempts to that address and port will get added to an accept queue; The application accepts connections from the queue and start reading the data stream sent on the connection.However, what part of your application does the accepting and what part does the reading and what part does the execution? You can architect your application in many ways based on your use cases. I have a medium post just exploring the different options, you may read the story here.In this post I explore the PostgreSQL process architecture in details. Please note that the information here is derived from both the Postgres doc and code. Discussions about scalability and performance are solely based on my opinions.Postmaster ProcessThis is the main process that manages everything in Postgres. It creates a listener on the configured interfaces and port (default 5432). It is also responsible for forking other processes to do various tasks.Backend ProcessThe postmaster process creates a new “backend” process for every connection it accepts. The connection is then handed over to the new backend process to perform the reading of the TCP stream, request parsing, SQL query parsing (yes those are different), planning, execution and returning the results. The process uses its local virtual memory for sorting and parsing logic, this memory is controlled by the work_mem parameter.The more connections the postmaster accepts the more backend processes are created. The number of user connections is directly proportional to the number of processes which means more resources, memory, CPU usage and context switching. The benefits of course, each process enjoys a dedicated virtual memory space isolated from other processes, so it is great for security especially that each connection is made to a single database.The problem with this architecture is scalability. With limited CPU cores, how can Postgres scale to thousands or tens of thousands of client connections on a single CPU? The context switching alone and the competition between all the dedicated backend processes for CPU time will cause processes to starve each other. Worth mentioning while some part of query execution will require the CPU, most of the asynchronous I/O to read and write to buffers/memory and won’t involve the CPU.Postgres knows this limitation and that is why the number of backend processes is capped by the number of connections, the max_connections parameter defaults to 100 which may look low but we will find out in the next few paragraphs is it actually enough for most cases. Perhaps Postgres set it this low to discourage large number of connections by default (for a good reason).You see, backend applications such as web servers and reverse proxies are directly exposed to end-user clients resulting in potentially millions of connections. While the “clients” of Postgres as a database tend to be other web servers and backend applications that are not as verbose and can safely share a pool of connections. The number of clients of a Web server coming from mobile phones and browsers is much higher than those of a database which is handful of applications that can share connections.In the next section, we will learn that Postgres has a feature to offload processing to a pool of worker threads known as background workers for parallel processing.Background WorkersMost proxies, web servers (and even databases e.g. memcached) create a handful of processes or threads (often one for every CPU core) and distribute connections among these processes. This keeps context switching to a minimum and allow sharing of resources.Spawning a backend process in Postgres for every connection and having that process do the work doesn’t scale in my opinion. Imagine having 1000 connections, the corresponding 1000 backend processes executing client queries and competing for CPU and resources, we are left at the mercy of the operating system scheduler deciding which process gets the CPU, as a result the overall performance of the system will degrade.Parallel QueriesIn version 9.6, Postgres introduced the parallel queries feature which allowed multiple workers to execute a single query. This allowed for two things: Break a single query and execute it on multiple cores in parallel so it completes faster. Delegate the work to a fixed size pool of workers instead of connection-bound backend processes.With parallel queries, the backend process builds a parallel plan and pulls x number of background workers from the pool to execute the plan. While a worker is executing a query it is marked as busy, no other queries can use that process. Even with large number of clients, the limited pool of background workers will serve as a configurable and predictable performance metric which didn’t exist prior to Postgres 9.6.Pros and Cons of Parallel QueriesBased on the doc, parallel queries can result in higher CPU utilization compared to non-parallel queries with the same number of clients and queries. A single query with joins and nested queries will run on a single process and single CPU core but when broken into parts it will run on multiple cores consuming more CPU. While this is true in normal/low load environment, it is slightly different in high load.When all background workers are busy in a high load environment, new parallel queries will have to wait causing only that client to experience the wait delay while the database remain stable. Compare this to when parallel queries are disabled, nothing (as far as I know) stops the backend processes from executing all or any incoming queries, leading to an overall system performance degradation with all the processes competing for CPU and OS context switching. Of course you can limit the number of backend processes with the max_connections parameter to avoid this problem and Postgres does set that to a low value of 100, but then you prevent clients from connecting. You will notice all my focus here is on a single machine, of course having many read replicas to distribute the load is a good idea when a single machine can’t handle it. But I also believe that we should not rush to distribute until we squeezed every bit of performance from a single machine.It is a trade-off, few clients will suffer “waits” when all workers are busy, but the waits can be measured, logged, understood and even in some cases tolerated. When we always let the backend process do the work everyone is competing for CPU time and we have little control. It is imporant to note that parallel queries will only get triggered if the backend process comes up with parallel plan. If the query is simple and the cost seems low the backend process will likely do the work. Parallel queries can be disabled by setting max_parallel_workers_per_gather to zero.Notes on Background workersBesides parallel queries, background workers are also responsible for logical replication and custom user-code extensions. Just like backend processes, background worker processes have their own virtual memory space mainly used to store the work_mem area which is used for holding data for sorting. It is important to understand that work_mem is per process, so really multiply that for each background worker and backend process.The background worker pool is limited by the max_worker_processes parameter which defaults to 8. To me, I would match it to the number of cores on the machine, but we also need to think about the nature of the workload here. Postgres is a database, while it uses the CPU for parsing, planning and sorting, the rest of the work is mostly I/O bound whether that is hitting the memory or disk. Again, each background worker will allocate a work_mem worth of memory in its private virtual memory space.Auxiliary ProcessesAside of query execution, Postgres does routine maintenance and management use auxiliary processes unrelated to background workers for these tasks. Below I illustrate the auxiliary processes in Postgres:Background Writer (bw)Noted in my diagram as bw, the Background writer is responsible for flushing dirty pages on the shared buffers to file system. HUGE emphasis on file system and NOT necessary disk. You see, the operating system has a file system in memory cache that holds writes until it has enough for them and flushes all them at once to disk to reduce disk I/O. In case of a crash we may lose those changes that haven’t been flushed to disk, that is why we have fsync O_DIRECT which bypasses all that stuff. Background writer simply writes the dirty pages to the file system cache to free some room in memory buffer for more pages to come in.All the processes we talked about so far has their own private virtual memory space not accessable to each other, but they also has access to a shared memory space so that pages can be readable by multiple processes. This way all processes have access to the latest and can detect locks and other changes.Whether it is from the backend process via non-parallel or one of the background workers through parallels, queries read pages from disk and put them in the shared buffers, and when they write they write to the pages in memory marking them dirty. The shared buffers size can be configured with the shared_buffer parameter and can get full, so dirty pages have to be written to the file system (and eventually to disk for durability) to free up some space for more pages to get to the shared buffers. The background writer job is to write the dirty pages to file system just to free up space in the shared buffers. There is another process that make sure dirty pages get flushed to disk and that is our next auxiliary process. You might say isn’t it bad to write changes to memory, what if the database crashed? wouldn’t we lose the changes? Actually no, we also write the changes to WAL and persist that to disk more frequently especially on commit. So even if we crashed we can pull whatever we have on disk and “redo” the changes from WAL to the data pages to get to the final state. Of course we might have also flushed pages with changes from transactions that have since rolled back, in that case Postgres “should” “undo” those changes (but doesn’t). The magic or redo and undo logs. To be frank if the data pages have uncommitted changes from transcations that rolledback it is fine, future queries know to ignore those tuples. This makes postgres start up even faster as UNDO technically is not implemented yet as of writing this post. It doesn’t change the fact that this bloat pages and eventually slows down performance, future vacuums should clean thoseCheckpointer (cp)The background writer writes the dirty pages from shared buffers to the file system cache to free up shared buffers. While changes to the file system eventually goes to disk, they do stay in the operating system file cache for a while in the hopes pages (OS pages that is) might receive more writes and then OS can flush all them in one I/O to disk. There is a possibility that we might lose pages in the file system cache in case of a crash so databases never relay on file system cache for durabilty.There is another auxiliary process called checkpointer, which bypasses the file system cache and enforces that the pages are written to disk. The checkpointer (cp) also creates a checkpoint record that guarantees that at this point the WAL and data files pages are 100% in sync and if we crash we will use that checkpoint as our starting point to redo the changes from the WAL, which also has been flushed to disk.Startup Process (st)While discussing the background writer and checkpointer we mentioned in case of crash, Postgres applies the WAL changes to data pages to come back to a consistent state. Well , it is the startup process auxiliary process that redo the changes. I suppose that nothing can be done until the startup process completes. This makes me think that the startup process might run even before the postmaster, I would sure implement it this way so that no one can connect unless I can recover my database.Logger (lg)Someone needs to write database events, warnings, errors, and (if you enabled tracing) logging the SQL statements, this is the job of the auxiliary process Logger also called syslogger.Autovacuum Launcher (avl)Another auxiliary process that wakes up and launches autovacuum workers which is a completely different pool to do the vacuum process. Not much info on this process so just adding it for completion. I suppose when autovacuum is disabled the launcher is not spawn nor its workers.WAL writer (ww)The WAL (Write-ahead log) lives as WAL records in the shared memory, many processes write to the WAL as transcations takes place. Eventually the WAL has to go into the WAL files on disk not just file system cache, but actually physically on disk for them to be useful. The auxiliary process WAL writer (ww) is responsible to flush the WAL.WAL archiver (wa)Once a checkpoint is created by the checkpointer process, older WAL records can be safely purged. However, for backup, recovery and replication purposes, WAL entries can be archived, the WAL archiver auxiliary process takes car of this.WAL receiver (wr)It is enough to stream WAL records from primary to standby databases to achieve replications. Data files can be updates accordingly as they are much larger. The auxiliary process WAL receiver runs on the replica to receive these changes and apply them to the pages in memory. It is worth mentioning that any process that understands the replication protocol can also receive WAL records.Other ProcessesI couldn’t find a category where I put these so I created one. Those are processes that are not backend nor auxiliary (don’t ask me why). Let us explore them.Autovaccume workersVacuum is the process that cleans up entries in pages that are no longer required, whether dead tuples or those left over from rollbacked transactions. Vacuum also cleans entries in the pages from transcations that didn’t get to commit because of a crash but their changes have made it to the disk by background or checkpointer, a process refered to by undo which the startup process should do but not yet implemented in postgres.The Autovacuum workers which are spawned by the autovacuum launcher take care of the vacuuming. The autovacuum workers also perform analyze to update the statistics on all tables and indexes.WAL sendersAccording to the postgres doc, these are referred to as special backend process that streams WAL changes to the WAL receiver auxiliary process which we discussed before. You can configure how many WAL senders postgres spins up with max_wal_senders parameter.Process vs ThreadTo the million dollar question. Why processes and not threads? To be honest I couldn’t find a convincing answer. I wanted to explore the differences between processes and threads here but that will make this post even longer. I rather do that in a new post. Will link it here once authored. Actually do me a favor, highlight this sentence to see how many of you actually reached this section and found this post interesting.Until then I’ll summarize what I know, processes are definitely heaver than threads, each process have its own virtual memory, it maintains metadata called PCB (process control block) which includes page table for mapping virtual to physical addresses and any other metadata about the process. The PCB has to be stored in memory and brought into the CPU cache registers do translate virtual memory addresses to physical addresses. Threads on the other hand share the virtual memory space with their parent process and their TCB (thread control block) is much smaller with a pointer to parent process PCB. So your cache hits are much higher with threads than processes.The only reason I can find as to why Postgres use processes instead of threads are because threads used to be unstable. This discussion is dated on 2004 but since then threading subsystem in operating system are much more stable of course. The question remain, is it really worth it for postgres to switch to threads instead of processes? To me I don’t think, that will far destabilize postgres and it will take years to implement and even then how much is the benefit really is?If I would change something in Postgres its not really the Processes, but the concept of one backend process per connection. We can create a pool of backend processes per database, effectively move the connection pooling from the application down to the database.Edit: 6/9/2023 A Postgres community thread has started to discuss the possiblity of making Postgres multi-threaded.SummaryPostgres uses processes for all its operations. In this post I illustrated the process architecture of Postgres and explored all the processes (that I’m aware of). Processes have mainly two categories of process groups, one called backend processes which is directly client facing and get one per connection and do the actual work (unless in parallel), the others are system auxiliary processes which do maintaince and routine tasks. Postgres also have other types of special processes such as autovacuum workers.If you enjoyed this post consider checking out my database and backend engineering courses.If you prefer to watch a video of this article#postgres#database#postgresql#software-architecture#process-vs-threadBack to Parent Page" }, { "title": "Part 5: Database Engineering Fundamentals: Postgres vs MySQL", "url": "/database-engineering-fundamental-part-5/postgres-vs-msysql/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-05 06:00:00 +0530", "snippet": "Postgres vs MySQL (The fundamental differences)Postgres vs MySQLThe main differences with examplesOne of you guys asked a question on the Q&amp;A section about the difference between Postgres and MySQL. The answer turned out too long I thought I’ll make it into a post.In a nutshell, the main difference between the two databases really boils down to the implementation of primary and secondary indexes and how data is stored and updated.Let us explore this further.But First.. FundamentalsAn index is a data structure (B+Tree mostly) that allows searching for keys through layers of nodes which databases implement as pages. The tree traversal allows eliminating pages that don’t have the result and norrowing down pages that has it. This continues until a leaf page where key live is found.Leaf nodes or pages contain a list of ordered keys and their values. When a key is found you get its value and the page is cached in the database shared buffers with the hope that future queries may request keys in the same page. This last sentence is the fundamental understanding of all what database engineering, administration, programming and modeling is about. Knowing that your queries are hitting keys next to each other in a page will minimize I/Os and increase performance.Keys in B+Tree indexes are the column(s) on the table the index is created on, and the value is what databases implement differently. Let us explore what value is in Postgres vs MySQL.MySQLIn a primary index, the value is the full row object with all the attributes*. This is why primary indexes are often referred to as clustered indexes or another term that I prefer index-organized table. This means the primary index is the table. *Note that this is true for row-store, databases might use different storage model such as column-store, graphs or documents, fundamentally those could be potential values.If you do a lookup for a key in the primary index you find the page where the key lives and its value which is the full row of that key, no more I/Os are necessary to get additional columns.In a secondary index the key is whatever column(s) you indexed and the value is a pointer to where the full row really live. The value of secondary index leaf pages are usually primary keys.This is the case in MySQL. In MySQL all tables must have a primary index and all additional secondary index point to the primary keys. If you don’t create a primary key in a MySQL table, one is created for you.Example of where MySQL innoDB all tables must have a clustered primary indexPostgresIn Postgres technically there is no primary index, all indexes are secondary and all point to system managed tuple ids in data pages loaded in the heap. The table data in the heap are unordered, unlike primary index leaf pages which are ordered. So if you insert rows 1–100 and all of them are in the same page, then later updated rows 1–20, those 20 rows may jump into a different page and become out of order. While in a clustered primary index, insert must go to page that satisfies they key’s order. That is why Postgres tables are often referred to as “heap organized tables” instead of “index organized tables”.It is important to note that updates and deletes in Postgres are actually inserts. Every update or delete creates a new tuple id and the old tuple id is kept for MVCC reasons. I’ll explore that later in the post. The truth is the tid by it itself is not enough. Really we need both the tuple id and also the page number, this is referred to as c_tid. Think about it, it is not enough to just know the tuple id we need to know which page the tuple live. Something we didn’t have to do for MySQL because we are actually doing a lookup to find the page of the primary key. Where as in Postgres we are simply doing an I/O to fetch the full row. Example of where Postgres tables are heap organized and all indexes point to the tuple idsQueries CostTake the following table for the examples below. TABLE T; PRIMARY INDEX ON PK AND SECONDARY INDEX ON C2, NO INDEX ON C1 C1 and C2 are text PK is integer PK C1 C2 1 x1 x2 2 y1 y2 3 z1 z1 Let us compare what happens in MySQL vs PostgresSELECT * FROM T WHERE C2 = 'x2';That query in MySQL will cost us two B+Tree lookups. We need first to lookup x2* using the secondary index to find x2’s primary key which is 1, then do another lookup for 1 on the primary index to find the full row so we return all the attributes (hence the *). *One might think this is simply two I/Os which isn’t true, a B+Tree lookup is a O(logN) and depending on the size of the tree it might result in many I/Os. While most of the I/Os might be logical (hitting cached pages in shared buffers) it is important to understand the difference.In Postgres looking up any secondary index will only require one index lookup followed by a constant single I/O to the heap to fetch the page where the full row live. One B+Tree lookup is better than two lookups of course.To make this example even more interesting, say if C2 is not-unique and there were multiple entries of x2, then we will find tons of tids (or PKs in MySQL) matching the x2. The problem is those row ids will be in different pages causing random reads. In MySQL it will cause Index lookups ( perhaps the planner may opt for an index scan vs a seek based on the volume of these keys) but both databases will result in many random I/Os.Postgres attempts to minimize random reads by using bitmap index scans, grouping the results into pages instead of tuples and fetching the pages from the heap in fewer I/Os possible. Later additional filtering is applied to present the candidate rows.Let us take a different query.SELECT * FROM T WHERE PK BETWEEN 1 AND 3;I think MySQL is the winner here for range queries on primary key index, with a single lookup we find the first key and we walk the B+Tree linked leaf pages to find the nearby keys as we walk we find the full rows.Postgres struggles here I think, sure the secondary index lookup will do the same B+Tree walk on leaf pages and it will find the keys however it will only collect tids and pages. Its work is not finished. Postgres still need to do random reads on the heap to fetch the full rows, and those rows might be all over the heap and not nicely tucked together, especially if the rows were updated. Update heavy workload is Postgres’s enemy, pick a good FillFactor for your table.Ok let us do an update.UPDATE T SET C1 = ‘XX1’ WHERE PK = 1;In MySQL updating a column that is not indexed will result in only updating the leaf page where the row is with the new value. No other secondary indexes need to be updated because all of them point to the primary key which didn’t change.In Postgres updating a column that is not indexed will generate a new tuple and *might** require ALL secondary indexes to be updated with the new tuple id because they only know about the old tuple id. This causes many write I/Os. Uber didn’t like this one in particular back in 2016, one of their main reasons to switch to MySQL off of Postgres. I said might here because in Postgres there is an optimization called HOT (heap only tuple) not to be confused with (Heap organized table) that keeps old tuple id in the secondary indexes but put a link on the heap page header that points old tuple to the new one.Data types MatterIn MySQL choosing the primary key data type is critical, as that key will live in all secondary indexes. For example a UUID primary key will bloat all secondary indexes size causing more storage and read I/Os.In Postgres the tuple id is fixed 4 bytes so the secondary indexes won’t have the UUID values but just the tids pointing the heap.Undo logsAll modern databases support multi version concurrency control (MVCC). In simple read committed isolation level if a transaction tx1 updates a row and didn’t commit yet while another concurrent transaction tx2 wants to read that row it MUST read the old row not the updated one. Most databases (MySQL included) implement this feature using undo logs.When a transaction makes a change to a row, the change is written to the page in the shared buffer pool so the page where the row live always has the latest data. The transaction then logs information of how to undo the latest changes to row (enough info to construct the old state) in an undo log, this way concurrent transactions that still need the old state based on their isolation level must crack the undo log and construct the old row.You might wonder if writing uncommitted changes to the page is a good idea. What happens if a background process flushes the page to disk and then the database crashed before the transaction can commit? That is where the undo log is critical. Right after a crash, the uncommitted changes are undone using undo logs on database startup*.One can’t deny the cost of undo logs for long running transactions on other running transactions. More I/Os will be required to construct the old states and there is a chance the undo logs can get full and the transaction might fail. In one case I have seen one database system takes over an hour to recover from a crash after running a 3 hour uncommitted long transaction. Yeah avoid long transactions at all costs.Postgres does this very differently, each update, insert and delete gets a new copy of the row with a new tuple id with hints about what transaction id created the tuple and what transaction id deleted the tuple. So Postgres can safely write the changes to the data pages and concurrent transactions can read the old or new tuples based on their transaction id. Clever design.Of course no solution is without its problems. We actually talked about the cost of creating new tuple ids on secondary indexes. Plus Postgres need to purge old tuples that are no longer required if all running transactions ids are greater than the transaction that deleted the tuples. Vacuum takes care of that.Processes vs ThreadsMySQL uses threads, Postgres uses processes, there are pros and cons for both I covered that in details in its own post here.Postgres Process ArchitectureI like threads better than processes in database systems. Just because they are lighter weight and share their parent process virtual memory address. Processes come with the overhead of dedicated virtual memory and larger control block (PCB) compared to the smaller thread control block (TCB).If we are eventually going to share memory and deal with mutexes and semaphores anyway why not use threads. Just my two cents.SummaryWith that in mind you get to pick which database system is right for you. What really matters is breaking down your use cases and queries and understand what each database does and see what works and what doesn’t for you.No wrong or right here.Back to Parent Page" }, { "title": "Part 5: Database Engineering Fundamentals: Postgres Locks — A Deep Dive", "url": "/database-engineering-fundamental-part-5/postgres-locks-a-deep-dive/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-05 05:00:00 +0530", "snippet": "Postgres Locks — A Deep DiveHussein NasserA VACUUM full can block a select, we will learn how and why in this blog and much moreI used to think database locks are two types, shared and exclusive. Readers acquire many shared locks on a resource (row, object or table) but only one writer can acquire an exclusive lock. If a writer has an exclusive lock no one can acquire shared locks and as a result no one can read (but the writer). When I started to dig into Postgres this binary view of locking changed completely and I understand why.You see, in Postgres there are five lock categories and over 12 individual lock types. To be honest knowing which command obtains which lock is less relevant than knowing which command can conflict which command. By conflict here I mean commands block each other and can’t run concurrently.In this blog I explore all types and categories of locks and at the end I show you the Postgres Lock Conflicts tool I wrote that shows what commands conflict with each other because the list can get huge looking at a matrix.All the information here are official Postgres doc , source code and ad-hoc testing on the app.Table LocksIf you ask me what a table lock three years ago I would say its a lock you obtain on a table so no one can do anything on that table while you hold that lock. No inserts, updates or deletes or any DDLs. But that is further from the truth in Postgres. There are eight types of table locks in Postgres and transactions can have multiple table locks on the same table. Some of those locks conflict, some don’t. Let us explore them one by one. You can view the table locks in mode column in pg_locks table.ACCESS EXCLUSIVEACCESS EXCLUSIVE (or AccessExclusiveLock in the code) is the most aggressive table lock. If a command obtains this lock type on a table nothing can be done to this table as this lock type conflicts with all other table locks. You can’t do DMLs so select, update or delete rows are blocked. You can’t do DDLs, alter a column, create an index and even system operation such as VACUUM cannot execute on the table. It is a complete block.What operations and commands in Postgres obtain this lock? I went through the doc pages and found all commands that obtain this kind of lock. Here they are, when anything in this list run, you can’t do anything to this table.--LIST OF POSTGRES COMMANDS THAT OBTAIN ACCESS EXCLUSIVE TABLE LOCKDROP TABLETRUNCATEREINDEXCLUSTERVACUUM FULLREFRESH MATERIALIZED VIEWALTER INDEX SET TABLESPACEALTER INDEX ATTACH PARTITIONALTER INDEX SET FILLFACTORALTER TABLE ADD COLUMNALTER TABLE DROP COLUMNALTER TABLE SET DATA TYPEALTER TABLE SET/DROP DEFAULTALTER TABLE DROP EXPRESSIONALTER TABLE SET SEQUENCEALTER TABLE SET STORAGEALTER TABLE SET COMPRESSIONALTER TABLE ALTER CONSTRAINTALTER TABLE DROP CONSTRAINTALTER TABLE ENABLE/DISABLE RULEALTER TABLE ENABLE/DISABLE ROW LEVEL SECURITYALTER TABLE SET TABLESPACEALTER TABLE RESET STORAGEALTER TABLE INHERIT PARENTALTER TABLE RENAMEThis means for example, if you run VACUUM FULL for instance on a table, you can’t select or update to this table. You might say why did you have to spell out different methods on Alter table, the reason because different Alter tables obtain different types of locks which makes some alters block certain operations while others might not.ACCESS SHAREACCESS SHARE (or AccessShareLock) is the lightest weight lock type. Only two commands that I’m aware of acquire this lock and those are SELECT and COPY TO. It is an indication that someone is reading the table, whether it is a single row, all the rows and yes even no rows if you do a query on a table that returned nothing, that lock is also acquired (I tested it).--LIST OF POSTGRES COMMANDS THAT OBTAIN ACCESS SHARE TABLE LOCKSELECTCOPY TOThis lock type only conflicts with the ACCESS EXCLUSIVE which makes it easy to understand, if run a transaction that does a select you can’t do a VACUUM FULL (normal VACUUM is fine). The reason is VACUUM FULL or any of the commands that acquire ACCESS EXCLUSIVE does sergical changes to the layout of the table which will break consistency when selects are running. For example VACUUM FULL actually changes tuple ids, purging tables and reshuffling data, we can’t be having people reading the table while this is happening. This is as opposed to normal VACUUM which really (almost) act like both update and deletes.EXCLUSIVEThe EXCLUSIVE (or ExclusiveLock) is very similar to the ACCESS EXCLUSIVE except it doesn’t conflict with reads acquired by ACCESS SHARE. This means you can do selects while an EXCLUSIVE table lock is on the table.The odd thing I only found one command (REFRESH MATERIALIZED VIEW CONCURRENTLY) that acquires this lock. If I had to guess, this lock type was added because people wanted a way to refresh their materialized views and select from the table at the same time. The Refresh materialized view acquires an ACCESS EXCLUSIVE blocking selects, so Postgres added both a new lock type Exclusive which conflicts with everything except ACCESS SHARE and then made a new command to allow refreshing the view concurrently. I’m sure more methods will fit into this slot lock type.--LIST OF POSTGRES COMMANDS THAT OBTAIN EXCLUSIVE lockREFRESH MATERIALIZED VIEW CONCURRENTLYSo you if you refresh your materialized view concurrently your table can’t be edited but can be read.ROW SHAREBuckle up the names only get more confusing from here on. ROW SHARE (or RowShareLock) is similar to ACCESS SHARE but was designed for the SELECT FORs command family. That is probably why it has the name ROW in it. While SELECT FOR UPDATE, SELECT FOR SHARE and others work on rows remember this is still a table lock. So these kind of commands actually acquire two types of locks row locks (will see later) and the ROW SHARE row locks.This lock type conflicts with ACCESS EXCLUSIVE and the EXCLUSIVE lock. Which means anything that is acquired by the ACCESS EXCLUSIVE + our refresh materialized view concurrently.Here is a list of commands that acquire ROW SHARE.--LIST OF POSTGRES COMMANDS THAT OBTAIN ROW SHARE table lockSELECT FOR UPDATESELECT FOR NO KEY SHARESELECT FOR SHARESELECT FOR KEY SHARESo while true you can do normal SELECTs while refreshing your materialized view concurrently, you can’t really do a SELECT FOR SHARE for instance.ROW EXCLUSIVEThe ROW EXCLUSIVE (or RowExclusiveLock) is obtained by DMLs (Insert, Update, Delete, Merge and Copy From. If you care about write latency to your table watch out for operations that conflict with this lock. Thus the name ROW in the lock name because methods often operates on rows.Gotta watch out again, you might you a update or a delete that end touching no rows, the ROW EXCLUSIVE lock is still acquired. So if you have long running transactions watch out for blocks.The methods acquiring ROW EXCLUSIVE are as follows--LIST OF POSTGRES COMMANDS THAT OBTAIN ROW EXCLUSIVE table lockUPDATEDELETEINSERTMERGECOPY FROMSHARE ROW EXCLUSIVESHARE ROW EXCLUSIVE (or ShareRowExclusiveLock) is similar to EXCLUSIVE but is relaxed so that ROW SHAREs don’t conflict, so you may do the SELECT FORs with this type of lock but you still can’t do any modifications through DMLs. So methods that obtain this type of locks want to allow reads even the SELECT FORs but block writes. In case you had the question why not just use EXCLUSIVE that is why. It is all about relaxing and minimizing blocks.What is interesting about SHARE ROW EXCLUSIVE is it does conflict with it self which means only one operation of this lock type can run so for instance you can’t run two create triggers (which is a method that acquire SHARE ROW EXCLUSIVE) at the same time on the same table, my guess is one create trigger might modify rows in the table and the other create trigger might also change the table and we don’t want that. Again remember if a transaction obtains a SHARE ROW EXCLUSIVE it can still make modifications to rows, other transactions can’t.Here are the methods that obtain this type.--LIST OF POSTGRES COMMANDS THAT OBTAIN SHARE ROW EXCLUSIVE table lockCREATE TRIGGERALTER TABLE ADD FOREIGN KEYALTER TABLE ENABLE/DISABLE TRIGGERSHAREThe SHARE (ShareLock) is similar to the SHARE ROW EXCLUSIVE in a sense it blocks concurrent modifications but it doesn’t conflict with itself. Only CREATE INDEX obtain this type of lock, which means while creating an index you can’t change the data (because the index is reading the table and building the b+tree) but technically nothing stopping 7 different transactions from running 7 CREATE INDEX on the same table, so if you are blocking writes to create indexes, you can technically create them all at the same time. Of course you can also use the CREATE INDEX Concurrently which allows concurrent modifications but that doesn’t run in a transaction.--LIST OF POSTGRES COMMANDS THAT OBTAIN SHARE table lockCREATE INDEXSHARE UPDATE EXCLUSIVEThe SHARE UPDATE EXCLUSIVE (or ShareUpdateExclusiveLock) is designed for methods who want to allow concurrent writes and reads but prevent schema changes and VACUUM runs. Normal VACUUM for instance acquire this lock which is why you can run VACUUM and still do edit to your table otherwise it will be a disaster.CREATE INDEX CONCURRENTLY is another interesting one where you can create an index and allow writes. This lock conflict with itself so no two VACUUMs can run concurrently and no two CREATE INDEX CONCURRENTLY as well. This also explains why many forms of ALTER TABLE commands acquire this type of lock, you want to allow edits but no two alters at the same time.Following are the commands that acquire SHARE UPDATE EXCLUSIVE.--LIST OF POSTGRES COMMANDS THAT OBTAIN SHARE UPDATE EXCLUSIVE table lockVACUUMREINDEX CONCURRENTLYCREATE STATISTICSCREATE INDEX CONCURRENTLYCOMMENT ONANALYZEALTER TABLE VALIDATE CONSTRAINTALTER TABLE SET WITHOUT CLUSTERALTER TABLE SET TOASTALTER TABLE SET STATISTICSALTER TABLE SET N_DISTINCTALTER TABLE SET FILLFACTORALTER TABLE SET AUTOVACUUUMALTER TABLE DETACH PARTITIONALTER TABLE CLUSTER ONALTER TABLE ATTACH PARTITION (PARENT)ALTER INDEX (RENAME)Table Lock MatrixThis is the matrix from the doc, it helps us understand what locks conflicts with what.   ACCESS SHARE ROW SHARE ROW EXCL. SHARE UPDATE EXCL. SHARE SHARE ROW EXCL. EXCL. ACCESS EXCL. ACCESS SHARE               X ROW SHARE             X X ROW EXCL.         X X X X SHARE UPDATE EXCL.       X X X X X SHARE     X X   X X X SHARE ROW EXCL.     X X X X X X EXCL.   X X X X X X X ACCESS EXCL. X X X X X X X X To me however the methods and commands that acquire the locks are more important than the locks themselves. Which is why I wrote this tool to dynamically show which method’s conflicts with what commands and what commands are allowed concurrently. You will see it referenced all over this blog. Table locks are in memory and can be retrieved from the pg_locks view. The memory requirements for table locks are low because they are coarser compared to row locks. Which we will discuss later.Here is an example from the Postgres Lock Conflicts tool on VACUUM.Row LocksNow that we talked about table locks time to go one level deeper to row locks. Row locks are critical to detect changes to row objects so we prevent two transactions changing the same row which results in lost updates.Worth noting that INSERTed tuples don’t require row locks in postgres because they are only visible to the transaction that creates them. One reason probably why Postgres doesn’t support read uncommitted isolation level.The methods that lock rows are limited to DELETE, UPDATE (NO KEY), UPDATE (KEY), and all the SELECT FORs.UPDATE (NO KEY) is an update to a column that doesn’t have a unique index while UPDATE (KEY) is an update to a column that does have a unique index. Those two acquire different locks that is why they are spelled out.Here are four row locks in Postgres we discuss them here.FOR UPDATEFOR UPDATE is the highest row lock, when a row is locked FOR UPDATE you cannot delete or update it or do a SELECT FOR UPDATE on it. However you can still read it through a normal SELECT, if you want your selects to be blocked if someone is touching a row you may use SELECT FOR KEY SHARE instead which conflicts.The following commands acquire a FOR UPDATE row lock.--LIST OF POSTGRES COMMANDS THAT OBTAIN FOR UPDATE row lockDELETEUPDATE (KEY) -- UPDATE TO A COLUMN WITH A UNIQUE INDEXSELECTFOR NO KEY UPDATEThis lock is acquired by UPDATES to columns without unique index, so it is weaker than FOR UPDATE as it allows SELECT FOR KEY SHARE.--LIST OF POSTGRES COMMANDS THAT OBTAIN FOR NO KEY UPDATE ROW LOCKUPDATE (NO KEY) -- UPDATE TO A COLUMN WITH NO INDEX OR REGULAR INDEX (NON-UNIQUE)FOR SHAREThis is the true shared lock, transactions can acquire multiple FOR SHARE locks on a row. When a row is FOR SHAREd no DML can modify it.--LIST OF POSTGRES COMMANDS THAT OBTAIN FOR SHARESELECT FOR SHAREFOR KEY SHAREThe weakest row lock, behaves like FOR SHARE but allows updates to columns without unique indexes.--LIST OF POSTGRES COMMANDS THAT OBTAIN FOR SHARESELECT FOR KEY SHARERow Lock MatrixThis matrix shows the 4 row locks and how they conflict with each other. The tool I wrote gives more visibility to the commands that block each other.   FOR KEY SHARE FOR SHARE FOR NO KEY UPDATE FOR UPDATE FOR KEY SHARE       X FOR SHARE     X X FOR NO KEY UPDATE   X X X FOR UPDATE X X X X Postgres table locks are in memory, row locks are stored in the tuple (xmax system field), which saves memory at a cost of potential disk writes. This isn’t so bad for deletes or updates because we are technically touching the row but select for updates for instance, those read operations can now cause pages to get dirty which will trigger the background writer to flush them to disk. Row locks are memory intensive in other databases, I work with SQL Server almost on daily basis and if row locks are enabled SQL Server can easily run out of memory and fail the transaction when row locks can’t be acquired. That is why I appreciate the brilliance of Postgres row-lock designs. Nothing is free though, disk writes.Page locksA postgres page is 8KB and stores tuples for table and indexes. Because the page is an in memory data structure, it needs to be protected from concurrent process accesses. You see, Postgres design is process based which means when you connect to the database you get your own backend process, and those backend processes compete to access pages stored in shared buffer pool.It is not so bad if multiple processes reading the same page, but it is a problem if we have two processes attempting to write to the same page. You want to serialize those accesses so you don’t corrupt data. This is classic operating system concepts with mutex and semaphores.Dead LocksDead locks happen when two transactions each holding different locks attempting to access each other’s resources and end up in an indefinite wait. Postgres detects dead locks and kills one of the transaction to move on.Here is an example (while unlikely it can happen)Tx1BEGIN;-- ACQUIRES AccessSharelock (OKSELECT * FROM TEST Tx2 BEGIN; -- ACQUIRES AccessSharelock (OK) SELECT * FROM TEST; -- Attempts to acquire -- AccessExlusiveLock get blocked by tx1 ALTER TABLE ADD COLUMN A TEXT;--Attempts to acquire Access--Exclusive blocks by tx2--dead lockTRUNCATE TABLE TEST;---DEAD LOCK X_XAnother example with row locksTx1BEGIN;-- ACQUIRES ShareRowLock (OK)SELECT * FROM TESTWHERE ID = 1FOR SHARE Tx2 BEGIN; -- ACQUIRES ShareRowLock (OK) SELECT * FROM TEST WHERE ID = 1 FOR SHARE --Attempts to update the row (X) --blocked by Tx1 share lock UPDATE TEST SET V = 1 WHERE ID = 1;--Attempts to delete the row--Blocked by Tx2 share lockDELETE FROM TESTWHERE ID = 1;---DEAD LOCK X_XAnother dead lock example from the doc.Tx1BEGIN;-- ACQUIRES FOR UPDATE lock-- ON row 11111 (OK)UPDATE accountsSET balance = balance + 100.00WHERE acctnum = 11111; Tx2 BEGIN; -- ACQUIRES FOR UPDATE lock (OK) -- ON row 22222 (OK) UPDATE accounts SET balance = balance + 100.00 WHERE acctnum = 22222; -- Attempts to acquire FOR UPDATE -- ON row 11111, blocked by tx1 (X) UPDATE accounts SET balance = balance - 100.00 WHERE acctnum = 11111;--Attempts to acquire FOR UPDATE--On row 22222, blocked by tx2 (X)UPDATE accountsSET balance = balance - 100.00WHERE acctnum = 22222;---DEAD LOCK X_XAdvisory LocksSometimes the application requirement makes the native MVCC locks insufficent. That is why most databases provide application-level locks that are controlled by the application to be held and released. While those locks still live in the database they are acquired and released by the application.You might say why not just use FOR SHARE and FOR UPDATE to simulate that. The problem is you have to have a row to lock in those cases and you might be unnecessarily blocking other transactions from doing legitment modifications to that row. Advisory locks are obtained on integer values not rows or tables. Those numbers can come from columns of rows they don’t have to be.Another reason why row locks don’t work is they are always tied to a transacation, if the transcation commits or rollsback the locks are gone and this is something your application might not want. Take for example a long — running operation in the app that does multiple database transactions, you want to prevent multiple users from running this long running operation concurrently. It is almost very hard to control that with just normal locks so what you can do is obtain a session level adversary lock when the operation starts so other users will attempt to also to obtain the same Advisory lock and get blocked.There are two types of advisory locks, session and transcation. Session locks obtained with pg_advisory_lock () are kept for the length of the session (connection), while transaction advisory locks obtained with pg_advisory_xact_lock () are kept for the length of the current running transaction.Here is an example.-- Start Applicaiton Operation-- Acquires a session lockSELECT pg_advisory_lock(100);--TxA1BEGIN;--DO WORKCOMMIT; -- Start Applicaiton Operation -- Attemps Acquires a session lock -- BLOCKS (X) SELECT pg_advisory_lock(100);--TxA2BEGIN;--DO MORE WORKCOMMIT;--Release session lock 100SELECT pg_advisory_unlock(100);-- End Applicaiton Operation -- TxB1, lock unblocks BEGIN; --DO MORE WORK COMMIT; -- TxB2 BEGIN; --DO MORE WORK COMMIT; --Release session lock 100 SELECT pg_advisory_unlock(100);Weak Locks6/21/2023 — I added this additional paragraph to mention weak locks, something I recently learned in Postgres. Postgres has weak locks, those are table locks that rarely conflicts , acquired by DMLs, they are mainly AccessShareLock, RowShareLock, RowExclusiveLock.Because they are common, and weak, Postgres manages them through a fast path a data structure in the process as oppose through the normal lock manager. But it can’t just use that data structure with no limit, it has a limit and that limit is 16 weak locks per backend process according to the constant FP_LOCK_SLOTS_PER_BACKEND which can’t be changed unless you recompile postgres alas. If you don’t know backend process == connection in postgres. So if your data model is heavily normalized OR you are using partitioning and your queries are scanning multiple partitioning in a long transaction watch out not to exceed that. otherwise you hit the lock manager and contention is created.SummaryUnderstanding Postgres locking will give you an edge to making better application design choices and better luck at trouble-shooting lock waiting, dead-locks and general latency when executing queries. Advisory locks can also be used to build interesting use cases that don’t work with normal MVCC locks. Hope you enjoyed this post.If you like this content, consider checking out my database course.Postgres+Locks+—+A+Deep+Dive-New.pdfBack to Parent Page" }, { "title": "Part 5: Database Engineering Fundamentals: InnoDB B-tree Latch Optimization History", "url": "/database-engineering-fundamental-part-5/innodb-b-tree-latch-optimization-history/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-05 04:00:00 +0530", "snippet": "InnoDB B-tree Latch Optimization HistoryJune 9, 2024 5:30 AMbaotiaoShare on:In general, in a database, “latch” refers to a physical lock, while “lock” refers to a logical lock in transactions. In this article, the terms are used interchangeably.In the InnoDB implementation, there are two main types of locks in the B-tree: index lock and page lock. Index lock refers to the lock on the entire index, which is represented in the code as dict_index-&gt;lock. Page lock refers to the lock present on each page within the B-tree.When we refer to B-tree locks, we generally mean both the index lock and the page lock working together.In the 5.6 implementation, the process of B-tree latching is relatively simple, as follows:1. For a query request: First, acquire an S LOCK on btree index-&gt;lock. Then, after finding the leaf node, acquire an S LOCK on the leaf node as well, and release the index-&gt;lock. 2. For a leaf page modification request: Similarly, acquire an S LOCK on btree index-&gt;lock. Then, after finding the leaf node, acquire an X LOCK on it because the page needs to be modified. After that, release the index-&gt;lock. At this point, there are two scenarios depending on whether the modification of this page will cause a change in the B-tree structure: If it doesn’t, that’s good. Once the X LOCK on the leaf node is acquired, modify the data and return. If it does, you will need to perform a pessimistic insert operation and re-traverse the B-tree. Acquire an X LOCK on the B-tree index and execute btr_cur_search_to_nth_level to the specified page. Since modifying the leaf node may cause changes to the B-tree all the way up to the root node, other threads must be prevented from accessing the B-tree during this time. Therefore, an X LOCK is required on the entire B-tree, meaning no other query requests can access it. Moreover, since an X LOCK is held on the index, and record insertion into the page might cause the upper-level pages to change, this process may involve disk I/O, potentially making the X LOCK last for an extended time. During this time, all read-related operations will be blocked. The specific code for this is in row_ins_clust_index_entry. Initially, an optimistic insert operation is attempted: err = row_ins_clust_index_entry_low( 0, BTR_MODIFY_LEAF, index, n_uniq, entry, n_ext, thr, &amp;page_no, &amp;modify_clock); If the insert fails, a pessimistic insert operation is attempted: return(row_ins_clust_index_entry_low( 0, BTR_MODIFY_TREE, index, n_uniq, entry, n_ext, thr, &amp;page_no, &amp;modify_clock)); As you can see, the only difference here is that the latch_mode is either BTR_MODIFY_LEAF or BTR_MODIFY_TREE. Since btr_cur_search_to_nth_level is executed in the row_ins_clust_index_entry_low function, the B-tree is re-traversed when the pessimistic insert is retried after a failed optimistic attempt. As shown above, in 5.6, the index lock is only applied to the entire B-tree index, and the page lock is applied only to leaf node pages in the B-tree. Non-leaf node pages in the B-tree are not locked.This simple implementation makes the code easy to understand, but it has obvious disadvantages. During SMO (Structure Modification Operation), read operations cannot proceed, and because SMOs may involve disk I/O, the resulting performance fluctuations are quite noticeable. We have often observed such phenomena in production.The 8.0 ImprovementsIn response, official changes were introduced, starting in 5.7. Here, we’ll take 8.0 as an example. The main improvements include: The introduction of SX LOCK. The introduction of non-leaf page locks.SX LOCK IntroductionLet’s first introduce SX LOCK. SX LOCK can be used for both index locks and page locks. SX LOCK does not conflict with S LOCK but does conflict with X LOCK. SX LOCKs also conflict with each other. The purpose of an SX LOCK is to indicate the intention to modify the protected area, but the modification has not yet started. Therefore, the resource is still accessible, but once the modification begins, access will no longer be allowed. Since an intention to modify exists, no other modifications can occur, so it conflicts with X LOCKs.The main usage now is that index SX LOCK does not conflict with S LOCK, which allows reads and optimistic writes to proceed even during pessimistic insert operations.SX LOCK was introduced through this work log: WL#6363.SX LOCK was primarily introduced to optimize read operations. Since SX LOCK conflicts with X LOCK but not with S LOCK, places that previously required X LOCKs were changed to SX LOCKs, making the system more read-friendly.Non-leaf Page Lock IntroductionIn fact, this is how most commercial databases operate—both leaf pages and non-leaf pages have page locks.The main idea is Latch Coupling, where during a top-down traversal of the B-tree, the page lock on the parent node is released only after acquiring the lock on the child node. This minimizes the lock coverage. To implement this, non-leaf pages must also have page locks.However, InnoDB did not completely remove the index-&gt;lock, which means that only one BTR_MODIFY_TREE operation can occur at a time. Therefore, when B-tree structure modifications are highly concurrent, performance can degrade significantly.Back to the 5.6 ProblemAs we can see, in 5.6, the worst-case scenario is when modifying a B-tree leaf page triggers a change in the B-tree structure. In this case, an X LOCK on the entire index is required. However, we know that such changes may only affect the current page and the page at the next level. If we can reduce the lock scope, it will undoubtedly help improve concurrency.In MySQL 8.01. For a query request: First, acquire an S LOCK on btree index-&gt;lock. Then, during the B-tree traversal, acquire an S LOCK on the non-leaf node pages encountered. After reaching the leaf node, acquire an S LOCK on the leaf node page and release the index-&gt;lock. 2. For a leaf page modification request: Similarly, acquire an S LOCK on btree index-&gt;lock and S LOCKs on the non-leaf node pages. After reaching the leaf node, acquire an X LOCK on the leaf node because the page needs to be modified, and then release the index-&gt;lock. At this point, the situation branches into two scenarios depending on whether the page modification triggers a B-tree structure change: If it doesn’t, then the X LOCK on the leaf node is sufficient. After modifying the data, return as normal. If it does, a pessimistic insert operation is performed by re-traversing the B-tree. At this point, the index-&gt;lock is acquired with an SX LOCK. Since the B-tree now has an SX LOCK, the pages along the search path do not require locks. However, the pages encountered during the search process need to be saved, and X LOCKs are applied to the pages that may undergo structural changes. This ensures that read operations are minimally affected during the search process. Only after confirming the scope of the B-tree changes at the final stage, and acquiring X LOCKs on the affected pages, will the operation proceed. In 8.0, the duration of holding the SX LOCK is as follows: Holding the SX LOCK: After the first btr_cur_optimistic_insert fails, row_ins_clust_index_entry calls row_ins_clust_index_entry_low(flags, BTR_MODIFY_TREE ...) to insert. Inside row_ins_clust_index_entry_low, the SX LOCK is acquired in the btr_cur_search_to_nth_level function. At this point, the B-tree is locked by the SX LOCK, preventing further SMO operations. An optimistic insert is still attempted at this stage, with the SX LOCK still being held. If that fails, a pessimistic insert is attempted. Releasing the SX LOCK: In a pessimistic insert, the SX LOCK is held until a new page (page2) is created and connected to the parent node. If the page undergoing SMO is a leaf page, the SX LOCK is released only after the SMO operation is completed, and the insert is successful. The function responsible for executing the SMO and inserting is btr_page_split_and_insert.The btr_page_split_and_insert operation consists of approximately 8 steps:​ 1. Find the record to split from the page that is about to be split. Ensure the split location is at the record boundary.​ 2. Allocate a new index page.​ 3. Calculate the boundary record for both the original page and the new page.​ 4. Add a new index entry for the new page to the parent index page. If the parent page does not have enough space, it triggers the split of the parent page.​ 5. Connect the current index page, the current page’s prev_page, next_page, father_page, and the newly created page. The connection order is to first connect the parent page, then prev_page/next_page, and finally connect the current page and the new page. (At this point, the index-&gt;sx lock can be released.)​ 6. Move some records from the current index page to the new index page.​ 7. The SMO operation is complete, and the insertion location for the current insert operation is calculated.​ 8. Perform the insert operation. If the insert fails, try reorganization of the page and attempt the insert again.In the existing code, there is only one scenario where index-&gt;lock will acquire an X lock, which is:if (lock_intention == BTR_INTENTION_DELETE &amp;&amp; trx_sys-&gt;rseg_history_len &gt; BTR_CUR_FINE_HISTORY_LENGTH &amp;&amp; buf_get_n_pending_read_ios()) {// If the lock_intention is BTR_INTENTION_DELETE and the history list is too long, the index will acquire an X lock.Summary:Improvements in 8.0 compared to 5.6In 5.6, during a write operation, if an SMO (structure modification operation) is in progress, the entire index-&gt;lock would be locked with an X lock. During this time, all read operations would be blocked.In 8.0, read operations and optimistic write operations are allowed to proceed during an SMO.However, in 8.0 there is still a limitation: only one SMO can occur at a time because the SX lock must be acquired during an SMO. Since SX locks conflict with other SX locks, this remains one of the main issues in 8.0.Optimization Points:Of course, there are still some optimization opportunities here. There is still a global index-&gt;lock. Although it is an SX LOCK, in theory, according to the 8.0 implementation, it is possible to fully release the index lock. However, many details need to be handled. During the actual split operation, can the holding of the index lock inside btr_page_split_and_insert be optimized further? For example, based on a certain sequence, could the index-&gt;lock be released after connecting the newly created page to the new_page? Another consideration is the holding time of the X LOCK on the page where the SMO (structure modification operation) occurs. Currently, the X LOCK is held on all pages along the path until the SMO is completed, and the current insert operation is finished. Meanwhile, the father_page, prev_page, and next_page also hold X LOCKs. Could the number of locked pages be reduced? For example, this optimization is mentioned in BUG#99948. In btr_attach_half_pages, multiple traversals of the B-tree using btr_cur_search_to_nth_level could be avoided. This function is responsible for establishing links like the father link, prev link, and next link. However, it redundantly executes btr_page_get_father_block to traverse the B-tree to find the parent node, which internally calls btr_cur_search_to_nth_level. This step could be avoided since the index is already SX LOCKed, and the father node won’t change. The result from the previous btr_cur_search_to_nth_level call could be reused. Can we mark pages undergoing SMO with a state similar to a B-link tree, where the page is still readable? Although the record to be read might not exist on the current page, the reader could attempt to retrieve it from the page’s next_page. If the record can be found, the read operation is still valid. Can the pages encountered during the btr_cur_search_to_nth_level search be preserved? This way, even for repeated searches, only the max trx_id of the upper-level pages needs to be checked. If unchanged, the entire search path hasn’t changed, so no full traversal is necessary. Is it still necessary to retain the optimistic insert followed by a pessimistic insert approach? My understanding is that this process exists because the cost of pessimistic inserts was too high in the 5.6 implementation. To minimize pessimistic inserts, this process was carried over into the current 8.0 implementation. However, multiple insert attempts require multiple B-tree traversals, leading to additional overhead. talking https://dom.as/2011/07/03/innodb-index-lock/ https://dev.mysql.com/worklog/task/?id=6326Back to Parent Page" }, { "title": "Part 5: Database Engineering Fundamentals: How Slow is select * in row store", "url": "/database-engineering-fundamental-part-5/how-slow-is-select-in-row-store/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-05 03:00:00 +0530", "snippet": "How Slow is select * in row store?How Slow is Select * in row stores?In a row-store database engine, rows are stored in units called pages. Each page has a fixed header and contains multiple rows, with each row having a record header followed by its respective columns. For instance, consider the following example in PostgreSQL:When the database fetches a page and places it in the shared buffer pool, we gain access to all rows and columns within that page. So, the question arises: if we have all the columns readily available in memory, why would SELECT * be slow and costly? Is it really as slow as people claim it to be? And if so why is it so? In this post, we will explore these questions and more.Kiss Index-Only Scans GoodbyeUsing SELECT * means that the database optimizer cannot choose index-only scans. For example, let’s say you need the IDs of students who scored above 90, and you have an index on the grades column that includes the student ID as a non-key, this index is perfect for this query.However, since you asked for all fields, the database needs to access the heap data page to get the remaining fields increasing random reads resulting in far more I/Os. In contrast, the database could have only scanned the grades index and returned the IDs if you hadn’t used SELECT *.Deserialization CostDeserialization, or decoding, is the process of converting raw bytes into data types. This involves taking a sequence of bytes (typically from a file, network communication, or another source) and converting it back into a more structured data format, such as objects or variables in a programming language.When you perform a SELECT * query, the database needs to deserialize all columns, even those you may not need for your specific use case. This can increase the computational overhead and slow down query performance. By only selecting the necessary columns, you can reduce the deserialization cost and improve the efficiency of your queries.Not All Columns Are InlineOne significant issue with SELECT * queries is that not all columns are stored inline within the page. Large columns, such as text or blobs, may be stored in external tables and only retrieved when requested (Postgres TOAST tables are example). These columns are often compressed, so when you perform a SELECT * query with many text fields, geometry data, or blobs, you place an additional load on the database to fetch the values from external tables, decompress them, and return the results to the client.Network CostBefore the query result is sent to the client, it must be serialized according to the communication protocol supported by the database. The more data needs to be serialized, the more work is required from the CPU. After the bytes are serialized, they are transmitted through TCP/IP. The more segments you need to send, the higher the cost of transmission, which ultimately affects network latency.Returning all columns may require deserialization of large columns, such as strings or blobs, that clients may never use.Client DeserializationOnce the client receives the raw bytes, the client app must deserialize the data to whatever language the client uses, adding to the overall processing time. The more data is in the pipe the slower this process.UnpredictabilityUsing SELECT * on the client side even if you have a single field can introduce unpredictability. Think of this example, you have a table with one or two fields and your app does a SELECT * , blazing fast two integer fields.However, later the admin decided to add an XML field, JSON, blob and other fields that are populated and used by other apps. While your code did not change at all, it will suddenly slow down because it is now picking up all the extra fields that your app didn’t need to begin with.SummaryIn conclusion, a SELECT * query involves many complex processes, so it’s best to only select the fields you need to avoid unnecessary overhead. Keep in mind that if your table has few columns with simple data types, the overhead of a SELECT * query might be negligible. However, it’s generally good practice to be selective about the columns you retrieve in your queries.Back to Parent Page" }, { "title": "Part 5: Database Engineering Fundamentals: How Shopify’s engineering improved database writes?", "url": "/database-engineering-fundamental-part-5/how-shopify-engineering-improved-database-writes/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-05 02:00:00 +0530", "snippet": "How Shopify’s engineering improved database writes by 50% with ULIDhttps://www.youtube.com/watch?v=f53-Iw_5ucAUse Idempotency KeysDistributed systems use unreliable networks, even if the networks look reliable most of the time. At Shopify’s scale, a once in a million chance of something unreliable occurring during payment processing means it’s happening many times a day. If this is a payments API call that timed out, we want to retry the request, but do so safely. Double charging a customer’s card isn’t just annoying for the card holder, it also opens up the merchant for a potential chargeback if they don’t notice the double charge and refund it. A double refund isn’t good for the merchant’s business either.In short, we want a payment or refund to happen exactly once despite the occasional hiccups that could lead to sending an API request more than once. Our centralized payment service can track attempts, which consists of at least one or more (retried) identical API requests, by sending an idempotency key that’s unique for each one. The idempotency key looks up the steps the attempt completed (such as creating a local database record of the transaction) and makes sure we send only a single request to our financial partners. If any of these steps fail and a retried request with the same idempotency key is received, recovery steps are run to recreate the same state before continuing. Building Resilient GraphQL APIs Using Idempotency describes how our idempotency mechanism works in more detail.An idempotency key needs to be unique for the time we want the request to be retryable, typically 24 hours or less. We prefer using an Universally Unique Lexicographically Sortable Identifier (ULID) for these idempotency keys instead of a random version 4 UUID. ULIDs contain a 48-bit timestamp followed by 80 bits of random data. The timestamp allows ULIDs to be sorted, which works much better with the b-tree data structure databases use for indexing. In one high-throughput system at Shopify we’ve seen a 50 percent decrease in INSERT statement duration by switching from UUIDv4 to ULID for idempotency keys.https://shopify.engineering/building-resilient-payment-systemsBack to Parent Page" }, { "title": "Part 5: Database Engineering Fundamentals", "url": "/database-engineering-fundamental-part-5/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-05 01:00:00 +0530", "snippet": "Database Security Enabling TLS and SSL in postgres.conf file don’t allow larger query as if around 14MB, it will crash serverBest Practices Working with REST &amp; Databases give different access to different user/client/application table own by one user should not be modified by another table. better to use database management toolsHomomorphic EncryptionWhy we can’t always encrypt? Database Queries can only be performed on plain text Analysis, Indexing, tuning Applications must read data to process it TLS Termination Layer 7 Reverse Proxies and Load BalancingMeet Homomorphic Encryption! Ability to perform arithmetic operations on encrypted data No need to decrypt! You can query a database that is encrypted! Layer 7 Reverse Proxies don’t have to terminate TLS, can route traffic based on rules without decrypting traffic Databases can index and optimize without decrypting dataExample,https://github.com/IBM/fhe-toolkit-linuxit is not Production ready yet. It takes 2 mins to fetch record in smaller DB.Questions and AnswersHeap index scan instead of index only scan?It is possible that statistics is not updated which causes the database to think based on incorrect information. This statistics is used by database to plan the execution and decide the index that it will use to look for the required data.In order to update statistics, we have to execute vacuum command in the database to trigger clean up of the unused page and also updation of the statistics.What is cost in the execution plan?It is effort/cost required by Database to fetch the data. It is not in milliseconds but number. higher means more cost. There is a possibility that cost in execution plan can be higher than the execution plan. It is probably due to statistics not been update. Well we have to execute vacuum command.All Isolation levelsPostgreSQL supports several isolation levels that control the visibility of data changes to other transactions. The isolation levels define how transactions interact with each other when reading and writing data, ensuring consistency and correctness of operations in multi-user environments.Here’s a breakdown of the four main isolation levels in PostgreSQL: Read Uncommitted (Not supported in PostgreSQL, but conceptually available in other DBMS like MySQL or SQL Server). Read Committed (default in PostgreSQL). Repeatable Read. Serializable.1. Read Committed (Default in PostgreSQL)In the Read Committed isolation level, each query within a transaction sees a consistent view of the database, but data changes made by other transactions can be visible during the execution of the transaction. This means: If two transactions run concurrently, one transaction might see the changes made by the other before it commits.Example Scenario: Transaction 1 starts and updates a record. Transaction 2 starts and reads the same record, seeing the changes made by Transaction 1, even though Transaction 1 hasn’t committed yet.2. Repeatable ReadIn Repeatable Read, the database guarantees that any data read by a transaction will remain the same throughout the entire transaction. This prevents “non-repeatable reads,” where a transaction could read different values for the same data if another transaction modifies it. Phantom Reads are still possible in this isolation level, which means that if new rows are added to the database, they might not be visible to the current transaction.Example Scenario: Transaction 1 starts and reads a record. Transaction 2 starts and updates the same record. Transaction 1 reads the record again and sees the same value as initially read.3. SerializableThe Serializable isolation level ensures the strictest consistency, where transactions are executed in a way that they could be serially ordered (i.e., one after the other) without conflict. This level prevents dirty reads, non-repeatable reads, and phantom reads. This isolation level effectively serializes access to the data, as if the transactions were run sequentially, ensuring that the final database state is the same as if the transactions were processed one by one, without overlap.Example Scenario: Transaction 1 starts and reads a record. Transaction 2 starts and attempts to update the same record but is blocked until Transaction 1 is completed.Key Differences Between Isolation Levels Isolation Level Dirty Reads Non-Repeatable Reads Phantom Reads Description Read Uncommitted Yes Yes Yes Transactions can see uncommitted changes from other transactions. (Not supported in PostgreSQL) Read Committed No Yes Yes Transactions see only committed data, but results might change between queries. Repeatable Read No No Yes Transactions see consistent data, but new rows can be inserted, leading to phantom reads. Serializable No No No Transactions are executed as if they were serially ordered, avoiding all anomalies. Repeatable read vs Snapshot IsolationIn repeatable read, if max value in the table is queried to be found that it will behave different in below scenario,If after query, some other transaction insert new record that changes the max score value, then next time execution of same query will return different value. so it is like a phantom read.This problem is solved by snapshot isolation level, here it says read rows older than the time it stared the transaction so newer record will not be read. so execution of same query like above will return same value both times.In postgres, repeatable read is same as snapshot isolation. basically internal implementation is different. due to versioning it by default solves the phantom read problemPostgres is using sequence scan instead of index scan on smaller table.I am using postgresql. I have created a table with 4 columns (id, first_name, last_name, dob), and created an index on id which is a primary key as well. When i do select query likeselect id where id = 2It is using seq scan rather than index-only scan. What’s the issue here?This table has only 7 rows.Answer:Your last statement is why.The 7 rows fits nicely in one page in the heap, so in postgres optimizer says hey. The entire table is one page I can just read that page directly instead of cracking up the id index and read the btree and go through the btree complex structure. My guess is the index is never loaded to memory at this stage too because the table is too small and not worth it.Add another 10k rows and it the plan will changeFetching page vs row in indexwhere you explain how indexes and table are stored, you mentioned that when we execute sql query, behind the scenes the whole page is fetched and only the row needed is filtered and sent to user. I have 2 questions:a) why does it get the whole page, why not just the row since the db now already knows the row location.b) How does this work during index? I mean an index will point you to the exact row, so does it still fetches the whole page the row is in?Answer:A) that is how disks work , you can’t fetch one row you have to fetch a “block” this is unlike RAM where you can do byte level addressing. There are new SSDs called ultraram that allows for byte addressability which will allow us to do what you are saying but for now we have to fetch a page and we get multiple rows with it.B) the index itself is stored on disk correct? And anything stored in disk has to be stored in pages :) so yes even the index read in pages and we get a bunch of row entries. Of course once a page is in memory you can pull any byte off it.Follow-up:it doesn’t make sense to me that the index is stored on disk !! won’t be better if the index is stored in memory for faster search &amp; access?  idk, if the index is relatively small, won’t it be stored in memory instead?Answer:the indexes are stored in memory but also stored on disk. otherwise if you shutdown the database you will have to recreate all indexes or all tables of all databases which is not feasiblefurthermore you will run out of memory what do you do in that case? do you lose your index.you need to store the index on disk and load it on memory for performanceIndex on column with duplicate valuesIf we are creating index on a column which can have duplicate values then how is it stored in index data structure?Does it maintain a key with name as duplicate value and values would be list of rows and corresponding page information?Answer:Good question, databases implements this differently. Postgres for the longest time stored the duplicated values in the index for each key. Then later improved thati in postgres 13 by deduplicating it (k1-(row1,row2,row7)Instead of k1-row1 / k1-row2 / k1-row7Is indexing a boolean column useful, let’s suppose we have a table which contains a processed flag, as rows are processed, this flag is marked as true/1. However, this table can contain huge number of rows and new rows are inserted with processed=false/0 for processing, so we need to query these columns periodically to process. Is it useful to add index on this column?Answer:this depends on the selectively of the boolean column.say you have your processed field,if we know that unprocessed rows (processed = 0 ) is FAR less than processed =1 say 1% of the rows are unprocessed then indexing is useful especially if you only query for unprocessed rowsbut if you query for processed rows = 1 you know you will get massive rows and the index will not he as useful.remember index is only useful if the rows that you predict to come back are smallDeduplication of B-tree Indexes in PostgreSQL 13In PostgreSQL 13, one of the significant changes related to indexing was the improvement in B-tree index deduplication. B-trees are a common data structure used for indexing in databases, and they are used by default in PostgreSQL for most index types. B-tree indexes organize data in a balanced tree structure that allows for fast search, insert, and delete operations.Previously, when multiple identical values were inserted into a B-tree index (for example, inserting rows with the same key value), each of these duplicate values would take up space in the index, even though they essentially represent the same data in the context of the index. This could lead to inefficient use of disk space and performance issues, especially with large datasets.With the introduction of deduplication in PostgreSQL 13, when multiple rows with identical indexed values are inserted, PostgreSQL now eliminates the need for storing duplicate entries in the index. Instead, it only stores the value once and maintains a list of the associated tuple (row) locations. This can significantly improve the efficiency of indexing, reduce index size, and improve performance for certain types of queries that involve duplicate indexed values.Benefits of B-tree Index Deduplication Reduced Disk Space Usage: Deduplication reduces the number of entries stored in the index, thereby saving disk space. Improved Performance: Since the index becomes smaller and more compact, searches, insertions, and deletions involving the indexed column can become faster. Faster Index Maintenance: When indexes are smaller, PostgreSQL can rebuild and maintain them more efficiently.Use CasesThis feature is particularly beneficial for columns where there are many duplicate values, such as: Status fields (e.g., active/inactive flags) Gender fields (e.g., “Male”, “Female”) Categorical data (e.g., product categories)Technical Details The deduplication happens automatically when creating or maintaining B-tree indexes. The database checks for duplicate values when inserting into the index and keeps only one entry for each distinct value. It also stores pointers to the corresponding tuples in the index, so the database can still retrieve the correct rows when queried.Scenario: Table of Sales RecordsLet’s consider a table of sales records, where we want to create an index on the product_id column. Many sales records might involve the same product, so there will be lots of duplicate product_id values.CREATE TABLE sales ( sale_id SERIAL PRIMARY KEY, product_id INT, quantity INT, sale_date DATE);Step 1: Insert Some DataLet’s say we insert a few rows into the sales table:INSERT INTO sales (product_id, quantity, sale_date) VALUES (1, 100, '2024-01-01'), (2, 50, '2024-01-02'), (1, 200, '2024-01-03'), (1, 150, '2024-01-04'), (3, 300, '2024-01-05');Now, the sales table looks like this: sale_id product_id quantity sale_date 1 1 100 2024-01-01 2 2 50 2024-01-02 3 1 200 2024-01-03 4 1 150 2024-01-04 5 3 300 2024-01-05 We have a product_id of 1 appearing multiple times (for product 1).Step 2: Create an Index on product_idNow, you decide to create an index on the product_id column to speed up queries that look for specific products:CREATE INDEX idx_product_id ON sales(product_id);Step 3: How the Index Looks Before PostgreSQL 13Before PostgreSQL 13, a B-tree index for product_id would look something like this (with no deduplication): product_id tuple IDs (row pointers) 1 (1, 3, 4) 2 (2) 3 (5) Here: For product_id = 1, we store three pointers (to rows 1, 3, and 4) because the same product appears in multiple rows. For product_id = 2, there is one pointer (to row 2). For product_id = 3, there is one pointer (to row 5).Step 4: Deduplication in PostgreSQL 13In PostgreSQL 13 (and later), the deduplication feature in B-tree indexes changes this process.Instead of storing multiple copies of product_id = 1 in the index, PostgreSQL 13 will store only one entry for each distinct value in the indexed column, with a list of tuple pointers (row IDs) for each occurrence.So, after deduplication, the index looks like this: product_id tuple IDs (row pointers) 1 (1, 3, 4) 2 (2) 3 (5) Explanation of Deduplication: Before deduplication (Pre-PostgreSQL 13): The index stored multiple entries for the same product_id if it appeared multiple times. For product_id = 1, it stored 3 entries, each pointing to a different row. After deduplication (PostgreSQL 13): The index stores only one entry for product_id = 1. Instead of multiple entries for each instance of product_id = 1, it only stores one product_id = 1 with a list of tuple IDs (row pointers) that refer to the actual rows where that product appears (rows 1, 3, and 4). Step 5: Benefits of Deduplication Reduced Index Size: In the deduplicated index, product_id = 1 only needs one entry (with a list of row pointers), whereas in the pre-PostgreSQL 13 index, product_id = 1 would have had three separate entries. This reduces the overall size of the index, which is especially important when dealing with large tables with many duplicates. Improved Performance: Query performance for searches on the indexed column (product_id) will be faster because the index is smaller and more efficient. For example, a search for product_id = 1 can quickly retrieve the row pointers without needing to traverse multiple entries. More Efficient Insertions/Updates: When new rows are inserted or existing rows are updated, PostgreSQL doesn’t need to insert multiple copies of the same product_id in the index. This saves time and resources, improving overall database performance. Example Query:Let’s say you run a query to find all sales for product_id = 1:SELECT * FROM sales WHERE product_id = 1;With the deduplicated index: PostgreSQL will find the index entry for product_id = 1, and since it points to rows 1, 3, and 4, it will directly fetch those rows.Conclusion:Deduplication in PostgreSQL 13 B-tree indexes significantly reduces index size and improves query performance by ensuring that each distinct value in the indexed column is stored only once in the index, even if it appears multiple times in the table. This is particularly useful in cases where there are many duplicate values in the indexed column.Lock with serializable isolationYou mentioned that serializable isolation level transactions are run one after another. So what’s a point of lock with pessimistically implementation with SELECT FOR UPDATE?Answer:Good question, the serializable isolation level will give you the result of having each transaction run one after the other without actually having the transactions being blocked. The transactions can still run concurrently in serializable isolation level and when the DBMS detects that a change will be out of order it issues an error (serialization failure). This is using optimistic concurrency control.With SELECT FOR UPDATE you achieve the serialization by actually blocking the transactions from running concurrently so you will get the same result but its at the cost of concurrency. Plus you won’t fail on other word pessimistic concurrency controlSummary serialization isolation level uses optimistic concurrency control which can be faster than pessimistic. But can fail and transactions will need to be retriedSuppose i have products table, i have two outlets of my shop accessing the same database (serializable isolation level)Consider two scenarios,scenario 1:(both transactions start simultaneously)T1:update QTY of product AT2:update QTY of product BSo here i assume that since both the transactions are accessing different rows, they will be ran concurrentlyscenario 2:(both transactions start simultaneously)T1:update QTY of product AT2:update QTY of product ASo here i assume that since both the transactions are accessing same row it will be ran concurrently and then DBMS will throw serialization failure error and hence it will fail both the transaction (or will it retry automatically but in a serializable way?)because both accessing the same the row the first transaction to update the row holds a FOR UPDATE lock, even in serializable mode.so the second transaction will just wait and be blocked. once the first transaction commits the second transaction attempts to update the same row and it will see the the value has changed from its snapshot and it will fail with a serilization error.remember that serializable doesn’t force the trans to run one after the other physically. they can still run concurrently as long as this rule is satisfiedif tx1 read x then tx2 changed it, tx1 will fail to commitif tx1 writes something that tx2 then reads that also fails because tx2 is now reading something that has changed. and as a result can’t guarantee the orderHOT updates in Postgres (by design some space in the page is left empty to accommodate future update in same page/location)In PostgreSQL, HOT (Heap-Only Tuple) updates refer to an optimization technique that aims to reduce the overhead of index updates when modifying a row in a table. The primary goal of HOT updates is to improve write performance by avoiding unnecessary index maintenance during certain kinds of row updates.What is a HOT Update?A HOT update occurs when a row in a table is updated in such a way that it doesn’t require any changes to the indexes that reference the row. This is possible when the update: Does not affect indexed columns (i.e., columns that are part of any index). The row remains physically in the same position in the table.In this case, PostgreSQL can simply update the tuple in the heap (the main storage of table data) without having to modify the associated index entries. This reduces the need for additional writes to the index, which is often the more expensive part of an update operation.How Does HOT Update Work Internally?Internally, PostgreSQL stores table data as tuples in a heap file. When an update is performed on a row, PostgreSQL typically writes a new version of the row to the heap and marks the old version as “dead” (in a special state called MVCC—Multi-Version Concurrency Control). The dead tuples are later cleaned up by a VACUUM process.In the case of a HOT update, the new tuple is written in place in the same physical location in the heap, without modifying any indexes. This allows the update to be more efficient than a traditional update, which would require index entries to be updated, potentially leading to more disk I/O.Here’s how HOT update works step by step: Update Request: An update is issued to a row, but the update does not modify any indexed columns (or does not change the values in such a way that it would require an index update). Heap Update: PostgreSQL checks whether the update can be classified as a HOT update. If the update can be performed without needing to update the index, the row is updated directly in the table’s heap. Tuple Versioning: The old version of the tuple is marked as obsolete but isn’t physically deleted until the next vacuum cycle (due to MVCC). The new version is written in the same location. No Index Update: Since indexed columns are not modified, no changes are made to the indexes, reducing the overhead of maintaining the index structures. Tuple Visibility: When querying the table, PostgreSQL ensures that the most recent tuple version (i.e., the new version after the HOT update) is visible to transactions, using its MVCC mechanism.Example of HOT UpdateLet’s take an example with a simple table:CREATE TABLE employees ( id SERIAL PRIMARY KEY, name TEXT, salary INT);And let’s say we have an index on the salary column:CREATE INDEX idx_salary ON employees(salary);Now, suppose we have a row with id=1, name='Alice', and salary=50000:INSERT INTO employees (name, salary) VALUES ('Alice', 50000);Now, let’s update this row to change the name:UPDATE employees SET name = 'Alicia' WHERE id = 1;In this case, the update only modifies the name column, which is not indexed, so PostgreSQL will perform a HOT update: Heap update: The new name (‘Alicia’) is written in the same heap tuple location. No index update: The salary column, which is part of the index, is not modified, so the index on salary is not updated. The old tuple (with name='Alice') is marked as dead but not removed immediately. The index on salary remains unchanged.When HOT Updates Are Not PossibleHOT updates are not possible when: Indexed columns are modified: If any column that is part of an index is updated, the index will need to be updated, and the update cannot be a HOT update. In such cases, PostgreSQL has to update the index, which incurs more overhead. For example: UPDATE employees SET salary = 55000 WHERE id = 1; Here, the salary column is indexed, so the index on salary must be updated, and a HOT update cannot be used. Row is moved: If the update changes the physical location of the row (e.g., due to the row becoming too large), a new version of the tuple will be written in a different location in the heap. This will require index updates to reflect the new location of the tuple. The table is not well-suited for HOT updates: If a table is heavily indexed, HOT updates will be less common because even minor changes may require updating the indexes.Performance Impact of HOT UpdatesHOT updates can significantly improve write performance because they reduce the need for index maintenance during updates. This means that: Fewer I/O operations: There is less disk I/O because the index does not need to be updated. Faster update times: Since the index does not need to be updated, the overall time to perform an update is faster. Reduced contention: Since indexes are not modified, there is less contention between different transactions trying to modify the same index.However, there are some trade-offs: Vacuum overhead: Dead tuples accumulate faster because updates are done in place without cleaning up old rows immediately. This means the vacuum process has to work harder to clean up old tuples that are no longer visible. Hotspotting: If rows are frequently updated, the same tuple may be rewritten in place multiple times, leading to potential tuple chaining or index bloat (especially if there are more complex updates later on).Example of Performance ImpactConsider the following two scenarios: Without HOT Update (Traditional Update): You have an index on salary. Every time the salary column is updated, PostgreSQL needs to: Write the new version of the row. Update the index on salary to point to the new location of the row. With HOT Update: You update the name column (which is not indexed). PostgreSQL can update the row in place without modifying the salary index, reducing I/O. If you update name frequently, PostgreSQL will only have to update the heap and not the index, which results in significant performance improvements in cases of high update traffic.ConclusionHOT updates are a performance optimization in PostgreSQL that allows for faster row updates by avoiding unnecessary index modifications. When certain conditions are met (such as not modifying indexed columns), PostgreSQL will update the row directly in the heap, reducing the overhead of maintaining indexes. This results in better performance, particularly in write-heavy applications, but comes with trade-offs like increased need for vacuuming to clean up dead tuples.How to choose the order of columns to create a composite index? If I have a query that does a filter on say 10 columns (joined by AND), is it advisable to have a composite index on all 10 columns? Let’s assume for simplicity that this is the only query for this table Is the Query Planner smart enough to arrange the filter in an order that will be a subset of the composite index? For example if I have a composite index on a,b, c and d columns. I hit a query saying select a,b,c,d where c=10 and a=20 and b=30; Will the Query Planner use the composite index? Is there good practice for the order in which the column order for composite indexes be chosen? Answer:1) you see think about how a composite index works, It includes all values of indexed columns in the b-tree structure. This increases the size of the index which leads to more IOs to read. This of course depends on the data types of what you are indexing. Another side effect is updates to any of those columns Would require updating the index, this is not necessary slow but just increases IO.I would be pragmatic and find out the frequency of values in each column and only index the column that would give me maximum benefit, this requires you understanding the data model, the nature of the data stored and the correlation between them.2) yes, the planner will do what’s necessary the order of the where clause doesn’t matter as long as everything is an AND.3) depends on your where clause and the minimum set of the query.Redis “Durability” vs “Persistence”You said that Redis doesn’t offer Durability because of course it is memory database “lives in RAM” if electricity goes down, data is lost. And also yo said but it has Persistence. I need to know what do you mean by that and i want to understand from your point of view. What is Durability vs Persistence?Thanks in advance and thank you so much for this amazing lecture, i loved it so much and i installed Postgres on my RaspberryPi and went with all examples and applied them all and took 160 lines of notes and steps :DAnswer:Persistence is the ability and feature that a database provide to persist and store data on disk.Durability is when you successfully write a value to the DB it should always persist to disk so it is available when the DB crashes/restarts.Redis is an in memory database that supports persistence but they do offer true durability.Redis persist data in an asynchronous snapshot every x seconds. So you can write a value in memory but if the power goes off before it gets persisted to snapshot, you lost it..I believe this might have changed in the recent versions of redis. Subtle difference but important to point out.Redis uses AOF (Append Only File) and hence supports high durability at high throughput. It also allows controlling the knob (Snapshot vs AOF)Reference to related articles Postgres vs MySQL (The fundamental differences) PostgreSQL Process Architecture WAL, Redo and undo logs in postgres How Shopify’s engineering improved database writes by 50% with ULID Postgres Locks — A Deep Dive How Slow is select * in row store? Why Uber Engineering Switched from Postgres to MySQL NULL.pdf Write Amplification Problem in PostgreSQL TOAST table in Postgres InnoDB B-tree Latch Optimization History" }, { "title": "Part 4: Database Engineering Fundamentals", "url": "/database-engineering-fundamental-part-4/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-04 01:00:00 +0530", "snippet": "Database Engine FundamentalsWhat is a Database Engine? Library that takes care of the on-disk storage and CRUD operations Can be as simple as a key-value store Or as rich and complex as full ACID support with transactions and foreign keys DBMS can use the database engine and build features on top (server, replication, isolation, stored procedures, etc.) Want to write a new database? Don’t start from scratch - use an engine Sometimes referred to as Storage Engine or embedded database Some DBMS gives you the flexibility to switch engines like MySQL &amp; MariaDB Some DBMS comes with a built-in engine that you can’t change (PostgreSQL)Popular Database EnginesMyISAM Stands for Indexed Sequential Access Method B-tree (Balanced tree) indexes point to the rows directly No transaction support Open Source &amp; Owned by Oracle Inserts are fast, updates and deletes are problematic (fragments) Database crashes corrupt tables (have to manually repair) Table level locking MySQL, MariaDB, Percona (MySQL forks) supports MyISAM Used to be default engine for MySQLAria Created by Michael Widenius Very similar to MyISAM Crash-safe unlike MyISAM Not owned by Oracle Designed specifically for MariaDB (MySQL Fork) In MariaDB 10.4 all system tables are AriaInnoDB B+tree - with indexes point to the primary key and the PK points to the row Replaces MyISAM Default for MySQL &amp; MariaDB ACID compliant transactions support Foreign keys Tablespaces Row level locking Spatial operations Owned by OracleXtraDB Fork of InnoDB Was the default for MariaDB until 10.1 In MariaDB 10.2 for InnoDB switched the default “XtraDB couldn’t be kept up to date with the latest features of InnoDB and cannot be used.” link System tables in MariaDB starting with 10.4 are all AriaSQLite Designed by D. Richard Hipp in 2000 Very popular embedded database for local data B-Tree (LSM as extension) PostgreSQL-like syntax Full ACID &amp; table locking Concurrent read &amp; writes Web SQL in browsers uses it Included in many operating systems by defaultBerkeley DB Developed by Sleepycat Software in 1994 (owned by Oracle) Key-value embedded database Supports ACID transactions, locks, replications etc. Used to be used in Bitcoin Core (switched to LevelDB) Used in MemcacheDBLevelDB Written by Jeff and Sanjay from Google in 2011 Log structured merge tree (LSM) (great for high insert and SSD) No transactions Inspired by Google BigTable Levels of files: Memtable Level 0 (young level) Level 1 - 6 As files grow, large levels are merged Used in Bitcoin Core blockchain, AutoCAD, MinecraftRocksDB Facebook forked LevelDB in 2012 to become RocksDB Transactional High Performance, Multi-threaded compaction Many features not in LevelDB MyRocks for MySQL, MariaDB and Percona MongoRocks for MongoDB Many more projects use it!Database CursorsWhat are Database Cursors?Database cursors are powerful tools that allow for row-by-row processing of result sets. In PostgreSQL, a cursor is a database object used to retrieve rows from a result set one at a time. This is particularly useful when working with large datasets or when you need to fetch data incrementally.Cursor Capabilities: Efficient row-by-row processing: Fetch and process rows one at a time or in small chunks Multiple result sets: Keep a cursor open to fetch additional rows later, helpful for procedural processingKey Steps for Using a Cursor in PostgreSQL: Declare a cursor – Define a cursor to point to a specific query or result set Open the cursor – Execute the query and generate the result set Fetch rows from the cursor – Retrieve individual or multiple rows Close the cursor – Release resources after processingExample of Using Cursors in PostgreSQL:Step 1: Declare the cursorTo declare a cursor, you need to use the DECLARE statement, followed by the cursor name and the SQL query.-- Declare a cursorDECLARE my_cursor CURSOR FORSELECT id, name FROM employees WHERE department = 'Sales';Here, my_cursor is the name of the cursor, and the SELECT statement fetches id and name columns from the employees table where the department is ‘Sales’.Step 2: Fetch rows from the cursorAfter declaring the cursor, you can use the FETCH command to retrieve rows from the result set.-- Fetch the first rowFETCH NEXT FROM my_cursor;You can also specify how many rows to fetch at once, such as FETCH 5 FROM my_cursor to fetch 5 rows at a time.Step 3: Loop to fetch multiple rowsYou can use a loop to fetch all rows one by one. Here is an example using a LOOP in PL/pgSQL, PostgreSQL’s procedural language.DO $$DECLARE rec RECORD;BEGIN -- Declare the cursor DECLARE my_cursor CURSOR FOR SELECT id, name FROM employees WHERE department = 'Sales'; -- Open the cursor OPEN my_cursor; -- Fetch and process rows LOOP FETCH NEXT FROM my_cursor INTO rec; EXIT WHEN NOT FOUND; -- Exit when no more rows -- Process the row (For example, just outputting the data) RAISE NOTICE 'Employee ID: %, Name: %', rec.id, rec.name; END LOOP; -- Close the cursor CLOSE my_cursor;END $$;In this example: rec is a RECORD type variable that holds the fetched row. The LOOP continues fetching rows using the FETCH NEXT statement until there are no more rows (EXIT WHEN NOT FOUND). After processing, the cursor is closed with CLOSE my_cursor.Step 4: Close the cursorCLOSE my_cursor;Once you’ve finished working with the cursor, it is good practice to close it. This is done with the CLOSE statement, as shown above.Important Notes: Implicit Cursors: PostgreSQL automatically creates implicit cursors for SELECT queries outside of procedural code. For complex operations, explicit cursors are necessary. Cursor Types: Simple cursor: Basic cursor that fetches rows in order Scroll cursor: Allows fetching rows both forward and backward No scroll cursor: Can only fetch rows in one direction (default) Memory Considerations: Cursors are more memory-efficient than loading entire result sets, but should be closed when no longer needed.Example in a Transaction Block:BEGIN;-- Declare and open the cursorDECLARE my_cursor CURSOR FORSELECT id, name FROM employees WHERE department = 'HR';-- Fetch and process rowsFETCH NEXT FROM my_cursor;-- Close the cursorCLOSE my_cursor;COMMIT;Pros and Cons of Database CursorsPros of Using Cursors Memory Efficiency: Process rows one at a time or in small batches Improved resource management for large result sets Avoids loading entire datasets into memory at once Better Performance for Large Datasets: Sequential processing can be more efficient for certain procedural operations Ability to pause and resume fetching process Advantageous for complex workflows or long-running tasks Control over Row Fetching: Control over order and frequency of row fetching Explicit iteration through result sets Scroll cursors provide flexibility for both forward and backward fetching Complex Query Handling: Useful for processing complex queries row-by-row Commonly used in stored procedures or functions Ideal for operations that need to be executed in steps Transactional Processing: Process data incrementally within transactions Useful for batch updates or long-running transactions Cons of Using Cursors Performance Overhead: Multiple context switches between database and application code Opening, fetching, and closing a cursor can be more costly than executing a single query Performance issues can be exacerbated for unoptimized queries Complexity and Maintenance: Adds complexity to code Requires explicit handling of cursor lifecycle Forgetting to close cursors can lead to resource leaks Concurrency Issues: Can lock resources depending on usage May hold locks on rows during fetching Potential for deadlocks if not handled properly Limited Use Case: Not always necessary; simple SQL queries often suffice Set-based operations are usually more efficient Often overkill for read-heavy use cases Potential for Unintended Side Effects: Stateful nature can lead to unexpected results if data changes between fetches May cause long-running transactions Potential for transaction contention or deadlocks Resource Management: Requires explicit resource management Open cursors consume memory and resources Can cause performance issues in high-concurrency environments When to Use Cursors: When row-by-row processing is necessary When memory constraints are a concern When implementing procedural logic in stored procedures or functions When performing complex updates or deletions with multiple operations per rowWhen to Avoid Cursors: For simple queries that don’t require row-by-row processing When performance is critical For read-heavy operations better handled by set-based SQL For simple CRUD operations where set-based operations are more efficientImplementing Cursor-like Functionality in Spring BootIn a Spring Boot application using Spring Data JPA, we typically work with repositories to perform CRUD (Create, Read, Update, Delete) operations on entities. However, if you need to implement cursor-like behavior (such as fetching results incrementally or processing rows one by one), you can use native SQL queries along with @Query annotation or JPA Criteria API in combination with pagination.While Spring Data JPA doesn’t directly provide a cursor concept like in PostgreSQL, you can simulate cursor-like behavior using pagination, streaming, or custom native queries.1. Using @Query Annotation with Pagination// Entity Class@Entitypublic class Employee { @Id private Long id; private String name; private String department; // Getters and Setters}// Repository Interfacepublic interface EmployeeRepository extends JpaRepository&lt;Employee, Long&gt; { // Custom query with pagination @Query(\"SELECT e FROM Employee e WHERE e.department = :department\") Page&lt;Employee&gt; findEmployeesByDepartment(String department, Pageable pageable);}// Service Layer@Servicepublic class EmployeeService { @Autowired private EmployeeRepository employeeRepository; public void processEmployeesInBatches(String department) { int page = 0; int pageSize = 10; // Batch size Pageable pageable = PageRequest.of(page, pageSize); Page&lt;Employee&gt; employeePage; // Fetch in batches (mimicking cursor behavior) do { employeePage = employeeRepository.findEmployeesByDepartment(department, pageable); employeePage.getContent().forEach(employee -&gt; { // Process each employee System.out.println(\"Processing Employee ID: \" + employee.getId()); }); // Move to next page (mimicking cursor movement) pageable = pageable.next(); page++; } while (employeePage.hasContent()); }}2. Using Stream for Processing Large Results// Repository with Streaming Query@Repositorypublic interface EmployeeRepository extends JpaRepository&lt;Employee, Long&gt; { @Transactional @Query(\"SELECT e FROM Employee e WHERE e.department = :department\") Stream&lt;Employee&gt; findEmployeesByDepartmentStream(String department);}// Service Layer (Streaming)@Servicepublic class EmployeeService { @Autowired private EmployeeRepository employeeRepository; public void processEmployeesInStream(String department) { // Open a stream to process the data lazily try (Stream&lt;Employee&gt; employeeStream = employeeRepository.findEmployeesByDepartmentStream(department)) { employeeStream.forEach(employee -&gt; { // Process each employee (row by row) System.out.println(\"Processing Employee ID: \" + employee.getId()); }); } }}3. Using Native SQL Queries with @Query and Streaming// Repository with Native Query@Repositorypublic interface EmployeeRepository extends JpaRepository&lt;Employee, Long&gt; { @Query(value = \"SELECT * FROM employees WHERE department = :department\", nativeQuery = true) Stream&lt;Employee&gt; findEmployeesByDepartmentNativeStream(String department);}// Service Layer (Using Native Stream)@Servicepublic class EmployeeService { @Autowired private EmployeeRepository employeeRepository; public void processEmployeesWithNativeQueryStream(String department) { try (Stream&lt;Employee&gt; employeeStream = employeeRepository.findEmployeesByDepartmentNativeStream(department)) { employeeStream.forEach(employee -&gt; { // Process each employee from the native query System.out.println(\"Processing Employee ID: \" + employee.getId()); }); } }}Server-Side vs. Client-Side CursorsServer-Side CursorA server-side cursor is managed by the database server, which handles the cursor state and row retrieval.Characteristics: Cursor management is done by the database server Client only issues fetch commands Memory-efficient as rows are sent as needed Ideal for large datasets Stateful: server maintains cursor position between fetchesAdvantages: Memory efficiency Better handling of large result sets Server-controlled optimizationsDisadvantages: Latency due to client-server communication Resource consumption on the serverExample in PostgreSQL:DECLARE my_cursor CURSOR FORSELECT id, name FROM employees WHERE department = 'Sales';Client-Side CursorA client-side cursor is managed by the client application, which retrieves the entire result set at once.Characteristics: Cursor management is done by the client application Client fetches all rows at once Client controls cursor position Used for smaller result sets that fit in memoryAdvantages: Simpler implementation Faster for small result sets Lower server loadDisadvantages: High memory consumption Inefficient for large result sets No incremental fetchingExample in Java (JDBC):Connection conn = DriverManager.getConnection(\"jdbc:postgresql://localhost:5432/mydb\", \"user\", \"password\");Statement stmt = conn.createStatement();ResultSet rs = stmt.executeQuery(\"SELECT id, name FROM employees\");while (rs.next()) { int id = rs.getInt(\"id\"); String name = rs.getString(\"name\"); // Process the row}Comparison: Server-Side vs. Client-Side Cursors Aspect Server-Side Cursor Client-Side Cursor Cursor Management Database server Client application Memory Usage Efficient: only portions in memory Inefficient: entire result set in memory Fetch Behavior Retrieving rows in batches All rows fetched at once Resource Consumption Server resources Client resources Use Case Large result sets Smaller result sets Performance Overhead from multiple round trips Faster for small datasets Latency Some latency due to batch fetching No latency once loaded Which One Should You Use? Server-Side Cursors: Best for large result sets where memory efficiency is crucial Client-Side Cursors: Appropriate for smaller result sets that fit comfortably in memorySQLServer-ServerSide-Cursor-Types.pdf" }, { "title": "Part 3: Database Engineering Fundamentals: MongoDB internal Architecture", "url": "/database-engineering-fundamental-part-3/mongodb-internal-architecture/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-03 04:00:00 +0530", "snippet": "MongoDB internal ArchitectureHussein NasserFollowandroidstudio·December 12, 2022 (Updated: February 25, 2023)·Free: NoI’m a big believer that database systems share similar core fundamentals at their storage layer and understanding them allows one to compare different DBMS objectively. For example, How documents are stored in MongoDB is no different from how MySQL or PostgreSQL store rows.Everything goes to disk, the trick is to fetch what you need from disk efficiently with as fewer I/Os as possible, the rest is API.Here my tweet to a very common question I get that clarify the difference between SQL vs NOSQL.In this article I discuss the evolution of MongoDB internal architecture on how documents are stored and retrieved focusing on the index storage representation. I assume the reader is well versed with fundamentals of database engineering such as indexes, B+Trees, data files, WAL etc, you may pick up my database course to learn the skills.Let us get started.Components of MongoDBIn order to explore the MongoDB architecture I think it is important to understand some of the components of the database system.Let us start by exploring the basic components in Mongo that I believe is relevant to our discussion.Documents &amp; CollectionsMongoDB is a document based NOSQL database which means it doesn’t deal with relational schema-based tables but rather non-relational schema-less documents.Users submit JSON documents to Mongo where they are stored internally as BSON (Binary JSON) format for faster and efficient storage. Mongo retrives BSON and cover them back to JSON for user consultation. Here is a definition of BSON from MongoDB BSON stands for “Binary JSON,” and that’s exactly what it was invented to be. BSON’s binary structure encodes type and length information, which allows it to be traversed much more quickly compared to JSON.Because documents can be large, Mongo can sometimes compress them to further reduce the size. Mongo didn’t always compress BSON and we will explore this later.Users create collections (think of them as tables in RDBMS) which hold multiple documents. Because MongoDB is schema-less database, collections can store documents with different fields and that is fine. Users can submit document with a field that never existed any document in the collection. This feature is one that Mongo attractive to devs and it is also the feature that is abused the most._id indexWhen you create a collection in Mongo, a primary key _id representing the document id is created along side a B+Tree index so that search is optimal. The _id uniquely identifies the document and can be used it to find the document.The _id type is objectId and it is a 12 bytes field. The reason it is large because Mongo uses it to uniquely identify the document across machines or shards for scalabilty. Read more about the structure of objectId here.I also think the user can override the _id field with a value of their choosing which could make the key even larger. Keep this mind as we go through the article.The _id primary index is used to map the _id to the BSON document through a B+Tree structure. The mapping itself has gone through stages as MongoDB evolved and we will also explore this.Secondary indexesUsers can create secondary B+Tree indexes on any field on the collection which then points back to BSON documents satisfying the index. This is very useful to allow fast traversal using different fields on the document not just the default _id field. Without secondary indexes, Mongo has to do a full collection scan looking for the document fields one by one.The size of the secondary index depends on two things, the key size which represents the field size being indexed, and the document pointer size. We will see that different versions of Mongo and storage engines makes big difference on that.Now that we know the main components of Mongo, let us explore the evolution of Mongo internals.Original MongoDB ArchitectureWhen Mongo first released, it used a storage engine called MMAPV1 which stands for Memory Map files. In MMAPV1 BSON documents are stored directly on disk uncompressed, and the _id primary key index maps to a special value called Diskloc. Diskloc is a pair of 32 bit integers representing the file number and the file offset on disk where the document lives.When you fetch a document using its _id, the B+Tree primary key index is used to find the Diskloc value which is used to read the document directly from disk using the file and offset.As you might have guessed, MMAPV1 comes with some of limitations. While Diskloc is an amazing O(1) way to find the document from disk using the file and offset, maintaining it is difficult as documents are inserted and updated. When you update a document, the size increases changing the offset values, which means now all Diskloc offsets after that document are now off and need to be updated. Another major limitation with MMapv1 is the single global databse lock for writes, which means only 1 writer per database can write at at time, signfically slowing down concurrent writes. In this image you can see the leaf pages of the B+Tree are linked together for effective range scans, this is a property of B+Trees.One index lookup is required in this architecture plus an I/O 𝑂(𝑙𝑜𝑔𝑛) + 𝑂(1)Mongo did improve MMapv1 to make it a collection level lock (table level lock) but later depreceated MMapv1 in 4.0 in favor of WiredTiger, their new and default storage engine.MongoDB WiredTiger ArchitectureIn 2014. MongoDB acquired WiredTiger and made it their default storage engine. WiredTiger has many features such as document level locking and compression. This allowed two concurrent writes to update different documents in the same collection without being serialized, something that wasn’t possible in the MMAPV1 engine. BSON documents in WiredTiger are compressed and stored in a hidden index where the leaf pages are recordId, BSON pairs. This means more BSON documents can be fetched with fewer I/Os making I/O in WiredTiger more effective increasing the overall performance.Primary index _id and secondary indexes have been changed to point to recordId (a 64 bit integer) instead of the Diskloc. This is a similar model to PostgreSQL where all indexes are secondary and point directly to the tupleid on the heap.However, this means if a user looks up the _id for a document, Mongo uses the primary index to find the recordId and then does another lookup on the hidden WT index to find the BSON document. The same goes for writes, inserting a new document require updating two indexes.Two look ups are required in this architecture 𝑂(𝑙𝑜𝑔𝑛) + 𝑂(𝑙𝑜𝑔𝑛)The double lookup cost consumes CPU, memory, time and disk space to store both primary index and the hidden clustered index. This is also true for secondary indexes. I can’t help but remember a blog article by Discord why they moved away from Mongo to Cassandra, one reason was their data files and indexes could no longer fit RAM. The storage of both indexes might have exacerbated their problem but I could be wrong.Despite the extra I/O and double index storage for _id in this architecture, both primary and secondary indexes are still predictable in size. The recordId is a 64 bit which is relatively small. Keep this in mind because this is going to change one more time.Clustered Collections ArchitectureClustered collections is a brand new feature in Mongo introduced in June 2022. A Clustered Index is an index where a lookup gives you all what you need, all fields are stored in the the leaf page resulting in what is commonly known in database systems as Index-only scans. Clustered collections were introduced in Mongo 5.3 making the primary _id index a clustered index where leaf pages contain the BSON documents and no more hidden WT index.This way a lookup on the _id returns the BSON document directly, improving performance for workloads using the _id field. No more second lookup.Only one index lookup is required in this architecture 𝑂(𝑙𝑜𝑔𝑛) against the _id indexBecause the data has technically moved, secondary indexes need to point to _id field instead of the recordId. This means secondary indexes will still need to do two lookups one on the secondary index to find the _id and another lookup on the primary index _id to find the BSON document. Nothing new here as this is what they used to do in non-clustered collections, except we find recordid instead of _id.However this creates a problem, the secondary index now stores 12 bytes (yes bytes not bits) as value to their key which significantly bloats all secondary indexes on clustered collection. What makes this worse is some users might define their own _id which can go beyond 12 bytes further exacerbating the size of secondary indexes. So watch out of this during data modeling.This changes Mongo architecture to be similar to MySQL InnoDB, where secondary indexes point to the primary key. But unlike MySQL where tables MUST be clusetered, in Mongo at least get a choice to cluster your collection or not. This is actually pretty good tradeoff.SummaryDatabase systems share the same fundamentals when it comes to their internal storage model. I really like this because it removes fluff and allows me to answer questions related to performance in a predictable manner. Marketing brochures where each database claims to be the best, fastest and scalable no longer has power on an engineer who understands the fundamentals.In this article I discussed the MongoDB internal architecture evolution. In MongoDB the clustered collection is an interesting feature. However, one must use it with caution as the more secondary indexes the larger the size of these indexes get the harder it is to put them in memory for faster traversal. The MongoDB docs on clustered collection.If you prefer to watch videos here is my coverage on my youtube channelhttps://www.youtube.com/watch?v=ONzdr4SmOngReferenceshttps://groups.google.com/g/wiredtiger-users/c/qQPqhjxyU00http://smalldatum.blogspot.com/2021/08/on-storage-engines.html?m=1http://smalldatum.blogspot.com/2015/07/linkbench-for-mysql-mongodb-with-cached.htmlhttps://groups.google.com/g/mongodb-dev/c/8dhOvNx9mBYhttps://www.mongodb.com/docs/upcoming/core/clustered-collections/#clustered-collectionshttps://jira.mongodb.org/browse/SERVER-14569https://www.mongodb.com/docs/v4.4/reference/operator/meta/showDiskLoc/https://github.com/mongodb/mongo/commit/374438c9134e6e31322b05c8ab4c5967d97bf3ebBack to Parent Page" }, { "title": "Part 3: Database Engineering Fundamentals: MongoDB collection clustered index", "url": "/database-engineering-fundamental-part-3/mongodb-collection-clustered-index/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-03 04:00:00 +0530", "snippet": "MongoDB collection clustered indexhttps://www.mongodb.com/docs/v6.2/core/clustered-collections/Clustered CollectionsNew in version 5.3.OverviewStarting in MongoDB 5.3, you can create a collection with a clustered index. Collections created with a clustered index are called clustered collections.BenefitsBecause clustered collections store documents ordered by the clustered index key value, clustered collections have the following benefits compared to non-clustered collections: Faster queries on clustered collections without needing a secondary index, such as queries with range scans and equality comparisons on the clustered index key. Clustered collections have a lower storage size, which improves performance for queries and bulk inserts. Clustered collections can eliminate the need for a secondary TTL (Time To Live) index. A clustered index is also a TTL index if you specify the expireAfterSeconds field. To be used as a TTL index, the _id field must be a supported date type. See TTL Indexes. If you use a clustered index as a TTL index, it improves document delete performance and reduces the clustered collection storage size. Clustered collections have additional performance improvements for inserts, updates, deletes, and queries. All collections have an _id index. A non-clustered collection stores the _id index separately from the documents. This requires two writes for inserts, updates, and deletes, and two reads for queries. A clustered collection stores the index and the documents together in _id value order. This requires one write for inserts, updates, and deletes, and one read for queries. BehaviorClustered collections store documents ordered by the clustered index key value.You can only have one clustered index in a collection because the documents can be stored in only one order. Only collections with a clustered index store the data in sorted order.You can have a clustered index and add secondary indexes to a clustered collection. Clustered indexes differ from secondary indexes: A clustered index can only be created when you create the collection. The clustered index keys are stored with the collection. The collection size returned by the collStats command includes the clustered index size.ImportantBackward-Incompatible FeatureYou must drop clustered collections before you can downgrade to a version of MongoDB earlier than 5.3.LimitationsClustered collection limitations: You cannot transform a non-clustered collection to a clustered collection, or the reverse. Instead, you can: Read documents from one collection and write them to another collection using an aggregation pipeline with an $out stage or a $merge stage. Export collection data with mongodump and import the data into another collection with mongorestore. By default, if a secondary index exists on a clustered collection and the secondary index is usable by your query, the secondary index is selected instead of the clustered index. You must provide a hint to use the clustered index because it is not automatically selected by the query optimizer. The clustered index is not automatically used by the query optimizer if a usable secondary index exists. When a query uses a clustered index, it will perform a bounded collection scan. The clustered index key must be on the _id field. You cannot hide a clustered index. See Hidden indexes. If there are secondary indexes for the clustered collection, the collection has a larger storage size. This is because secondary indexes on a clustered collection with large clustered index keys may have a larger storage size than secondary indexes on a non-clustered collection. Clustered collections may not be capped collections.Set Your Own Clustered Index Key ValuesBy default, the clustered index key values are the unique document object identifiers.You can set your own clustered index key values. Your key: Must contain unique values. Must be immutable. Should contain sequentially increasing values. This is not a requirement but improves insert performance. Should be as small in size as possible. A clustered index supports keys up to 8 MB in size, but a much smaller clustered index key is best. A large clustered index key causes the clustered collection to increase in size and secondary indexes are also larger. This reduces the performance and storage benefits of the clustered collection. Secondary indexes on clustered collections with large clustered index keys may use more space compared to secondary indexes on non-clustered collections. ExamplesThis section shows clustered collection examples.Create ExampleThe following create example adds a clustered collection named products:db.runCommand( {create:\"products\",clusteredIndex: {\"key\": {_id:1 },\"unique\":true,\"name\":\"products clustered key\" }} )In the example, clusteredIndex specifies: “key”: { _id: 1 }, which sets the clustered index key to the _id field. “unique”: true, which indicates the clustered index key value must be unique. “name”: “products clustered key”, which sets the clustered index name.db.createCollection ExampleThe following db.createCollection() example adds a clustered collection named stocks:db.createCollection(\"stocks\", {clusteredIndex: {\"key\": {_id:1 },\"unique\":true,\"name\":\"stocks clustered key\" } })In the example, clusteredIndex specifies: “key”: { _id: 1 }, which sets the clustered index key to the _id field. “unique”: true, which indicates the clustered index key value must be unique. “name”: “stocks clustered key”, which sets the clustered index name.Date Clustered Index Key ExampleThe following create example adds a clustered collection named orders:db.createCollection(\"orders\", {clusteredIndex: {\"key\": {_id:1 },\"unique\":true,\"name\":\"orders clustered key\" } })In the example, clusteredIndex specifies: \"key\": { _id: 1 }, which sets the clustered index key to the _id field. \"unique\": true, which indicates the clustered index key value must be unique. \"name\": \"orders clustered key\", which sets the clustered index name.The following example adds documents to the orders collection:db.orders.insertMany( [ {_id:ISODate(\"2022-03-18T12:45:20Z\" ),\"quantity\":50,\"totalOrderPrice\":500 }, {_id:ISODate(\"2022-03-18T12:47:00Z\" ),\"quantity\":5,\"totalOrderPrice\":50 }, {_id:ISODate(\"2022-03-18T12:50:00Z\" ),\"quantity\":1,\"totalOrderPrice\":10 }] )The _id clusteredIndex key stores the order date.If you use the _id field in a range query, performance is improved. For example, the following query uses _id and $gt to return the orders where the order date is greater than the supplied date:db.orders.find( {_id: {$gt:ISODate(\"2022-03-18T12:47:00.000Z\" ) } } )Example output:[ { _id: ISODate( \"2022-03-18T12:50:00.000Z\" ), quantity: 1, totalOrderPrice: 10 }]Determine if a Collection is ClusteredTo determine if a collection is clustered, use the listCollections command:db.runCommand( {listCollections:1 } )For clustered collections, you will see the clusteredIndex details in the output. For example, the following output shows the details for the orders clustered collection:...name: 'orders',type: 'collection',options: { clusteredIndex: { v: 2, key: { _id: 1 }, name: 'orders clustered key', unique: true }},...v is the index version.Back to Parent Page" }, { "title": "Part 3: Database Engineering Fundamentals: MemCached In-Memory database Architecture", "url": "/database-engineering-fundamental-part-3/memcached-in-memory-database-architecture/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-03 04:00:00 +0530", "snippet": "MemCached In-Memory database ArchitectureMemcached ArchitectureMemcached is an in-memory key-value store originally written in Perl and later rewritten in C. It is popular with companies such as Facebook, Netflix and Wikipedia for its simplicity.While the word “simple” has lost its meaning when it comes to describing software, I think Memcached is one of the few remaining software that is truly simple. Memcached doesn’t try to have fancy features like persistence or rich data types. Even the distributed cache is the responsibility of the client not Memcached server.Memcached backend has one job only, an in-memory key value store.To Cache is a cop-outMemcached is used as a cache for slow database queries or HTTP responses that are expensive to compute. While caching is critical for scalability, it should only be used as a last resort, let me explain.I think running a cache for every encountered slow query is cop-out. I believe understanding the cause of performance degradation is key, otherwise the cache is just a duct-tape. If it is a database query, look at the plan, do you need an index? Are there a lot logic reads can you rewrite the query or include an additional filter predict to minimize the search space?If it is an RPC, consider why are the calls chatty in the first place and can you eliminate the calls at the client side. Often times libraries and framework issue a fleet of queries when mis-used. Another reason black-boxes must be understood.After running out of tuning options a cache might be needed, and that’s where Memcached is best suited for.Memcached ArchitectureIn this article I’d like to do a deep dive into the architecture of Memcached and how the devs fought to keep it simple and feature-stripped. I’ll give my opinion on certain components that I believed should have been an option.I’ll cover the following topics in this article. Memory Management Threading LRU Read/Writes Collisions Distributed Cache DemoWhat is Memcached?Memcached is a key-value store used as a cache. It is designed to be simple and is, therefore, limited in some ways. These limitations can also be looked at as features because they make Memcached transparent.Keys in Memcached are strings, and they are limited to 250 characters. Values can be any type, but they are limited to 1 MB by default. Keys also have an expiration date or time to live (TTL). However, this should not be relied on, as the least recently used (LRU) algorithm may remove expired keys before they are accessed. Memcached is a good choice for caching expensive queries, but it should not be relied on for persistent or durable storage. Always build your application around Memcached not having what you want. Plan for the worse, hope for the best.Memory ManagementWhen allocating items like arrays, strings or integers, they usually go to random places in the process memory. This leaves small gaps of unused memory scattered across the physical memory, a problem referred to as fragmentation.Fragmented memoryFragmentation occurs when the gaps between allocated items continue to increase. This makes it difficult to find a contiguous block of memory that is large enough to hold new items. Technically there might be enough memory to hold the item but the memory is scattered all over the physical space.Does that mean that the item fails to store if no contiguous memory exists? Not really, with the help of virtual memory, the OS gives the illusion that the app is using a contiguous block of memory. Behind the scenes, this block is mapped to tiny small areas in physical memory.When fragmentation occurs, it can cause a program to run more slowly, as the system assembles the memory fragments. The cost of virtual memory mapping and the cost of multiple I/Os to fetch what could have been a single block of memory is relatively high. That is why we try to avoid memory fragmentation.Memcached avoids fragmentation by pre-allocating 1 MB-sized memory pages, which is why values are capped to 1 MB by default.Memcached allocates pagesThe operating system thinks that Memcached is using the allocated memory, but Memcached isn’t storing anything in it yet. As new items are created, Memcached will write items to the allocated page forcing the items to be next to each other. This avoids fragmentation by moving memory management to Memcached instead of the OS.The pages are divided into equal size Chunks. The chunk has a fixed size determined by the slab class. A slab class defines the chunk size, for example Slab class 1 has a chunk size of 72 bytes while slab class 43 has a chunk size of a 1MB.Pages of slab class 1 (72 bytes) stores 14563 chunks per page.Pages of slab class 43 (1MB) stores 1 chunk per pageItems consistent of key, value and some metadata and they are stored in chunks. For example, If the item size is 40 bytes in size, a whole chunk is used to store the item. The closest chunk size to the 40 bytes item is 72 bytes which is slab class 1, leaving 32 bytes unused in chunk. That is why the client should be smart to pick items that fit nicely in chunks leaving as little unused space as possible.Memcached tries to minimize the unused space by putting the item in the most appropriate slab class. Each slab class has multiple pages. Slab class 1, there are 14,563 chunks per page since each chunk is 72 bytes. If an item is less than or equal 72 bytes, it’ll fit nicely in the chunk. But if the item is larger, say 900 kilobytes, it doesn’t fit slab class 1. So, Memcached finds a slab class appropriate for the item. Slab class 43 of chunk size 1MB is the closest one, and the item will be put in that chunk. The entire item fits in a single page. Note that we don’t need to allocate memory for the item because the memory is already pre-allocated.Memcached fits items in the appropriate chunk sizeLet’s take a new example where there is a new item of size 40 bytes, but all allocated pages for this slab class are full, so the item can’t be inserted.Slab class 1 is fullMemcached handles this by allocating a new page and storing the item in a free chunk.A new page is allocated and the new item is placedThreadingMemcached accepts remote clients, it has to have networking. Memcached uses TCP as its native transport they support. UDP is also supported but was disabled by default because of an attack that happened in 2018 called the reflection attack.The Memcached listener thread creates a TCP socket to listen on port 11211. It has one thread that spins up and listens for incoming connections. This thread creates a socket and accepts incoming connections.Memcached then distributes the connections to a pool of threads. When a new connection is established, Memcached allocates a thread from the pool and gives the connection file descriptor to that thread. That worker thread is now responsible of reading data from the connection.If a stream of data or request to get a key is sent to the connection, the thread polls the file descriptor to read the request. Each thread can hosts one or more connections and the number of threads in the pool can be configured.Listener thread accepts connection and distribute them to worker threadsThreading was more critical years ago when asynchronous workload wasn’t as abundant it is in 2022. You see when a thread reads from a connection, this operation used to be blocking in the early 2000s. The thread cannot do anything else until it gets the data. Engineers realized that this isn’t scalable and asynchronous I/O was born. Almost all read calls are now asynchronous, this means the thread can call read on one connection and move on to serve other connection.However, the threading is still important in Memcached because read/write involves some CPU time in hashing and LRU computations. That one thread doing all this work for all connections might not scale.LRU (Least Recently Used)The problem with memory is it’s limited. If you store a lot of keys, even with good expiration dates, memory eventually fills up. What would you do when memory is full?Well, you have two options as an architect. Block new inserts and return an error to the client to free some items Release old unused itemsMemcached did the latter. This is where I wished they made this an option. It is as if I’m with the designers in the room arguing what to do. Having an LRU complicated Memcached and stripped it from its pure simplicity. The voice of reason has lost, and instead the client convenience won. Alas, it is what it is.Memcached releases anything in memory that hasn’t been used for a very long time. That’s another reason why Memcached is called transient memory. Even if you set the expiration for an hour, you can’t rely on the key being there before the hour expires. It can be released at any time, which is another limitation (or feature!) of Memcached.Memcached uses a data structure called a linked list LRU (Least recently used) to release items when memory is full. Every item in the Memcached key-value store is in the linked list, and every slab class has its own LRU.If an item is accessed, it is moved from its current position to the head. This process is repeated every time an item is accessed. As a result, items that are not used frequently will be pushed down to the tail of the list and eventually removed if the memory becomes full.LRUHere is a big picture of the LRU cache. I derived this from reading the source code and the Memcached doc. In the diagram, we have pages and chunks. Each chunk is included in an LRU cache with a head and a tail pointer, and every item between the head and tail are linked to each other.While the LRU is useful, it can also be quite costly in terms of performance. The locks that are necessary to maintain LRU can slow down throughput and complicate the application. If LRU was an option that could be disabled, Memcached could have remained simple. This would allow users to allocate a certain amount of memory to Memcached without having to worry about the overhead of managing LRU. The responsibility to free up items becomes the client’s.LRU LockingNo two threads can update the same data structure concurrently. To solve this, the thread that needs to update any data structure in memory must obtain a mutex and other threads wait for the mutex to be freed. This is the basic locking model and it is used in all applications. Memcached is no different with the LRU data structures.The original Memcached design had one global lock — all operations and LRU management were serialized rendering multi-threaded not as effective. As a result, clients could not access two different items at the same time, all reads were serialized.Memcached fixed this by updating the locking model to LRU per slab class. This means clients can access two items from different slab classes without waits. However, they are still serialized when accessing two items from the same slab class because the LRU needs to get updated. This was later improved by minimizing the LRU updates to once every 60 seconds which allowed for multiple items to be accessed. However this wasn’t good enough.In 2018, Memcached completely redesigned the LRU to introduce sub-LRUs per slab class breaking it by temperature. This has significantly reduced locking and improved performance, but still the locking remained for items within the same temperature.Memcached New LRU design (image memcached.org)Now you know why I wished LRU was an option.Reads and WritesReadsLet’s go through a read example in Memcached. To identify where the item lives in memory for a given key, Memcached uses hash tables. A hash table is an associative array. The beauty of an associative array is that it is consecutive, meaning that if you have an array of 1000 elements, accessing elements 7, 12, 24, 33, or 1 is just as fast because you know the index. Once you know the index, you can immediately go to that location in memory.With hash tables, you don’t have an index — you have a key. The trick is to convert the key to an index and then access the element. We take the key and calculate its hash and do a modulus of the hash table size. Let us take an exampleTo read key “test” we hash “test” and then do modulus N where N is the size of the array of the hash table. This will give us a number between 0 and N — 1. This number can be used to index into the hash table and get the value for the key. This all happens in O(1) time.The value of the key takes you to the page on the specific slab class for that item. When we read the value, we update the slab class LRU by pushing the item to the LRU head. This requires a mutex lock on the LRU data structure so multiple threads don’t corrupt the LRU. Note that if two items are trying to be read of the same slab class, then the reads are serialized because the LRU needs to be locked.Reading key “test”When key test is read, and we get item d, the LRU is updated so that d is now in the head of the linked list.d is at the head of LRUWhat happens if we read the key buzz pointing to item c? The LRU is updated so that c is now the head right after d.c is now the head, followed by dWritesIf we need to write a key with a new value of 44 bytes, we first need to calculate the hash and find its index in the hash table. If the index location is empty, a new pointer is created, and a slab class is allocated with a chunk. The item is then placed in memory, with the chunk fitting into the appropriate slab class.Writing to keyCollisionsBecause hashing maps keys to a fixed size, two keys may hash to the same index causing a collision. Let’s say I’m going to write a new key called “Nani”. The hash of “Nani” collides with another existing key.To solve this, Memcached makes each index in the hash table map to a chain of items as opposed to the item directly. We add the key “Nani” to the chain which has now two items. When the key is read, all items in the chain need to be looked up to determine which one matches the desired key, giving us a O(N) at worse case. Here is an exampleWrite and Read CollisionMemcached measures the growth of these chains. If the growth is too excessive, read performance may suffer. To read a key the entire collision chain must be looked up to find actual key. The longer the collision chain, the slower the read.If the performance of the reads starts to decrease, Memcached does a hash resize and shifts everything around to flatten the structure down.Distributed CacheMemcached servers are isolated — servers don’t talk to each other. I absolutely love the simplicity and elegance of this design. If you want to distribute your keys, the clients have to do that, and you can build your own Memcached client to do just that.Having servers communicate complicates the architecture significantly. Just like at zookeeper.Telnet DemoIn this demo, we’re going to spin up a bunch of Memcached Docker instances. You’ll need to have Docker installed and have a Docker account because the Memcached image is locked behind an account. Once you do that, you can download the image, and you can spin up as many Memcached instances as you want. One thing to note is that Memcached does not support authentication, so you’ll have to implement authentication yourself.The first thing we’re going to do is spin up a Docker container that has a Memcached instance. This can be done with the following command. We can add -d to avoid blocking the terminal.docker run --name mem1 -p 11211:11211 -d memcachedRun docker ps to ensure that the image is running.Let’s test it out with telnet. Run telnet husseinmac 11211 , replace husseinmac with your hostname. You should be logged in once this happens and can issue commands like stats.We can also set key-value pairs in this console. For example, set foo 0 3600 2. foo is the name of the key, 0 is the flags, 3600 is the TTL, and 2 is the # of characters. Once you hit enter, it’ll prompt you for the value of the foo key. The value of the key should exactly match the # of characters that was set in the set command. The key-value pair can be read with the get command: get foo. It can be deleted with delete foo.The interesting part of this demo is talking about the architecture of Memcached. You’ll notice that once you have added your foo key, you can type in stats slabs to get the slab statistics. You’ll see that there is one slab class (see STAT 1, the 1 represents the slab class). Additionally, you can see a lot of interesting statistics about chunks, pages, hits, the total amount of memory used, etc…Distributed Memcached with NodeJSTo test the distributed cache, NodeJS provides a smart client that supports a pool of memcached instances. First let us connect to a single memcached instance from NodeJS and write a bunch of keys.Create a folder for your project mkdir nodemem and cd into it. Then, initialize the project with npm init -y. Next, we’ll create an index.js file with the contents: Note: you’ll need to replace husseinmac with your hostname. const MEMCACHED = require(\"memcached\"); const serverPool = new MEMCACHED([\"husseinmac:11211\"]); function run() { [1, 2, 3, 4, 5, 6, 7, 8, 9].forEach(a =&gt; serverPool.set(\"foo\" + a, \"bar\" + a, 3600, err =&gt; console.log(err))) } run();This piece of code uses the Memcached Node.js client and initializes a server pool of Memcached servers. Then, it adds a nine key-value pairs of the format foo1: bar1, foo2: bar2, etc… to the Memcached server pool.Before running the script, we’ll need to install the Memcached Node.js client with npm install memcached. Then run the code withnode index.jsAfter the script runs, you should be able to telnet into the Memcached server and read the key-value pairs with get foo1, get foo2. All keys will be stored in the 11211 server.Now, let us spin up even more memcached instances:docker run --name mem2 -p 11212:11211 -d memcacheddocker run --name mem3 -p 11213:11211 -d memcacheddocker run --name mem4 -p 11214:11211 -d memcachedOnce you start the containers, you’ll need to add the servers to the pool in your Node.js script: const MEMCACHED = require(\"memcached\"); const serverPool = new MEMCACHED([\"husseinmac:11211\", \"husseinmac:11212\", \"husseinmac:11213\", \"husseinmac:11214\"]); function run() { [1, 2, 3, 4, 5, 6, 7, 8, 9].forEach(a =&gt; serverPool.set(\"foo\" + a, \"bar\" + a, 3600, err =&gt; console.log(err))) } run();Once you run this script, you’ll notice that all the key-value pairs will be distributed to all 4 servers. Try it! telnet into the Memcached servers and run get foo1, get foo2, etc… to see which key-value pairs are where. This happens because the Node.js client picks one server in the pool to put the key-value pair based on its own hashing algorithm.For reading, the NodeJS memcached client will do a hash to find which server has the key and issue a command to that server. Note that this hashing is different from the hash performed by memcached. const MEMCACHED = require(\"memcached\"); const serverPool = new MEMCACHED([\"husseinmac:11211\", \"husseinmac:11212\", \"husseinmac:11213\", \"husseinmac:11214\"]); function run() { [1, 2, 3, 4, 5, 6, 7, 8, 9].forEach(a =&gt; serverPool.set(\"foo\" + a, \"bar\" + a, 3600, err =&gt; console.log(err))) } function read() { [1, 2, 3, 4, 5, 6, 7, 8, 9].forEach(a =&gt; serverPool.get(\"foo\" + a, (err, data) =&gt; console.log(data))) } read();If you run this with node index.js, you’ll notice that all the values will show up despite not all Memcached servers having all the answers.SummaryIn this article, we talked about the Memcached architecture. We discussed the memory management and the importance to use slabs and pages to allocate memory to avoid fragmentation. We also talked about the LRU, which, in my opinion, should have been an option the user can disable. Next, we talked about threads which improve performance for high number of connections. We went through some examples of reads and writes with Memcached and how locking impacts them. Finally, we talked about the distributed cache architecture with a demo using Node.Back to Parent Page" }, { "title": "Part 3: Database Engineering Fundamentals", "url": "/database-engineering-fundamental-part-3/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-03 01:00:00 +0530", "snippet": "Concurrency ControlExclusive Lock vs Shared LockExclusive locks and shared locks are mechanisms used in database systems to manage concurrent access to data. Let’s explain these concepts with a graphical example:graph TD A[Database Resource] B[Transaction 1] C[Transaction 2] D[Transaction 3] subgraph \"Exclusive Lock\" A --&gt;|Locks| B C -.-x A D -.-x A end subgraph \"Shared Lock\" E[Database Resource] F[Transaction 4] G[Transaction 5] H[Transaction 6] E --&gt;|Locks| F E --&gt;|Locks| G E --&gt;|Locks| H end style A fill:#f9f,stroke:#333,stroke-width:2px style E fill:#ccf,stroke:#333,stroke-width:2pxExplanation:Exclusive Lock (X-Lock) Only one transaction can hold an exclusive lock on a resource at a time Prevents other transactions from reading or writing to the locked resource Used for write operations to ensure data consistency In the diagram, Transaction 1 has an exclusive lock, blocking Transactions 2 and 3Shared Lock (S-Lock) Multiple transactions can hold shared locks on the same resource simultaneously Allows concurrent read access but prevents write access Used for read operations to improve concurrency In the diagram, Transactions 4, 5, and 6 all have shared locks on the same resourceThis visual representation illustrates how exclusive locks provide strict isolation for write operations, while shared locks allow concurrent read access, enhancing overall system performance in scenarios where multiple transactions need to read the same data simultaneously.Dead LockIn this case, the one who enters in dead lock first will get chance to execute the query and others will be aborted.If the second transaction only had executed the first command and skipped the second command that would put it in a dead lock situation and made commit, then the first transaction will fail with duplicate key violation exception as the second insert is already committed in the table and due to primary key constraint it cannot add.Question:Deadlock when inserting 20In the deadlock video, you are trying to insert 20, which means 20 doesn't exist in the table yet. So when transaction A inserts 20 but doesn't commit it,why is transaction B getting a lock issue when it tries to also insert it? Because both transactions should be isolated right? For B, 20 doesn't exist yet,so the question of lock or no lock should not even arise right?Or this is a scenario where its not 100% isolation and transactions can read each other metadata?Answer:This happens in all isolation levels., both transactions are racing to acquirea key lock on the unique primary key. Since tx1 wrote 20 it acquires a lock on that value, so no other transactions attempt to write a similar value to guarantee uniqueness. Tx2 attempting to write 20 will be blocked by the lock fromtx2.The dead lock happens when we do the followingTx1 and Tx2 beginTx1 writes 20 (succeeds and locks the row value 20)Tx2 writes 30 (succeeds and locks the row with value 30)Tx1 writes 30 (blocks waiting for tx2 to either commit or rollback to release the lock on 30)Tx2 writes 20 (blocks waiting for tx1 to either commit or rollback to release the lock on 20)Dead lock as both transactions are waiting on each otherFollow-up Question:but why would Tx1 and Tx2 wait on each other. For example, Tx1 writes 20, but the value has not been committed to table right? So how can you acquire alock for a row that doesn't exist yet in the table? The row only exists ifcommit has happened isn't it?Follow-up Answer:Fantastic question! The row is not committed yet to disk because we did not commit but it is available in memory (dirty page) so postgres checks its memory for row 20 and perform the locking to prevent multiple entriesTwo-Phase LockingOne use case is a movie ticket booking system where for a Cinema, show and seat multiple booking problem can happen as at microsecond level this is difficult to handle for the database.One solution is Two-phase locking.In this at database level, it is about acquiring exclusive lock at row level for a row id.Using below to acquire lock on a table:select * from seats where id=15 for update;Above command will acquire exclusive write lock. Now if other transactions attempt to acquire any lock it will be blocked until the first transaction rollbacks or commits which basically releases the lock.So the above command for other transactions will return the updated data which was committed by the first transaction.Spring Boot Example with JDBC Template:@Servicepublic class TicketBookingService { @Autowired private JdbcTemplate jdbcTemplate; @Transactional public boolean bookSeat(Long seatId, Long userId) { try { // Phase 1: Acquire lock String lockSql = \"SELECT * FROM seats WHERE id = ? FOR UPDATE\"; jdbcTemplate.queryForObject(lockSql, new Object[]{seatId}, (rs, rowNum) -&gt; { return rs.getLong(\"id\"); }); // Check if seat is available String checkSql = \"SELECT is_booked FROM seats WHERE id = ?\"; boolean isBooked = jdbcTemplate.queryForObject(checkSql, new Object[]{seatId}, Boolean.class); if (!isBooked) { // Phase 2: Perform update String updateSql = \"UPDATE seats SET is_booked = true, user_id = ? WHERE id = ?\"; int updatedRows = jdbcTemplate.update(updateSql, userId, seatId); return updatedRows &gt; 0; } return false; } catch (Exception e) { // Handle exceptions (e.g., deadlock) return false; } }}This example uses Spring’s JdbcTemplate to implement Two-Phase Locking: Phase 1 (Acquiring the lock): The “SELECT … FOR UPDATE” statement acquires an exclusive lock on the seat row. Phase 2 (Performing the update): If the seat is available, the code updates it to mark it as booked.The @Transactional annotation ensures that the entire method runs within a single transaction, maintaining consistency. Note: This is a simplified example. In a production environment, you’d need to handle more edge cases, implement proper error handling, and possibly use more sophisticated locking mechanisms depending on your specific requirements and database capabilities.Spring Data JPA Implementation:import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.Lock;import org.springframework.data.jpa.repository.Query;import org.springframework.data.repository.query.Param;import javax.persistence.LockModeType;import java.util.Optional;public interface SeatRepository extends JpaRepository&lt;Seat, Long&gt; { @Lock(LockModeType.PESSIMISTIC_WRITE) @Query(\"SELECT s FROM Seat s WHERE s.id = :id\") Optional&lt;Seat&gt; findByIdAndLock(@Param(\"id\") Long id);}@Servicepublic class TicketBookingService { @Autowired private SeatRepository seatRepository; @Transactional public boolean bookSeat(Long seatId, Long userId) { try { // Phase 1: Acquire lock Optional&lt;Seat&gt; seatOptional = seatRepository.findByIdAndLock(seatId); if (seatOptional.isPresent()) { Seat seat = seatOptional.get(); // Check if seat is available if (!seat.isBooked()) { // Phase 2: Perform update seat.setBooked(true); seat.setUserId(userId); seatRepository.save(seat); return true; } } return false; } catch (Exception e) { // Handle exceptions (e.g., deadlock) return false; } }}@Entity@Table(name = \"seats\")public class Seat { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @Column(name = \"is_booked\") private boolean isBooked; @Column(name = \"user_id\") private Long userId; // Getters and setters}In this example: We define a custom query method findByIdAndLock in the SeatRepository interface. This method uses the @Lock annotation with LockModeType.PESSIMISTIC_WRITE to acquire an exclusive lock on the seat row. The TicketBookingService uses this repository method to implement the Two-Phase Locking: Phase 1: Acquiring the lock with findByIdAndLock Phase 2: Performing the update if the seat is available This approach leverages Spring Data JPA’s features to implement the locking mechanism, making the code more concise and easier to maintain compared to the JDBC Template version.Two-Phase Locking: Alternative ApproachThe core idea is the same: the database will acquire an exclusive lock. Instead of checking whether the seat is booked, we will execute the update command with the condition that is_booked is false. The update command will internally acquire a row lock (internal lock, not user lock).UPDATE seats SET is_booked=1, u='Pravin' WHERE id=1 AND is_booked=0;If transaction 1 executes the above command, it will fetch the record and see that the row is not locked, so it will acquire the lock as it can see the condition is met.Now if transaction 2 also executes the above command, it will see the condition is met but the row is currently locked as this information is stored in heap at row level. So it will be blocked.Now if transaction 1 commits, that data is saved in the heap/table. Transaction 2 ideally will get unblocked and update the data, but PostgreSQL does this differently - it will refresh the data as this is stored at row level for lock info. So now on refresh it will see the condition is failed and it will not update. Important: This behavior is database-specific and we cannot guarantee similar behavior in different database systems.For the above approach to work, you need to be in read commit mode of isolation.SQL Pagination with Offset Is Very Slowoffset.mp4Using offset puts the database to do a lot of work. It will fetch all the records matching the condition and then apply offset and use limit to retrieve the selected rows.This is fine for small datasets but for large datasets when in a single TCP connection we are requesting multiple pages, our DB is fetching the same rows again from the beginning and doing the same operation multiple times. Such a wastage!Also, if some new records get added to the table, then the offset approach can give you duplicate records in the response.Let’s take an example to understand:EXPLAIN ANALYZE SELECT * FROM employees ORDER BY id DESC OFFSET 1000 LIMIT 10;Limit (cost=31.51..31.82 rows=10 width=10) (actual time=92.830..93.295 rows=10 loops=1) -&gt; Index Scan Backward using employees_pkey on employees (cost=0.42..31085.44 rows=1000001 width=10) (actual time=0.040..76.360 rows=1010 loops=1)Planning Time: 0.297 msExecution Time: 93.534 msYou can see 1010 rows are fetched → 1000 due to offset and 10 due to limit. Let’s increase the offset:EXPLAIN ANALYZE SELECT * FROM employees ORDER BY id DESC OFFSET 100000 LIMIT 10;Limit (cost=3108.92..3109.23 rows=10 width=10) (actual time=2940.183..2940.972 rows=10 loops=1) -&gt; Index Scan Backward using employees_pkey on employees (cost=0.42..31085.44 rows=1000001 width=10) (actual time=0.030..1721.297 rows=100010 loops=1)Planning Time: 0.086 msExecution Time: 2941.167 msOMG! You can see the number of rows retrieved is 100010 which causes the execution time to go around 100x times the previous query just to retrieve 10 rows. Obviously, the database will do the caching but that is not the solution.Better Approach: Using ID for PaginationA better approach would be to use the ID to tell the DB not to reach anything before that. If let’s say the last ID read by the database is 999992 then we can use it to retrieve from that onwards:EXPLAIN ANALYZE SELECT * FROM employees WHERE id &lt; 999992 ORDER BY id DESC LIMIT 10;Limit (cost=0.42..0.76 rows=10 width=10) (actual time=0.054..0.521 rows=10 loops=1) -&gt; Index Scan Backward using employees_pkey on employees (cost=0.42..33585.25 rows=999990 width=10) (actual time=0.023..0.183 rows=10 loops=1) Index Cond: (id &lt; 999992)Planning Time: 0.106 msExecution Time: 0.750 msData is already cached so it will be quite fast, but the above query is fast as we are using the index to decide from where to start the scan and it will eliminate the need to scan entire rows from offset 0.Also, the number of rows retrieved is 10 so it is better for the DB as well since data read by the DB is less.Database Connection PoolingDatabase connection pooling is a technique used to manage and reuse database connections efficiently. In a production system, it offers several benefits: Improved performance: Reusing existing connections reduces the overhead of creating new ones Better resource management: It limits the number of open connections, preventing database overload Reduced latency: Connections are readily available, minimizing wait times for database operations Enhanced scalability: It allows applications to handle more concurrent users with fewer resourcesHere’s an example of how to configure connection pooling in a Spring Boot application using HikariCP (the default connection pool):# application.properties# DataSource configurationspring.datasource.url=jdbc:postgresql://localhost:5432/mydbspring.datasource.username=myuserspring.datasource.password=mypassword# HikariCP settingsspring.datasource.hikari.maximum-pool-size=10spring.datasource.hikari.minimum-idle=5spring.datasource.hikari.idle-timeout=600000spring.datasource.hikari.max-lifetime=1800000In this configuration: maximum-pool-size: Sets the maximum number of connections in the pool minimum-idle: Sets the minimum number of idle connections to maintain in the pool idle-timeout: Specifies how long a connection can remain idle in the pool before being removed max-lifetime: Defines the maximum lifetime of a connection in the poolWith this setup, Spring Boot will automatically use the connection pool for database operations. The benefits in a production system include: Developers: Simplified code as connection management is handled automatically Operations team: Improved application stability and easier resource management End-users: Faster response times and better overall application performance Business: Increased system capacity and potential cost savings on infrastructureBy efficiently managing database connections, connection pooling helps create more robust and scalable applications in production environments.ReplicationMaster/Backup Replication One Master/Leader node that accepts writes/ddls One or more backup/standby nodes that receive those writes from the master Simple to implement no conflictsMulti-Master Replication Multiple Master/Leader node that accepts writes/ddls One or more backup/follower nodes that receive those writes from the masters Need to resolves conflictSynchronous vs Asynchronous Replication Synchronous Replication, A write transaction to the master will be blocked until it is written to the backup/standby nodes First 2, First 1 or Any Asynchronous Rep, A write transaction is considered successful if it written to the master, then asynchronously the writes are applied to backup nodesPros &amp; Cons of ReplicationPros Horizontal Scaling Region based queries - DB per regionCons Eventual Consistency Slow Writes (synchronous) Complex to Implement (multi-master)SQL vs NoSQL Architecture MongoDB internal Architecture MongoDB collection clustered index MemCached In-Memory database Architecture" }, { "title": "Part 2: Database Engineering Fundamentals: Sharding with Postgres", "url": "/database-engineering-fundamental-part-2/sharding-with-postgres/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-02 02:00:00 +0530", "snippet": "Sharding with PostgresSimple Java Implementation for Database Sharding: URL Shortener ExampleStep 1: Table CreationFirst, we’ll create a simple table for our URL shortener on each shard. Here’s the SQL command:CREATE TABLE url_shortener ( id SERIAL PRIMARY KEY, short_code VARCHAR(10) UNIQUE NOT NULL, long_url TEXT NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);Step 2: Setting up Docker for Multiple ShardsWe’ll use Docker to set up three PostgreSQL instances as our shards. Create a docker-compose.yml file with the following content:version: '3'services: shard1: image: postgres:13 environment: POSTGRES_DB: urlshortener POSTGRES_USER: user POSTGRES_PASSWORD: password ports: - \"5432:5432\" shard2: image: postgres:13 environment: POSTGRES_DB: urlshortener POSTGRES_USER: user POSTGRES_PASSWORD: password ports: - \"5433:5432\" shard3: image: postgres:13 environment: POSTGRES_DB: urlshortener POSTGRES_USER: user POSTGRES_PASSWORD: password ports: - \"5434:5432\"Run the following command to start the Docker containers:docker-compose up -dStep 3: Java Implementation with Consistent HashingNow, let’s implement a simple Java application that uses consistent hashing for sharding. We’ll use the java-consistent-hash library for consistent hashing.First, add the following dependencies to your pom.xml:&lt;dependency&gt; &lt;groupId&gt;org.postgresql&lt;/groupId&gt; &lt;artifactId&gt;postgresql&lt;/artifactId&gt; &lt;version&gt;42.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.ssedano&lt;/groupId&gt; &lt;artifactId&gt;java-consistent-hash&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt;Here’s the Java code for our sharded URL shortener:import com.github.ssedano.hash.ConsistentHash;import java.sql.*;import java.util.*;public class ShardedUrlShortener { private final ConsistentHash&lt;String&gt; consistentHash; private final Map&lt;String, String&gt; shardConnections; public ShardedUrlShortener() { List&lt;String&gt; shards = Arrays.asList(\"shard1\", \"shard2\", \"shard3\"); consistentHash = new ConsistentHash&lt;&gt;(shards, 10); shardConnections = new HashMap&lt;&gt;(); shardConnections.put(\"shard1\", \"jdbc:postgresql://localhost:5432/urlshortener\"); shardConnections.put(\"shard2\", \"jdbc:postgresql://localhost:5433/urlshortener\"); shardConnections.put(\"shard3\", \"jdbc:postgresql://localhost:5434/urlshortener\"); } public void insertUrl(String shortCode, String longUrl) throws SQLException { String shard = consistentHash.get(shortCode); String connectionUrl = shardConnections.get(shard); try (Connection conn = DriverManager.getConnection(connectionUrl, \"user\", \"password\"); PreparedStatement pstmt = conn.prepareStatement(\"INSERT INTO url_shortener (short_code, long_url) VALUES (?, ?)\")) { pstmt.setString(1, shortCode); pstmt.setString(2, longUrl); pstmt.executeUpdate(); } } public String getLongUrl(String shortCode) throws SQLException { String shard = consistentHash.get(shortCode); String connectionUrl = shardConnections.get(shard); try (Connection conn = DriverManager.getConnection(connectionUrl, \"user\", \"password\"); PreparedStatement pstmt = conn.prepareStatement(\"SELECT long_url FROM url_shortener WHERE short_code = ?\")) { pstmt.setString(1, shortCode); try (ResultSet rs = pstmt.executeQuery()) { if (rs.next()) { return rs.getString(\"long_url\"); } } } return null; } public static void main(String[] args) { ShardedUrlShortener shortener = new ShardedUrlShortener(); try { // Insert a URL shortener.insertUrl(\"abc123\", \"https://www.example.com\"); System.out.println(\"URL inserted successfully\"); // Retrieve the long URL String longUrl = shortener.getLongUrl(\"abc123\"); System.out.println(\"Retrieved long URL: \" + longUrl); } catch (SQLException e) { e.printStackTrace(); } }}Explanation of the Implementation We create a ConsistentHash object with our three shards and a replication factor of 10. The shardConnections map stores the JDBC connection URLs for each shard. The insertUrl method uses consistent hashing to determine which shard to use based on the short code, then inserts the URL into the appropriate shard. The getLongUrl method similarly uses consistent hashing to determine which shard to query for a given short code. In the main method, we demonstrate inserting a URL and then retrieving it.This implementation provides a simple example of database sharding using consistent hashing. In a production environment, you’d want to add more error handling, connection pooling, and possibly a caching layer for improved performance.Back to Parent Page" }, { "title": "Part 2: Database Engineering Fundamentals", "url": "/database-engineering-fundamental-part-2/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-02 01:00:00 +0530", "snippet": "Table of ContentsB-Tree vs B+ Tree in Production Database Systems Full Table Scans B-Tree B-Tree limitations [B+Tree] [B+Tree Considerations] B+Tree storage cost in MySQL vs PostgresDatabase Partitioning What is Partitioning? Horizontal Partitioning vs Vertical Partitioning Partitioning Types Partitioning vs Sharding Demo Pros &amp; ConsDatabase Sharding What is sharding? Consistent Hashing Horizontal Partitioning vs Sharding Pros &amp; Cons When Should You Consider Sharding Your Database?B-Tree vs B+ Tree in Production Database SystemsAnd their impact on production database systemsTopics Overview Full Table Scans B-Tree B-Tree limitations B+Tree B+Tree Considerations B+Tree storage cost in MySQL vs Postgres SummaryFull Table Scan To find a row in a large table we perform full table scan Reading the entire table is slow (many IO to fetch pages) We need a way to reduce the search spaceB-Tree Balanced data structure for fast traversal B-Tree has nodes In B-Tree of “m” degree each node can have (m) child nodes Node has up to (m-1) elements Each element has key and value The value is usually data pointer to the row Data pointer can point to primary key or tuple Root Node, internal node and leaf nodes A node = disk pageLimitation of B-Tree Elements in all nodes store both key and value Internal nodes take more space thus require more IO and can slow down traversal Range queries are slow because of random access (give me all values 1-5) B+Tree solves both these problems Hard to fit internal nodes in memoryB+Tree Exactly like B-Tree but only stores keys in internal nodes Values are only stored in leaf nodes Internal nodes are smaller since they only store keys and they can fit more elements Leaf nodes are “linked” so once you find a key you can find all values before and after that key Great for range queriesB+Tree &amp; DBMS Considerations Cost of leaf pointer (cheap) 1 Node fits a DBMS page (most DBMS) Can fit internal nodes easily in memory for fast traversal Leaf nodes can live in data files in the heap Most DBMS systems use B+Tree MongoDB (WiredTiger) uses B-TreeStorage Cost in Postgres vs MySQL B+Trees secondary index values can either point directly to the tuple (Postgres) or to the primary key (MySQL) If the Primary key data type is expensive this can cause bloat in all secondary indexes for databases such as MySQL (InnoDB) Leaf nodes in MySQL (InnoDB) contains the full row since it’s an IOT / clustered indexReference: b-tree-original-paper.pdfDatabase PartitioningTopics Overview What is Partitioning? Horizontal Partitioning vs Vertical Partitioning Partitioning Types Partitioning vs Sharding Demo Pros &amp; ConsUsing partitioning, we can split large tables into smaller manageable tables that make lookups quite efficient.Vertical vs Horizontal Partitioning Horizontal Partitioning splits rows into partitions Range or list Vertical partitioning splits columns into partitions Large column (blob) that you can store in a slow access drive in its own tablespace Partitioning Types By Range Dates, ids (e.g., by logdate or customerid from-to) By List Discrete values (e.g., states CA, AL, etc.) or zip codes By Hash Hash functions (consistent hashing) Horizontal Partitioning vs Sharding HP splits big table into multiple tables in the same database, client is agnostic Sharding splits big table into multiple tables across multiple database servers HP table name changes (or schema) Sharding everything is the same but server changesImplementing Partitioning in PostgreSQLTo create a partition, follow these steps:-- Create the parent table with partitioning schemeCREATE TABLE grades_parts(id serial NOT NULL, g int NOT NULL) PARTITION BY RANGE(g);-- Create individual partition tablesCREATE TABLE grade0035 (LIKE grades_parts INCLUDING INDEXES);CREATE TABLE grade3560 (LIKE grades_parts INCLUDING INDEXES);CREATE TABLE grade6080 (LIKE grades_parts INCLUDING INDEXES);CREATE TABLE grade80100 (LIKE grades_parts INCLUDING INDEXES);Once we have created the main table and partition tables, we attach the partition tables to the main table:-- Attach partitions with their respective rangesALTER TABLE grades_parts ATTACH PARTITION grade0035 FOR VALUES FROM (0) TO (35);ALTER TABLE grades_parts ATTACH PARTITION grade3560 FOR VALUES FROM (35) TO (60);ALTER TABLE grades_parts ATTACH PARTITION grade6080 FOR VALUES FROM (60) TO (80);ALTER TABLE grades_parts ATTACH PARTITION grade80100 FOR VALUES FROM (80) TO (100);Now our partition is attached and ready to populate data.When we insert data into the main table (grades_parts), the database will decide which partition that record should go to based on the g values.If you want to have an index on each partition, which is a common use case, you can do so by creating an index on the parent table:CREATE INDEX grades_parts_idx ON grades_parts(g);This is powerful and simplified - separate indexes will be created by the database for each partition table.Some queries may check other partitions even though data isn’t present there:SELECT COUNT(*) FROM grades_parts WHERE g = 30;Even though records should only be in the first partition (grade0035), the database might check other partitions too.To optimize this behavior, you can enable partition pruning:SHOW enable_partition_pruning;SET enable_partition_pruning = on;This will make lookup happen in the first partition only and avoid looking at other partitions. It’s enabled by default in PostgreSQL.Pros of Partitioning Improves query performance when accessing a single partition Sequential scan vs scattered index scan Easy bulk loading (attach partition) Archive old data that are barely accessed into cheap storageCons of Partitioning Updates that move rows from a partition to another (slow or fail sometimes) Inefficient queries could accidentally scan all partitions resulting in slower performance Schema changes can be challenging (DBMS could manage it though)Database ShardingTopics Overview What is sharding? Consistent Hashing Horizontal Partitioning vs Sharding Pros &amp; ConsWhat is Sharding?Sharding is a database architecture pattern related to horizontal partitioning, which is the practice of separating one table’s rows into multiple different tables, known as partitions or shards. Each partition has the same schema and columns, but entirely different rows.In sharding, these smaller partitions are distributed across separate database nodes, often on different physical servers. This approach allows for better scalability and performance in large-scale applications. Sharding distributes data across multiple machines Each shard is an independent database, and collectively, the shards make up a single logical database Sharding is typically used to improve the performance and scalability of very large databases It allows for horizontal scaling, which can be more cost-effective than vertical scaling (upgrading to more powerful hardware)Sharding is particularly useful for applications that deal with big data or high traffic, as it helps distribute the load and improve query response times.Consistent HashingConsistent hashing is a technique used in database sharding to distribute data across multiple nodes efficiently. Here’s how it works: The hash space is represented as a fixed circular ring (usually 0 to 2^32 - 1) Both data items and nodes (shards) are mapped to this ring using a hash function To find which shard a data item belongs to, move clockwise on the ring from the item’s hash position until a node is found When adding or removing nodes, only a fraction of the data needs to be redistributedThis approach provides better distribution and minimizes data movement when the number of shards changes, making it ideal for dynamic environments.Benefits of consistent hashing in database sharding: Scalability: Easily add or remove shards without major data redistribution Load balancing: Evenly distributes data across shards Fault tolerance: If a shard fails, its data is redistributed among remaining shards Reduced hotspots: Helps prevent overloading of specific shardsMany distributed databases and caching systems, like Cassandra and Redis, use consistent hashing for efficient data distribution across shards.Horizontal Partitioning vs Sharding HP splits big table into multiple tables in the same database Sharding splits big table into multiple tables across multiple database servers HP table name changes (or schema) Sharding everything is the same but server changesSharding with PostgresSharding with PostgresPros of Sharding Scalability Data Memory Security (users can access certain shards) Optimal and smaller index sizeCons of Sharding Complex client (aware of the shard) Transactions across shards problem Rollbacks Schema changes are hard Joins Has to be something you know in the queryWhen Should You Consider Sharding Your Database?It should be the last option in your scaling strategy.You should consider sharding only if these options aren’t working for you: Apply indexing to tables If indexing is not working, split tables into multiple partitions, each partition maintains small tables and has their own indexes Have master-slave setup Replicate the setup to region-specific instances like Asia, America, or EU usersSharding is one of the most complicated database scaling strategies.In sharding, ACID transactions are difficult as they are distributed across different servers, becoming a distributed transaction use case.You typically sacrifice full rollback and commit capabilities.The application must be aware of the shards.There are additional server maintenance costs." }, { "title": "Part 1: Database Engineering Fundamentals: The Cost of Long running Transactions", "url": "/database-engineering-fundamental-part-1/the-cost-of-long-running-transactions/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-01 04:00:00 +0530", "snippet": "Article - The Cost of Long running TransactionsThe cost of a long-running update transaction that eventually failed in Postgres (or any other database for that matter.In Postgres, any DML transaction touching a row creates a new version of that row. if the row is referenced in indexes, those need to be updated with the new tuple id as well. There are exceptions with optimization such as heap only tuples (HOT) where the index doesn’t need to be updated but that only happens if the page where the row lives have enough space (fill factor &lt; 100%)If a long transaction that has updated millions of rows rolls back, then the new row versions created by this transaction (millions in my case) are now invalid and should NOT be read by any new transaction. You have many ways to address this, do you clean all dead rows eagerly on transaction rollback? Or do you do it lazily as a post-process? Or do you lock the table and clean those up until the database fully restarts?Postgres does the lazy approach, a command called vacuum which is called periodically. Postgres attempts to remove dead rows and free up space on the page.What’s the harm of leaving those dead rows in? It’s not really correctness issues at all, in fact, transactions know not to read those dead rows by checking the state of the transaction that created them. This is however an expensive check, the check to see if the transaction that created this row is committed or rolled back. Also, the fact that those dead rows live in disk pages with alive rows makes an IO not efficient as the database has to filter out dead rows. For example, a page may have contained 1000 rows, but only 1 live row and 999 dead rows, the database will make that IO but only will get a single row of it. Repeat that and you end up making more IOs. More IOs = slower performance.Other databases do the eager approach and won’t let you even start the database before rolling back is successfully complete, using undo logs. Which one is right and which one is wrong? Here is the fun part! Nothing is wrong or right, it’s all decisions that we engineers make. It’s all fundamentals. It’s up to you to understand and pick. Anything can work. You can make anything work if you know what you are dealing with.Back to Parent Page" }, { "title": "Part 1: Database Engineering Fundamentals: Microsoft SQL Server Clustered Index Design", "url": "/database-engineering-fundamental-part-1/microsoft-sql-server-clustered-index-des/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-01 03:00:00 +0530", "snippet": "Article - Microsoft SQL Server Clustered Index DesignThis is one of the most interesting documentation pages I have read and I really thought I would share it with you. You can read the entire design guide with this link and I also included the pdf of the design.  I took a snippet of the two most important parts that I found interesting from this design guide, pay close attention to the images and how clustered index is persisted compared to non-clustered index in the b-tree architecture. Keep in mind that this is Microsoft way of solving things and it’s not necessarily the way to go. Try to challenge how is this done and come up with a different approach.Enjoy HusseinClustered Index ArchitectureIn SQL Server, indexes are organized as B-Trees. Each page in an index B-tree is called an index node. The top node of the B-tree is called the root node. The bottom nodes in the index are called the leaf nodes. Any index levels between the root and the leaf nodes are collectively known as intermediate levels. In a clustered index, the leaf nodes contain the data pages of the underlying table. The root and intermediate level nodes contain index pages holding index rows. Each index row contains a key value and a pointer to either an intermediate level page in the B-tree, or a data row in the leaf level of the index. The pages in each level of the index are linked in a doubly-linked list.Clustered indexes have one row in sys.partitions, with index_id = 1 for each partition used by the index. By default, a clustered index has a single partition. When a clustered index has multiple partitions, each partition has a B-tree structure that contains the data for that specific partition. For example, if a clustered index has four partitions, there are four B-tree structures; one in each partition.Depending on the data types in the clustered index, each clustered index structure will have one or more allocation units in which to store and manage the data for a specific partition. At a minimum, each clustered index will have one IN_ROW_DATA allocation unit per partition. The clustered index will also have one LOB_DATA allocation unit per partition if it contains large object (LOB) columns. It will also have one ROW_OVERFLOW_DATA allocation unit per partition if it contains variable length columns that exceed the 8,060 byte row size limit.The pages in the data chain and the rows in them are ordered on the value of the clustered index key. All inserts are made at the point where the key value in the inserted row fits in the ordering sequence among existing rows.This illustration shows the structure of a clustered index in a single partition.Nonclustered Index ArchitectureNonclustered indexes have the same B-tree structure as clustered indexes, except for the following significant differences: The data rows of the underlying table are not sorted and stored in order based on their nonclustered keys. The leaf level of a nonclustered index is made up of index pages instead of data pages.The row locators in nonclustered index rows are either a pointer to a row or are a clustered index key for a row, as described in the following: If the table is a heap, which means it does not have a clustered index, the row locator is a pointer to the row. The pointer is built from the file identifier (ID), page number, and number of the row on the page. The whole pointer is known as a Row ID (RID). If the table has a clustered index, or the index is on an indexed view, the row locator is the clustered index key for the row.Nonclustered indexes have one row in sys.partitions with index_id &gt; 1 for each partition used by the index. By default, a nonclustered index has a single partition. When a nonclustered index has multiple partitions, each partition has a B-tree structure that contains the index rows for that specific partition. For example, if a nonclustered index has four partitions, there are four B-tree structures, with one in each partition.Depending on the data types in the nonclustered index, each nonclustered index structure will have one or more allocation units in which to store and manage the data for a specific partition. At a minimum, each nonclustered index will have one IN_ROW_DATA allocation unit per partition that stores the index B-tree pages. The nonclustered index will also have one LOB_DATA allocation unit per partition if it contains large object (LOB) columns. Additionally, it will have one ROW_OVERFLOW_DATA allocation unit per partition if it contains variable length columns that exceed the 8,060 byte row size limit.The following illustration shows the structure of a nonclustered index in a single partition.Resources for this lectureSQL+Server+Index+Architecture+and+Design+Guide+-+SQL+Server+_+Microsoft+Docs.pdfBack to Parent Page" }, { "title": "Part 1: Database Engineering Fundamentals: Database Page", "url": "/database-engineering-fundamental-part-1/database-page/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-01 02:00:00 +0530", "snippet": "Databases Pages (Article)Database Pages — A deep diveDatabases often use fixed-size pages to store data. Tables, collections, rows, columns, indexes, sequences, documents and more eventually end up as bytes in a page. This way the storage engine can be separated from the database frontend responsible for data format and API. Moreover, this makes it easier to read, write or cache data when everything is a page.Here is an example of SQL Server page layout.SQL Server page layout (link).In this article I explore the idea of a database page, how it is read and written to disk, how they are stored on disk, and finally I go through an example of page layout in Postgres.A Pool of PagesDatabases read and write in pages. When you read a row from a table, the database finds the page where the row lives and identifies the file and offset where the page is located on disk. The database then asks the OS to read from the file on the particular offset for the length of the page. The OS checks its filesystem cache and if the required data isn’t there, the OS issues the read and pulls the page in memory for the database to consume.The database allocates a pool of memory, often called shared or buffer pool. Pages read from disk are placed in the buffer pool. Once a page is in the buffer pool, not only we get access to the requested row but also other rows in the page too depending on how wide the rows are. This makes reads efficient especially those resulting from index range scans. The smaller the rows, the more rows fit in a single page, the more bang for our buck a single I/O gives us.The same goes for writes, when a user updates a row, the database finds the page where the row lives, pull the page in the buffer pool and update the row in memory and make a journal entry of the change (often called WAL) persisted to disk. The page can remain in memory so it may receive more writes before it is finally flushed back to disk, minimizing the number of I/Os. Deletes and inserts work the same but implementation may vary.Page ContentWhat you store in pages is up to you. Row-store databases write rows and all their attributes one after the other packed in the page so that OLTP workloads are better especially write workload.Column-store databases write the rows in pages column by column such OLAP workloads that run a summary fewer fields are more efficient. A single page read will be packed with values from one column, making aggregate functions like SUM much more effective. I made a video comparing row vs column based storage engines,Document based databases compress documents and store them in page just like row stores and graph based databases persist the connectivity in pages such that page read is efficient for traversing graphs, this also can be tuned for depth vs breadth vs search.Whether you are storing rows, columns, documents or graphs, the goal is to pack your items in the page such that a page read is effective. The page should give you as much useful information as possible to help with client side workload. If you find yourself reading many pages to do tiny little work consider rethinking your data modeling. This is a whole different article, data modeling, underrated.Small vs Large PagesSmall pages are faster to read and write especially if the page size is closer to the media block size, however the overhead cost of the page header metadata compare to useful data can get high. On the other hand, larger sizes can minimize metadata overhead and page splits but at a cost of higher cold read and write.Of course this gets very complicated the closer you get to the disk/SSD. Great minds in storage industry are working on technologies like Zoned and key value store namespaces in NVMe to optimize read/writes between host and media. I’m not even going to attempt to explain it here because frankly speaking I’m still dipping my toes in these waters.Postgres Default page size is 8KB, MySQL InnoDB is 16KB, MongoDB WiredTiger is 32KB, SQL Server is 8KB and Oracle is also 8KB. Database defaults work for most cases but it is important to know these default and be prepared to configure it for your use case.How page are stored on DiskThere are many ways we can store and retrieve pages to and from disk. One way is to make a file per table or collection as an array of fixed-size pages. Page 0 followed by page 1 followed by page 2. To read something from disk we need to information, the file name, offset and the length, with this design we have all three!To read page x, we know the file name from the table, to get the offset it is X *Page_Size, and the length in bytes are the page size.Example reading table test, assume a page size of 8KB, to read pages 2 through 9, we read the file where table test live, with an offset 16484 (28192) for 8 pages, 65536 bytes (88192).But that is just one way, the beauty of databases is every database implementation is differnet.Postgres Page LayoutAmong the database, I would like to explore how PostgreSQL store pages and provide my critisim of certain choices made. In Postgres the default page size is 8KB, and here is how it looks like.Postgres page layout (link)Let me try to go through each section:Page header — 24 bytesThe page must have metadata to describe what is in the page including the free space available. This is a 24 bytes fixed header.ItemIds — 4 byte eachThis is an array of item pointers (not the items or tuples themselves. Each itemId is a 4 byte offset:length pointer which points to the offset in the page of where the item is and how large is it.It is the fact that this pointer exist allows the HOT optimization (Heap only tuple), when an update happens to a row in postgres, a new tuple is generated, if the tuple happened to fit in the same page as the old tuple, the HOT optiimization changes the old item id pointer to point to the new tuple. This way indexes and other data structures can still point to the old tuple id. Very powerful.Although one criticism is the size the item pointers take, at 4 bytes each, if I can store 1000 items, half the page (4KB) is wasted on headers. I use items, tuples and rows but there a difference, the row is what the user see, the tuple is the physical instance of the row in the page, the item is the tuple. The same row can have 10 tuples, one active tuple and 7 left for older transactions (MVCC reasons) to read and 2 dead tuples that no one needs any more.Items — variable lengthThis is where the items themselves live in the page one after the other.Special — variable lengthThis section is only applicable to B+Tree index leaf pages where each page links to the previous and forward. Information about page pointers are stored here.Here is an example of how tuples are referenced.SummaryData in databases end in pages, whether this is index, sequence, or a table rows. This makes it easier for the database to work with pages regardless what is in the page itself. The page it self has a header and data and is stored on disk in as part of a file. Each database has a different implementation of how the page looks like and how it is physically stored on disk but at the end, the concept is the same.Back to Parent Page" }, { "title": "Part 1: Database Engineering Fundamentals", "url": "/database-engineering-fundamental-part-1/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, database-engineering", "date": "2024-07-01 01:00:00 +0530", "snippet": "Table of Contents ACID Properties Understanding Database Internals Row-Based vs Column-Based Databases Primary Key vs Secondary Key Database Indexing SQL Query Planner and Optimizer Scan Types Key vs Non-key Column Database Indexing Index Scan vs Index Only Scan Combining Database Indexes Database Optimizer Decisions Creating Indexes Concurrently Bloom Filters Working with Billion Row Tables ArticlesACID PropertiesWhat is a transaction? A collection of queries One unit of work Example: Select, Update, Update Lifespan: Transaction BEGIN Transaction COMMIT Transaction ROLLBACK Transaction unexpected ending → ROLLBACK Different databases try to optimize operations like rollback or commit. In Postgres, commit I/O is more frequent and commit is faster.Nature of transactions Usually transactions are used to change and modify data However, it is perfectly normal to have a read-only transaction Example: You want to generate a report and you want to get a consistent snapshot based at the time of transaction We will learn more about this in the Isolation sectionAtomicity All queries in a transaction must succeed If one query fails, all prior successful queries in the transaction should rollback If the database went down prior to a commit of a transaction, all the successful queries in the transaction should rollback After we restart the machine, the first account has been debited but the other account has not been credited This is really bad as we just lost data, and the information is inconsistent An atomic transaction is a transaction that will rollback all queries if one or more queries failed The database should clean this up after restartSome databases don’t allow operations unless rollback is completed. For long transactions, it can take longer time (like 1 hour) to rollback.Isolation Can my inflight transaction see changes made by other transactions? Read phenomena Isolation LevelsIsolation - Read phenomena Dirty reads: Reading uncommitted data Non-repeatable reads: In a transaction, execution of the same read query results in different values at different times. It is expensive to undo. The value read is committed value but that happened during transaction Phantom reads: You are reading data which did not exist at the beginning of the transaction but got committed by another transaction in between. This data is read by a subsequent query Lost updates: Two transactions reading the same data and updating it in parallel. The update done by the first transaction gets modified by the second transaction. This makes the result inconsistentIsolation - Levels Read uncommitted: No isolation, any change from the outside is visible to the transaction, committed or not Read committed: Each query in a transaction only sees committed changes by other transactions Repeatable Read: The transaction will make sure that when a query reads a row, that row will remain unchanged while it’s running. (In Postgres, it is called snapshot isolation, it is versioned) Snapshot: Each query in a transaction only sees changes that have been committed up to the start of the transaction. It’s like a snapshot version of the database at that moment Serializable: Transactions are run as if they were serialized one after the other Each DBMS implements isolation levels differently Pessimistic: Row level locks, table locks, page locks to avoid lost updates Optimistic: No locks, just track if things changed and fail the transaction if so Repeatable read “locks” the rows it reads but it could be expensive if you read a lot of rows; Postgres implements RR as snapshot. That is why you don’t get phantom reads with Postgres in repeatable read Serializable are usually implemented with optimistic concurrency control, you can implement it pessimistically with SELECT FOR UPDATEUnderstanding Database InternalsHow tables and indexes are stored on diskStorage concepts: Table Row_id Page IO Heap data structure Index data structure b-tree Example of a queryLogical TableRow_IDPageIO IO operation (input/output) is a read request to the disk We try to minimize this as much as possible An IO can fetch 1 page or more depending on the disk partitions and other factors An IO cannot read a single row; it’s a page with many rows in them, you get them for free You want to minimize the number of IOs as they are expensive Some IOs in operating systems go to the operating system cache and not diskHeap The Heap is a data structure where the table is stored with all its pages one after another This is where the actual data is stored including everything Traversing the heap is expensive as we need to read so much data to find what we want That is why we need indexes that help tell us exactly what part of the heap we need to read, which page(s) of the heap we need to pullIndex An index is another data structure separate from the heap that has “pointers” to the heap It has part of the data and is used to quickly search for something You can index on one column or more Once you find a value of the index, you go to the heap to fetch more information where everything is stored Index tells you EXACTLY which page to fetch in the heap instead of taking the hit to scan every page in the heap The index is also stored as pages and costs IO to pull the entries of the index The smaller the index, the more it can fit in memory, and the faster the search Popular data structure for index is b-trees, learn more on that in the b-tree sectionNotes: Sometimes the heap table can be organized around a single index. This is called a clustered index or an Index Organized Table Primary key is usually a clustered index unless otherwise specified MySQL InnoDB always has a primary key (clustered index); other indexes point to the primary key “value” Postgres only has secondary indexes and all indexes point directly to the row_id which lives in the heapRow-Based vs Column-Based DatabasesIn row-based databases, data is organized in a continuous sequence of bytes which contains all columns of the row together. When a record is fetched, the whole page is returned which contains a number of rows including the one that the query is made for. Tables are stored as rows in disk A single block IO read to the table fetches multiple rows with all their columns More IOs are required to find a particular row in a table scan but once you find the row, you get all columns for that rowFor column-based databases, columns are stored separately where column 1 of all records will be together and the same for all other columns. Tables are stored as columns first in disk A single block IO read to the table fetches multiple columns with all matching rows Less IOs are required to get more values of a given column. But working with multiple columns requires more IOs Typically used for OLAP workloadsPrimary Key vs Secondary KeyClustering - organizing a table around a key → Primary key (Oracle and MySQL offer this)A cluster is nothing but a heap → it is designed for speed for operations like range searches in one IO.Secondary key → having an additional index in the form of a btree. It has a separate structure from the order in which data is stored. It has row_id as a reference, which is used to locate the actual data.All indexes in Postgres are secondary indexes.Database IndexingTo check query execution strategy: explain analyze select id from employees e where e.id = 20000;To create an index: create index &lt;index_name&gt; on table(column_name);create table employees( id serial primary key, name text);create or replace function random_string(length integer) returns text as $$declare chars text[] := '{0,1,2,3,4,5,6,7,8,9,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z}'; result text := ''; i integer := 0; length2 integer := (select trunc(random() * length + 1));begin if length2 &lt; 0 then raise exception 'Given length cannot be less than 0'; end if; for i in 1..length2 loop result := result || chars[1+random()*(array_length(chars, 1)-1)]; end loop; return result;end;$$ language plpgsql;insert into employees(name)(select random_string(10) from generate_series(0, 1000000));create index employees_name on employees(name);Query: explain analyze select id,name from employees e where e.name like ‘%Pra%’;Output:Scan type:---- inline index scan → when details present in index, it will not check table/heapIndex Only Scan using employees_pkey on employees e (cost=0.42..8.44 rows=1 width=4) (actual time=0.051..0.082 rows=1 loops=1)Index Cond: (id = 20000)Heap Fetches: 1Planning Time: 0.241 msExecution Time: 0.178 ms- parallel sequencial scan → when using non index column to searchGather (cost=1000.00..11310.94 rows=6 width=10) (actual time=57.742..64.152 rows=0 loops=1)Workers Planned: 2Workers Launched: 2-&gt; Parallel Seq Scan on employees e (cost=0.00..10310.34 rows=2 width=10) (actual time=30.379..30.398 rows=0 loops=3)Filter: (name = 'Pravin'::text)Rows Removed by Filter: 333334Planning Time: 0.159 msExecution Time: 64.236 ms- Bitmap index scan → when using index column to searchcreate index using → **create** **index** employees_name **on** employees(**name**); → this takes time as it build the b tree before using it. if we run same query like above, it will return in lesser time.Bitmap Heap Scan on employees e (cost=4.47..27.93 rows=6 width=10) (actual time=0.113..0.160 rows=0 loops=1)Recheck Cond: (name = 'Pravin'::text)-&gt; Bitmap Index Scan on employees_name (cost=0.00..4.47 rows=6 width=0) (actual time=0.052..0.068 rows=0 loops=1)Index Cond: (name = 'Pravin'::text)Planning Time: 0.416 msExecution Time: 0.261 msif we use query that has like usage, it make the DB to check all the rows.Gather (cost=1000.00..11319.34 rows=90 width=10) (actual time=6.080..66.693 rows=19 loops=1)Workers Planned: 2Workers Launched: 2-&gt; Parallel Seq Scan on employees e (cost=0.00..10310.34 rows=38 width=10) (actual time=15.069..42.456 rows=6 loops=3)Filter: (name ~~ '%Pra%'::text)Rows Removed by Filter: 333327Planning Time: 3.247 msExecution Time: 67.227 msUsing explain analyze, we can come to know why query is slower possibly it is using parallel seq scan despite presence of index. so like query is bad query for index.SQL Query Planner and Optimizercreate table grades ( id serial primary key, g int, name text ); create index grades_score on grades(g);insert into grades (g, name) select random()*100, substring(md5(random()::text ),0,floor(random()*31)::int) from generate_series(0, 500);vacuum (analyze, verbose, full);explain analyze select id,g from grades where g &gt; 80 and g &lt; 95 order by g;explain this keyword is used to check what planning the database will use.explain select * from grades;The output is:Seq Scan on grades (cost=0.00..75859.70 rows=4218170 width=23) cost=0.00..75859.7 → here the first part means, the database took no time to find the first record. As we scan the next record, this time will increase and that is what the database is showing in the second part where it says that the estimate is around 75859.7 ms. rows=4218170 → this is just a guess number as the database has not executed the query yet. It is the fastest approach and can be used to replace count(*). In most usecase, you want to show a close-to-exact number which the above query does very efficiently. width=23, it is in bytes and represents result size.Let’s spice it up a bit:explain select * from grades order by g;It returns:Index Scan using grades_score on grades (cost=0.43..226671.63 rows=5000502 width=23)Here, the database will use an index scan. It says it will take a cost of .43 ms as something more is going on here. The rows scanned it shows higher.If you try to sort based on a non-index column:explain select * from grades order by name;Here is the output:Gather Merge (cost=359650.50..845844.25 rows=4167084 width=23) Workers Planned: 2 -&gt; Sort (cost=358650.48..363859.33 rows=2083542 width=23) Sort Key: name -&gt; Parallel Seq Scan on grades (cost=0.00..54513.43 rows=2083542 width=23)Scan TypesSequential Scan vs Index Scan vs Bitmap Index ScanAll 3 are different scan types that the database uses to retrieve records.Index scan → If the database finds that the number of records is few, it uses Index scan. In index scan, it uses both the index and heap to retrieve the record.explain select name from grades where g &gt; 100;Index Scan using grades_score on grades (cost=0.43..8.45 rows=1 width=15) Index Cond: (g &gt; 100)You can see the number of rows fetched is small.Sequential scan → If the database finds that the number of records is large, it will use Sequential scan instead of Index scan. It simply uses Sequential scan and avoids referring to the index to scan the record as there is too much overhead and it slows the performance. The database uses a smart approach here.explain select name from grades where g &lt; 100;Seq Scan on grades (cost=0.00..96184.27 rows=4977500 width=15) Filter: (g &lt; 100)You can see the number of rows fetched is very high, so it uses Sequential scan.In Bitmap scan there are 3 things happening: Bitmap index scan → It first scans the index for grades that are more than 95, it will not visit the heap yet. It will mark the pages that contain the required index. Bitmap Heap scan → Once index scan is completed, it will now scan the heap and look for all the pages in the heap. The heap returns the pages which contain more rows, and can have rows which don’t have grade greater than 95. In the next step it performs recheck. Recheck index condition → This step applies the final filter to pick the required rows and discard others.explain select name from grades where g &gt; 95;Bitmap Heap Scan on grades (cost=2177.00..38294.62 rows=195170 width=15) Recheck Cond: (g &gt; 95) -&gt; Bitmap Index Scan on grades_score (cost=0.00..2128.21 rows=195170 width=0) Index Cond: (g &gt; 95)Let’s take another example where we’re adding another condition for id &lt; 10,000:There will be an index scan that will happen for the id column where it will mark pages that have ids less than 10000, and another bitmap index scan happens for pages that contain grades &gt; 95. Later the result from both scans undergoes a bitwise AND operation and is returned for page retrieval from Bitmap heap scan. The same can be understood from the diagram below:explain select name from grades where g &gt; 95 and id &gt; 10000;Bitmap Heap Scan on grades (cost=3988.96..5479.62 rows=394 width=4) Recheck Cond: ((g &gt; 95) AND (id &lt; 10000)) -&gt; BitmapAnd (cost=3988.96..3988.96 rows=394 width=0) -&gt; Bitmap Index Scan on grades_score (cost=0.00..180.21 rows=9570 width=0) Index Cond: (g &gt; 95) -&gt; Bitmap Index Scan on grades_pkey (cost=0.00..3803.31 rows=195170 width=0) Index Cond: (id &lt; 10000)Key vs Non-key Column Database IndexingLet’s understand using an example to see performance when data is added in index vs not in index.For a table created using this script:create table students ( id serial primary key, g int, firstname text, lastname text, middlename text, address text, bio text, dob date, id1 int, id2 int, id3 int, id4 int, id5 int, id6 int, id7 int, id8 int, id9 int); insert into students (g, firstname, lastname, middlename, address, bio, dob, id1, id2, id3, id4, id5, id6, id7, id8, id9) select random()*100, substring(md5(random()::text ),0,floor(random()*31)::int), substring(md5(random()::text ),0,floor(random()*31)::int), substring(md5(random()::text ),0,floor(random()*31)::int), substring(md5(random()::text ),0,floor(random()*31)::int), substring(md5(random()::text ),0,floor(random()*31)::int), now(), random()*100000, random()*100000, random()*100000, random()*100000, random()*100000, random()*100000, random()*100000, random()*100000, random()*100000from generate_series(0, 5000000);vacuum (analyze, verbose, full);Result of this query:explain analyze select id,g from students where g &gt; 80 and g &lt; 95 order by g;The output shows:Gather Merge (cost=157048.00..223006.83 rows=565322 width=8) (actual time=6018.645..21020.085 rows=700746 loops=1) Workers Planned: 2 Workers Launched: 2 -&gt; Sort (cost=156047.97..156754.63 rows=282661 width=8) (actual time=5979.320..8942.869 rows=233582 loops=3) Sort Key: g Sort Method: external merge Disk: 4136kB Worker 0: Sort Method: external merge Disk: 4128kB Worker 1: Sort Method: external merge Disk: 4120kB -&gt; Parallel Seq Scan on students (cost=0.00..126587.34 rows=282661 width=8) (actual time=4.775..3025.051 rows=233582 loops=3) Filter: ((g &gt; 80) AND (g &lt; 95)) Rows Removed by Filter: 1433085Planning Time: 0.138 msJIT: Functions: 12 Options: Inlining false, Optimization false, Expressions true, Deforming true Timing: Generation 0.986 ms, Inlining 0.000 ms, Optimization 0.717 ms, Emission 13.269 ms, Total 14.972 msExecution Time: 29755.348 msAs we can see, the query is very slow for 5 million records. Now let’s create an index on the g column:create index g_idx on students(g);Now if we run the query again:explain analyze select id,g from students where g &gt; 80 and g &lt; 95 order by g;The output is:Gather Merge (cost=157015.38..222971.41 rows=565298 width=8) (actual time=5943.602..20992.052 rows=700746 loops=1) Workers Planned: 2 Workers Launched: 2 -&gt; Sort (cost=156015.35..156721.97 rows=282649 width=8) (actual time=5906.640..8890.320 rows=233582 loops=3) Sort Key: g Sort Method: external merge Disk: 4128kB Worker 0: Sort Method: external merge Disk: 4128kB Worker 1: Sort Method: external merge Disk: 4128kB -&gt; Parallel Bitmap Heap Scan on students (cost=9253.60..126555.89 rows=282649 width=8) (actual time=64.650..3006.930 rows=233582 loops=3) Recheck Cond: ((g &gt; 80) AND (g &lt; 95)) Rows Removed by Index Recheck: 494862 Heap Blocks: exact=20568 lossy=11260 -&gt; Bitmap Index Scan on g_idx (cost=0.00..9084.01 rows=678358 width=0) (actual time=66.808..66.819 rows=700746 loops=1) Index Cond: ((g &gt; 80) AND (g &lt; 95))Planning Time: 0.378 msJIT: Functions: 12 Options: Inlining false, Optimization false, Expressions true, Deforming true Timing: Generation 1.021 ms, Inlining 0.000 ms, Optimization 0.769 ms, Emission 13.504 ms, Total 15.294 msExecution Time: 29852.581 msNormally anyone would expect the query to be faster now that we’re using an index, but the execution time is still the same or increased a little bit.One theory is that previously the database was looking only in the table, but now it is looking in the table plus at the primary key index id to retrieve the fields.Using this command we can determine whether a query is using buffer/cached data and for how many records:explain (analyze,buffers) select id,g from students where g &gt; 80 and g &lt; 95 order by g desc limit 1000;Output:Limit (cost=0.43..585.94 rows=1000 width=8) (actual time=0.072..51.714 rows=1000 loops=1) Buffers: shared hit=782 read=3 -&gt; Index Scan Backward using g_idx on students (cost=0.43..397181.31 rows=678358 width=8) (actual time=0.034..19.658 rows=1000 loops=1) Index Cond: ((g &gt; 80) AND (g &lt; 95)) Buffers: shared hit=782 read=3Planning: Buffers: shared hit=10 read=6Planning Time: 0.222 msExecution Time: 67.558 msread=3 → 3 IO operations to diskLet’s drop the index:drop index g_idx;Since we are retrieving id also with grade, let’s include id in the index:create index g_idx on students(g) include(id);Now if we run our query:explain (analyze,buffers) select id,g from students where g &gt; 80 and g &lt; 95;Output:Bitmap Heap Scan on students (cost=14256.95..162273.28 rows=671270 width=8) (actual time=128.376..9821.213 rows=700746 loops=1) Recheck Cond: ((g &gt; 80) AND (g &lt; 95)) Rows Removed by Index Recheck: 1484586 Heap Blocks: exact=62039 lossy=33262 Buffers: shared hit=2 read=97217 -&gt; Bitmap Index Scan on g_idx (cost=0.00..14089.13 rows=671270 width=0) (actual time=111.000..111.011 rows=700746 loops=1) Index Cond: ((g &gt; 80) AND (g &lt; 95)) Buffers: shared hit=1 read=1917Planning: Buffers: shared hit=10 read=6Planning Time: 0.237 msJIT: Functions: 4 Options: Inlining false, Optimization false, Expressions true, Deforming true Timing: Generation 0.249 ms, Inlining 0.000 ms, Optimization 0.211 ms, Emission 2.586 ms, Total 3.046 msExecution Time: 18434.583 msWe see there is some improvement, but even though id is present in the index, it is not being used and the query planner is using the heap for scanning id. The expectation is that it should not refer to the heap. On repeat queries, it uses cache. High IO means that some part of the index is loaded from disk.Index Scan vs Index Only Scanexplain analyze select id, name from grades where id = 10000;Index Scan using grades_pkey on grades (cost=0.43..8.45 rows=1 width=19) (actual time=0.098..0.135 rows=1 loops=1) Index Cond: (id = 10000)Planning Time: 1.225 msExecution Time: 0.246 msHere we had an index on the id column, so when we queried using id = it performed an index scan and returned the data.In this case, first an index lookup happens, then it uses the row_id (internal reference) to fetch the row from the heap. All columns of the row are retrieved, then it picks name and displays it.explain analyze select id from grades where id = 10000;Index Only Scan using grades_pkey on grades (cost=0.43..8.45 rows=1 width=4) (actual time=0.056..0.089 rows=1 loops=1) Index Cond: (id = 10000) Heap Fetches: 1Planning Time: 0.077 msExecution Time: 0.183 msFor this query, the DB doesn’t have to lookup the heap to fetch the id as it is already part of the index and it returns that data back. That’s why it’s an Index Only Scan.If our query somehow uses index only scan then it’s a jackpot as it reduces the response time of the DB to a very low value. One way to use this is to include non-key columns in the index. It is possible to include commonly retrieved data in the index so next time it can use the index.Here’s an example:create index id_idx on grades(id) include (name);explain analyze select id,name from grades where id=1000;Index Only Scan using id_idx on grades (cost=0.43..8.45 rows=1 width=19) (actual time=0.036..0.069 rows=1 loops=1) Index Cond: (id = 1000) Heap Fetches: 1Planning Time: 0.082 msExecution Time: 0.164 msNow I am able to retrieve name from the index and avoid lookup from the table.We need to have a good reason to include non-key columns in an index as it could create more IO operations for index loading once it starts growing larger. Size will become an issue.Combining Database Indexes for Better PerformanceLet’s consider a table:CREATE TABLE test( a INT, b INT, c TEXT);CREATE INDEX a_idx ON test(a);CREATE INDEX b_idx ON test(b);Query Behavior AnalysisCase 1: Simple WHERE ClauseSELECT c FROM test WHERE a = 70; Database will use Bitmap index scan to check a=70 in different parts of the page Later retrieves all records at once using Bitmap heap scan If we add LIMIT (e.g., LIMIT 2), it will use Index scan instead to avoid Bitmap overheadCase 2: AND ConditionSELECT c FROM test WHERE a = 100 AND b = 100; Uses Bitmap Index scan for both a and b separately Performs an AND operation on the results Returns the final result from the heapCase 3: OR ConditionSELECT c FROM test WHERE a = 100 OR b = 100; Execution time is longer than the AND query Performs a union of results from both index scans More pages to read due to the OR operationComposite Index ApproachDROP INDEX a_idx, b_idx;CREATE INDEX a_b_idx ON test(a,b);Composite Index Behavior: It does better job then doing AND on a and b or search on a but not search on b. It is a limitation of Postgres that for a composite key, it will use the index when both column is involved or only left side of the key column (a in our case) but not right side of the key column (b in our case will not use index if queried). For b-only queries, PostgreSQL will use a full table scan in parallel. If you want have faster scan for b column, you can separate index so in case for query involving b column, it will use index specific for b column. The advantage for this approach is, in case of OR operation, DB will use composite index when scanning for rows for a column and use b_idx for b column when scanning for rows for B column. It will leverage both index to avoid full table scan.Best Practice: For frequently queried columns, maintain separate indexes For OR operations, database will use appropriate indexes for each column This approach leverages both indexes to avoid full table scansDatabase Optimizer StrategiesReference: multiple-indexes.pdfCreating Indexes ConcurrentlyStandard Index CreationCREATE INDEX idx ON grade(g); Blocks write operations Allows read operations Makes application partially unusable during creationConcurrent Index CreationCREATE INDEX CONCURRENTLY g ON grades(g);Benefits: Allows index creation alongside write operations Waits for transaction completion before proceeding Application remains fully functionalDrawbacks: Slower index creation process Will fail if there are constraint violations Risk of failure increases as application continues adding recordsBloom FiltersReference: bloom-filter.pdfWorking with Billion-Row TablesApproach 1: Brute Force Full scan table with parallel workers Not practical - database will be very slowApproach 2: Indexing Reduces search from billions to millions of rows Significant performance improvementApproach 3: Partitioning Slice table into smaller parts Apply indexing to each partition Each partition can be replicated across different nodes Further narrows down search spaceApproach 4: Sharding + Partitioning Multiple shards, each with multiple partitions Each partition has indexing for efficient searching Reduces search space to few thousand recordsLast Resort: If none of the above works, use MapReduce Process all records in parallelRelated Articles Article - The Cost of Long running Transactions Article - Microsoft SQL Server Clustered Index Design Database Page" }, { "title": "Comprehensive Roadmap for Low-Level and High-Level Design Interview Preparation", "url": "/posts/system-design-roadmap/", "categories": "Blogging, Article", "tags": "softwareengineering, system-design, interview", "date": "2024-06-09 01:00:00 +0530", "snippet": "System design interviews are a critical part of the hiring process for software engineering roles, especially for senior positions. These interviews assess your ability to design scalable, maintainable, and efficient systems to solve real-world problems. In this blog, we’ll cover a combined roadmap for both Low-Level Design (LLD) and High-Level Design (HLD) interview preparation, along with strategies, expectations, and steps to excel in these interviews.What is a System Design Interview?In a system design interview, you are given a real-world problem and expected to design a system to solve it. Since no system is perfect, you must also identify trade-offs, pros, and cons of your design. The goal is to evaluate your ability to: Analyze requirements and define system constraints. Design scalable and maintainable systems. Identify trade-offs and justify your design decisions.Whether it’s low-level or high-level design, the core focus is on your problem-solving skills, understanding of system architecture, and ability to communicate your ideas effectively.Low-Level Design (LLD) InterviewsLow-Level Design focuses on the implementation details of a system. It involves designing class structures, defining entities, and applying design principles and patterns to create a robust and extensible system.Expectations in LLD Interviews: Requirement Gathering: Identify and define system requirements and constraints. Entity Identification: Define entities (classes) and their relationships. Design Principles: Apply principles like SOLID to ensure good design. Design Patterns: Use patterns like Strategy, Builder, Singleton, etc., to solve common design problems. System Maintainability: Ensure your design is extensible, loosely coupled, and easy to maintain.LLD Learning Strategy1. Understand Core Concepts Learn the fundamentals of object-oriented programming (OOP). Understand design principles like SOLID, DRY, and KISS. Study design patterns such as: Strategy Builder Singleton Factory Command Composition over Inheritance 2. Practice Problems Solve real-world problems like designing a Parking Lot, Elevator System, or Library Management System. Focus on identifying trade-offs and comparing your design with others.3. Steps in an LLD Interview Step 1: Define Model Classes: Identify entities and create classes for them. Step 2: Define Properties: Add fields and attributes to each class. Step 3: Define Behavior: Implement methods for each class. Start with top-level methods and drill down to discover additional methods as needed.High-Level Design (HLD) InterviewsHigh-Level Design focuses on the architecture and scalability of a system. It involves designing services, choosing data storage solutions, and ensuring the system can handle high loads and remain fault-tolerant.Expectations in HLD Interviews: Requirement Gathering: Define system constraints and use cases. Component Identification: Identify core components like databases, queues, caches, etc. Scalability and Availability: Design for high scalability, fault tolerance, and concurrency control. Trade-offs: Justify your design choices and discuss pros and cons.HLD Learning Strategy1. Learn Core Concepts Study scalability techniques like partitioning, sharding, and replication. Understand high availability concepts like quorums and leader election. Learn about backend components: Databases: SQL vs. NoSQL, database schemas, and indexing. Queues: Kafka, RabbitMQ, and their use cases. Caches: Redis, Memcached, and caching strategies. Blob Storage: S3, Google Cloud Storage, etc. 2. Practice Problems Solve problems like designing URL Shorteners, Social Media Platforms, or E-commerce Systems. Focus on creating component diagrams and defining high-level flows.3. Steps in an HLD Interview Step 1: Gather Requirements: Clarify use cases, constraints, and assumptions. Step 2: Create a High-Level Design: Sketch components (e.g., services, databases, caches) and their interactions. Step 3: Design Core Components: Dive deep into critical components (e.g., URL hashing, database schemas). Step 4: Scale the Design: Address bottlenecks using techniques like load balancing, caching, and sharding.Combined Roadmap for LLD and HLD Preparation1. Learn the Fundamentals LLD: Focus on OOP, design principles, and patterns. HLD: Study scalability, availability, and backend components.2. Practice Real-World Problems LLD: Practice designing small systems like Parking Lots or Elevator Systems. HLD: Practice designing large-scale systems like Instagram, Netflix, or URL Shorteners.3. Understand Trade-offs Analyze the pros and cons of your designs. Compare your solutions with others to identify improvements.4. Mock Interviews Simulate real interview scenarios with peers or mentors. Focus on clear communication and justifying your design choices.5. Resources Books: Designing Data-Intensive Applications by Martin Kleppmann. Videos: Watch system design tutorials on platforms like YouTube. Open Source: Study the documentation of tools like Kafka, Redis, and databases.Example Problems for PracticeLow-Level Design Problems: Design a Parking Lot System. Create a Library Management System. Design an Elevator Control System. Build a Vending Machine System.High-Level Design Problems: Design Instagram or Twitter. Create a URL Shortening Service. Build a Distributed Job Scheduler. Design a Payment Gateway. Create a Netflix-like Streaming Service.Another Format for System Design InterviewsYou need to understand that there is a flow to the System design interview. Which mostly looks like this: Requirement clarification - functional &amp; non-functional Estimations - Storage, Bandwidth, etc. Data flow High-level component design Detailed design Identify and address issues (System bottlenecks)Detailed Interview Steps1. Problem Statement Functional Requirements Non-Functional Requirements2. Back of the Envelope Estimate Estimating Queries Per Second Read vs. Write Characteristics Query Distribution Ratio Breaking Down Query Distribution Estimating Data Storage3. API Design Define different API endpoints to support the requirements4. Database Design Identifying Queries Record Size Estimation Schema Design Sharding Approach Selecting the right Database Data Partitioning and related problems5. High Level Design Create component level design to show what system will look like Use tools like Excalidraw for diagrams6. Technology-Specific DesignsDesign with Sorted Set Redis Read/Write Access Pattern Query PatternDesign with Cassandra Read/Write Access Pattern How token-aware driver works Query Pattern7. Maintaining System Reliability Address points to support this part of the systemReferencesBasic/Fundamental of System Design System Design Fundamentals: A Complete Guide for Beginners or view pdf System Design Fundamentals (YouTube) The System Design PrimerRoadmap Interview Prep: High Level Design Roadmap or view pdf Low Level Design Roadmap or view pdf High Level Design Roadmap or view pdfFinal Tips for Success Ask Questions: Clarify requirements and constraints before starting. Think Aloud: Communicate your thought process clearly. Iterate: Start with a basic design and refine it step by step. Focus on Trade-offs: No design is perfect; justify your choices. Practice, Practice, Practice: The more you practice, the better you’ll get." }, { "title": "Understanding the impact of inaccurate User Acceptance Testing Environment", "url": "/posts/understanding-the-impact-of-inaccurate-user-acceptance-testing-environment/", "categories": "Blogging, Article", "tags": "backenddevelopment, design, uat", "date": "2024-01-22 00:00:00 +0530", "snippet": "User Acceptance Testing (UAT) is a crucial stage in the software development lifecycle. This process provides an opportunity for end-users to validate the functionality, usability, and compatibility of a system before it is officially launched or delivered. However, the UAT phase can often be marked by unexpected complications, with systems frequently exhibiting incorrect responses or incomplete functioning. These issues can lead to significant delays, increased developmental costs, and potential damage to the client’s reputation. In this article, we will delve deeper into understanding these challenges and explore effective strategies to address them.Identifying the ChallengesThe journey to the UAT phase often begins with a system that seems to function perfectly in the development environment. The software, in this stage, appears to meet all specified requirements and performs as expected. However, when the system is introduced to the UAT environment, the smooth sailing often turns into a stormy ride. The system may begin to return incorrect responses or demonstrate incomplete or inconsistent functionality. These unexpected issues can significantly prolong the project timeline, inflate development costs, and potentially harm the vendor’s reputation.Problems due to inaccurate setupLet me share my recent experience with one of the stock depository services (Let’s call them D).The D’s system plays a vital role in the financial market, and one of the products that I am using is designed to facilitate the pledging of shares. This process involves an investor pledging their shares to a lender as collateral for a loan. Once the pledge is created, the D’s system is supposed to send a callback response confirming the successful pledging of shares. However, issues arise due to the faulty state management, session management, security management design of the system in UAT. In Production environment, all works fine.generated using bing aiFaulty state management of the transaction in UATLet’s first start the discussion with state management of the transaction. It is of serious concern, if system is poorly handling state management. Lets me share one instance where I have faced problem due to this. In D’s system, every time a client’s customer wants to start the pledging flow, one transaction gets created which is used to identify the request in D’s internal system. Clients (Bank or NBFC) have to persist necessary details to query the state of the transaction at a later point in time to know progress as flow is performed on D’s portal and the only way to know is via callback response.A transaction can be in one of the following states, customer login to the web portal customer pledged the shares (using their Demat account) Depository Participants processed the shares for pledging Shares got transferred to Clients (Demat account)Failure response is not returned in status API, but callback response returns the status in UATTransaction status returned in callback response and transaction status API is not consistent. If some error happens, the callback response will provide the details of failure but let’s say if we call transaction status API, it will return the last successful status but not the failure details. This inconsistency forced clients to implement some state management at their end with additional logic. This is not good.Some cases cannot be tested in UAT for that requires a Prod account to testThere were some cases that I wanted to try out on the UAT environment which is supposed to be a replica of the prod environment, but when I heard that those cases I could not test in UAT, I asked question Why it cannot be tested in UAT. well, I didn’t get any proper response. The impact of this is I have to test in Prod and if all cases are successful then great, otherwise I have to fix it. This is just causing more delay for product release and causing inconvenience to the clients. If the system is well tested and proper testing is been employed with proper data, I don’t have to wait until Prod deployment to test the functionality. This simply explains the engineering practice by D’s tech team is bad and they have to do something. Since they have a good number of active Demat accounts, Banks and NBFCs have to work on some solutions to mitigate various problems. Also, It is not easy for newcomers to build a new depository service business as a barrier to entering this type of market is not easy, So seems like they don’t care now. But if other existing competitors come up with robust and better systems, D will be in trouble.Faulty session management in UATSession management is complex, specially if your user is allowed to login on multiple device. Let me share one instance where poor configuration caused trouble to my me. As per the documentation, D’s system should timeout the session if the transaction is in one of the below states, Got redirected the client’s app to D’s portal for login but no action for 10 mins. Post login, If the customer didn’t pledge shares within 15 min then the session will time out.In UAT, the second point cannot be tested and their team says some development is going on so it is disabled. This is a very bad management of the environment by D’s tech team. Ideally, they should not be using UAT as it is used by the external client to test their system and use that behavior as a reference for prod release. In one of the tests, I can go beyond 35 minutes in a single session post-login. There are various complex things clients could be doing and based on session timeout clients may want to restart the transaction as they want to lend the loan to the customer. This simply proves that D’s UAT is not in good shape. Due to the above reasons, my team has to rely on documentation to proceed with the code changes. It is frustrating as we cannot test our system using their UAT system. Also, the UAT system is not at all close to the production environment which gives less confidence to me whether my code will work properly or not. This is a kind of nightmare for anyone who is chasing a tight deadline.Faulty security management in UATThis is something no one wants to ignore being in the finance industry and you are building an app that your clients going to use for financial transactions.generated using bing aiA digital signature is asked for but not validatedDigital signature verification is a good security practice. Let me share an instance where inconsistency with this functionality in two different environments caused more confusion. According to the documentation, the server checks whether data has been tampered with for requests coming from the client. It creates a new digital signature of the data and compares it with the one sent by the client. But in UAT, this is not working, and the request works with a wrong digital signature. This is a serious problem as I don’t know whether the code changes added by me will work in production or not; And whether the digital signature generated is accepted by the server. If in production, the request fails due to an issue in digital signature it will again cause a delay to the product release.API contract asks for more details than needed to get transaction status.It is always a good practice to ask only necessary information to return data to the consumer of the API. In my case, The API that I am going to discuss has more details like digital signature, requestor name, requestor ID, transaction ID, and transaction initiation time.I found that, if the requestor name and transaction ID are provided then it could return the data. If only 2 information is requested, it is a waste to provide other information. The creation of a digital signature is computationally expensive, and if not required, it should be removed from the contract.One of the biggest risks that I see with this API is that, it works if we provide wrong data for digital signature, requestor ID, and transaction initiation time but correct data for requestor name and transaction ID. It exposes another brute force attack which could leak information for other clients in UAT.Navigating the Challengesgenerated using bing aiGiven the potential implications of these challenges, it’s crucial to navigate them effectively. One of the most effective strategies is to ensure that the UAT environment closely mirrors the actual production environment. This involves aligning the system configurations, using realistic test data, and simulating actual user behavior and load.Regular synchronization between the UAT and production environments can also play a critical role in the early detection and resolution of potential issues. By continually aligning these two environments, discrepancies can be identified and addressed before they escalate into larger problems.Another key part of navigating these challenges involves adopting a comprehensive and thorough testing strategy. This strategy should encompass various types of testing, including unit testing, integration testing, system testing, and acceptance testing, among others. Each phase of testing should be designed to identify specific types of issues, thereby enhancing the system’s overall robustness and reliability.ConclusionThe impact of inaccurate User Acceptance Testing (UAT) environments on software development projects, as illustrated through the experiences with a stock depository service (referred to as D), highlights significant challenges and potential consequences. The identified issues in the faulty state management, session management, and security checks during the UAT phase can lead to delays, increased development costs, and erode confidence in the system’s reliability.The challenges stem from discrepancies between the UAT and production environments, where unexpected behaviors arise in the UAT environment, jeopardizing the accuracy of testing outcomes. The issues with transaction status inconsistencies, untestable scenarios, session timeout discrepancies, and security vulnerabilities underscore the need for a more robust UAT environment.To address these challenges, it is crucial for organizations to ensure that their UAT environment closely mirrors the production environment. Regular synchronization, realistic test data, and simulation of actual user behavior can contribute to early issue detection and resolution. Additionally, adopting a comprehensive testing strategy that includes various testing phases is essential for enhancing the system’s robustness.The experiences shared serve as a reminder of the importance of maintaining the integrity of UAT processes. Successful navigation of these challenges requires a commitment to aligning environments, implementing thorough testing practices, and fostering a collaborative approach between development teams and end-users. Ultimately, a well-structured and accurate UAT environment is imperative for ensuring the successful and timely release of high-quality software products." }, { "title": "REST API Based Workflow Design Using iWF Framework", "url": "/posts/workflow-using-iwf-framework/", "categories": "Blogging, Article", "tags": "backenddevelopment, design, java, softwareengineering", "date": "2023-10-02 00:00:00 +0530", "snippet": "Using the iWF DSL framework to write workflow on the top of the Temporal platformIn this article, We are going to discuss Workflow and design a simple use case using the iWF framework (with Temporal Server).Part 1: Basics ConceptsBefore directly jumping to code, Let’s see some concepts about workflow and Temporal Server.Runtime platformProvide the ecosystem to run your applications and take care of the durability, availability, and scalability of the application. Both Cadence(from Uber) and Temporal share the same behavior as Temporal is forked from Cadence. Worker Processes are hosted by you and execute your code. The communication within the Cluster uses gRPC. Cadence/Temporal service is responsible for keeping workflow state and associated durable timers. It maintains internal queues (called task lists) which are used to dispatch tasks to external workers. Workflow execution is resumable, recoverable, and reactive. Cadence Doc Temporal Doc iWF ProjectTemporal System Overview for workflow executionWhat is Workflow?The term Workflow frequently denotes either a Workflow Type, a Workflow Definition, or a Workflow Execution. Workflows are sequences of tasks/steps that are executed in a specific order. It is based on the principle of separation of concerns. It focuses on the design and implementation of business processes as workflows. Workflow Definition: A Workflow Definition is the code that defines the constraints of a Workflow Execution. A Workflow Definition is often also referred to as a Workflow Function. Deterministic constraints: A critical aspect of developing Workflow Definitions is ensuring they exhibit certain deterministic traits – that is, making sure that the same Commands are emitted in the same sequence, whenever a corresponding Workflow Function Execution (instance of the Function Definition) is re-executed. Handling unreliable Worker Processes: Workflow Function Executions are completely oblivious to the Worker Process in terms of failures or downtime. Event Loop: Workflow execution states:What is a Workflow Engine? A workflow engine facilitates the flow of information, tasks, and events. The workflow engine is responsible for managing the execution of workflows. Workflow engines may also be referred to as Workflow Orchestration Engines The other components of the system are responsible for performing the specific tasks that make up the workflowsWhat is the activities or workflow state? An Activity is a normal function or method that executes a single, well-defined action (either short or long-running), such as calling another service, transcoding a media file, or sending an email message. Workflow code orchestrates the execution of Activities, persisting the results. If an Activity Function Execution fails, any future execution starts from the initial state Activity Functions are executed by Worker Processes Workflow State is used in the domain of the iWF framework which is the same as Activities in Cadence or Temporal.Event handlingWorkflows can be signaled about an external event. A signal is always point-to-point destined to a specific workflow instance. Signals are always processed in the order in which they are received. Human Tasks Process Execution Alteration SynchronizationExample: there is a requirement that all messages for a single user are processed sequentially but the underlying messaging infrastructure can deliver them in parallel. The Cadence solution would be to have a workflow per user and signal it when an event is received. Then the workflow would buffer all signals in an internal data structure and then call an activity for every signal received.Visibility View, Filter, and Search for Workflow Executions https://docs.temporal.io/visibility#list-filter-examples https://docs.temporal.io/visibility#search-attribute Query Workflow statePart 2: Temporal Server DesignBoth Cadence and Temporal provide a platform to execute our workflow function which is nothing but business logic.What are the components of the Cadence/Temporal server?The server consists of four independently scalable services: Frontend gateway: for rate limiting, routing, and authorizing. History service:maintains data (workflow mutable state, event and history storage, task queues, and timers). Matching service: hosts Task Queues for dispatching. Worker Service: Worker Service: for internal background Workflows (replication queue, system Workflows). Read more…Part 3: iWF Framework Design (Temporal as Backend)iWF is the framework that is developed to simply run the workflow and harness the full potential of the Cadence/Temporal Server.High-Level DesignAn iWF application is composed of several iWF workflow workers. These workers host REST APIs as “worker APIs” for the server to call. This callback pattern is similar to AWS Step Functions invoking Lambdas if you are familiar with it.An application also performs actions on workflow executions, such as starting, stopping, signaling, and retrieving results by calling iWF service APIs “service APIs”.The service APIs are provided by the “API service” in the iWF server. Internally, this API service communicates with the Cadence/Temporal service as its backend.Low-Level DesignUsers define their workflow code with a new SDK “iWF SDK” and the code is running in workers that talk to the iWF interpreter engine.The user workflow code defines a list of WorkflowState and kicks off a workflow execution. At any workflow state, the interpreter will call back the user workflow code to invoke some APIs (waitUntil or execute). Calling the waitUntil API will return some command requests. When the command requests are finished, the interpreter will then call the user workflow code to invoke the “execute” API to return a decision.The decision will decide how to complete or transition to other workflow states. At any API, workflow code can mutate the data/search attributes or publish to internal channels.RPC: Interact with workflow via APIUsing RPC annotation is one of the ways to interact with the workflow from external sources like REST API, and Kafka event. It can access persistence, internal channels, and state execution. RPC vs SignalBoth RPC and Signal are the two ways to communicate from an external system with the workflow execution. RPC is a synchronous API call - Definition The signal channel is an Asynchronous API.Some recommend, as a best practice, to use RPC with an Internal channel to asynchronously call the workflow. It is basically to replace the Signal API.RPC + Internal Channel =&gt; Signal Channel Internal-Channel and Signal Channel are both message queuesiWF Approach to Determinism and VersioningThere are some problems with the history replay for the workflow which causes non-determinism issues due to events like workflow state deletion or business logic changes, etc. iWF framework recommends using the flag to control the code execution as versioning is removed. Since there is no versioning, the non-determinism issue will not happen. Read more: IWF docExample of Atomicity using RPC for sending message, state transition, and saving data in DB.public class UserSignupWorkflow implements ObjectWorkflow { ... // Atomically read/write/send message in RPC @RPC public String verify(Context context, Persistence persistence, Communication communication) { String status = persistence.getDataAttribute(DA_Status, String.class); if (status.equals(\"verified\")) { return \"already verified\"; } persistence.setDataAttribute(DA_Status, \"verified\"); communication.publishInternalChannel(VERIFY_CHANNEL, null); return \"done\"; } ...}Part 4: Simple workflow example using iWFBelow is the workflow diagram of the KYC application based on Aadhaar.Step 1: Write Workflow definitionpublic class AadhaarKycWorkflow implements ObjectWorkflow { private final List&lt;StateDef&gt; stateDefs; public AadhaarKycWorkflow(Client client) { this.stateDefs = List.of( StateDef.startingState(new GenerateAadhaarOtpStep()), StateDef.nonStartingState(new ValidateAadhaarOtpStep()), StateDef.nonStartingState(new SaveAadhaarDetailsStep(client)) ); } @Override public List&lt;StateDef&gt; getWorkflowStates() { return stateDefs; } @Override public List&lt;PersistenceFieldDef&gt; getPersistenceSchema() { return List.of( SearchAttributeDef.create(SearchAttributeValueType.TEXT, \"customer_id\"), SearchAttributeDef.create(SearchAttributeValueType.TEXT, \"aadhaar_id\"), SearchAttributeDef.create(SearchAttributeValueType.TEXT, \"parentWorkflowId\") ); } @Override public List&lt;CommunicationMethodDef&gt; getCommunicationSchema() { return List.of( SignalChannelDef.create(String.class, \"AadhaarOtpSignal\"), SignalChannelDef.create(String.class, SC_SYSTEM_KYC_COMPLETED) ); }}StateDef.startingState: Starting step/task/activity which workflow will execute.StateDef.nonStartingState: It will be executed based on the State’s decision.getPersistenceSchema(): return types of data that will be accessed by the workflow. This data will be persisted as long as workflow history is preserved.getCommunicationSchema(): different types of communication that workflow will require to complete the tasks.Step 2: Write Workflow StateIt is also called the actual business rules that you want workflow to execute.public class ValidateAadhaarOtpStep implements WorkflowState&lt;String&gt; { @Override public Class&lt;String&gt; getInputType() { return String.class; } @Override public CommandRequest waitUntil(Context context, String input, Persistence persistence, Communication communication) { return CommandRequest.forAllCommandCompleted( SignalCommand.create(\"AadhaarOtpSignal\") ); } @Override public StateDecision execute(Context context, String aadhaarReferenceId, CommandResults commandResults, Persistence persistence, Communication communication) { var otp = (String) commandResults.getSignalValueByIndex(0); if (validateOtp(aadhaarReferenceId, otp)) { var details = fetchAadhaarDetails(aadhaarReferenceId, otp); return StateDecision.singleNextState(SaveAadhaarDetailsStep.class, details); } return StateDecision.singleNextState(ValidateAadhaarOtpStep.class, aadhaarReferenceId); } private Boolean validateOtp(String aadhaarReferenceId, String otp) { log.info(\"call aadhaar validate OTP API and fetch details for referenceId:{} and OTP:{}\", aadhaarReferenceId, otp); return Objects.equals(otp, \"1234\"); }}waitUntil() and execute(): are the two sub-steps that the workflow state executed in sequence to finish the task.waitUntil(): It returns Signals, Timer, or Internal event that the task is waiting to happen. Once that event is completed, execute() will be invoked.StateDecision: It returns the next state that workflow should be expected to execute. This will be executed only when the Temporal/Cadence Server schedules the task on the internal worker queue.Step 3: REST API endpoint to provide input to workflow @PostMapping(\"/kyc/aadhaar/otp\") ResponseEntity&lt;Response&gt; validateAadhaarOtp( @RequestParam String otp, @RequestParam String customerId) { var workflowId = getWorkflowIdForAadhaar(customerId); var response = client.describeWorkflow(workflowId); if (response.getWorkflowStatus().equals(WorkflowStatus.RUNNING)) { client.signalWorkflow(AadhaarKycWorkflow.class, workflowId, \"AadhaarOtpSignal\", otp); return ResponseEntity.ok(new Response(\"success\", \"\")); } return ResponseEntity.internalServerError().body(new Response(\"Workflow not running\", \"\")); } private String getWorkflowIdForAadhaar(String customerId) { return \"WF-LAMF-KYC-\"+customerId; }Part 5: Different Use CasesBelow are the examples to understand the usage of different APIs of the iWF framework. Microservice Orchestration user signup workflow KYC Workflow Product order workflow Loan application workflowProject Link: Github project iWF ProjectConclusioniWF framework has really simplified writing applications using workflow-oriented architecture. Writing applications with the direct APIs provided by Cadence/Temporal has a steep learning curve. Due to this, beginners make some common mistakes, and writing a workflow that uses the full potential of the system is challenging for newcomers.iWF Project is basically a wrapper on the top of Cadence and Temporal System which helps lower the learning curve and also helps writing workflow that uses the full potential of the system which is really great." }, { "title": "Code Smell Series: Big Class", "url": "/posts/code-smell-big-classs/", "categories": "Blogging, CodeSmellSeries", "tags": "coding, smells", "date": "2023-05-13 00:00:00 +0530", "snippet": " The secret to high-quality code is following the path of clean code.Is it a bad approach to have large code files?Yes, It’s bad to have big classes/files in programming. A big file means it is doing many things, and gets modified for multiple reasons. A file should have only one reason to change. It means it should be responsible for one thing.How to resolve the issue of big class?One of the techniques is to have the test in place for the target code for refactoring. If the previous developer has followed TDD, you already have the test code; Otherwise, write enough tests to cover the core functionality of the target code. Perform the refactoring to segregate the target code into different small files until each file serves a single purpose. Rerun the test. Once all test is green, we confidently say, the core functionality is preserved and refactoring is successful.Why is my refactoring taking more time?If there is a refactoring problem, it means your code is complex and has more dependencies on other objects. In the long run, creates an unexpected class that is dangerous to touch and hard to understand called God Class. God class means knowing more than it should know and is doing more than it should.Too much dependency causes code to take more space and time to instantiate. Refactoring takes more time, which indirectly means the target code has poor understandability. Here, most of the time spent in understanding the behavior and flow. It is not good, and refactoring method like method extraction and segregation could take more time than anticipated by the other developer.Below is an example of an implementation of a wrapper class that aggregates 3 API and expose a single API endpoint to be used by the requester to invoke 3 APIs in sequence.file name: LoanCreationWrapper.class(simplified)@Autowired ClientService clientService;@Autowired LoanAccountService loanAccountService;@Autowired LoanContractService loanContractService;public WrapperResponse createLoan (WrapperRequest wrapperRequest) {...}private void prepareWrapperResponse (WrapperResponse response, WrapperResponseBody responseBody) {...} private static LoanAccountCreation getSkippedLoanAccountResponse ( WrapperLoanAccountCreationRequest loanAccountData){...}private static LoanContractCreation getSkippedLoanContractResponse ( WrapperLoanContractCreationRequest loanContractData){...}private LoanContractCreation createLoanContract ( WrapperLoanContractCreationRequest loanContractData,String loanAccountId){...}private static LoanContractCreation getFailedIrrecoverableLoanContractCreationResponse( WrapperLoanContractCreationRequest loanContractData, String message) {...}private static LoanContractCreation getFailedRecoverableLoanContractCreationResponse ( WrapperLoanContractCreationRequest loanContractData, String message) {...}private static LoanContractCreation getSkippedLoanContractCreationResponse ( WrapperLoanContractCreationRequest loanContractData){...}private static LoanContractCreation getSuccessLoanContractCreationResponse ( WrapperLoanContractCreationRequest loanContractData,LoanContractCreationSuccessResponse loanContractCreationServiceResponse){...}private LoanContractCreationCallerRequest getLoanContractCreationCallerRequest ( WrapperLoanContractCreationRequest loanContractData,String loanAccountId ) throws DataNotFoundException,JsonProcessingException, InternalServerError{...}private LoanAccountCreation createLoanAccount ( WrapperLoanAccountCreationRequest loanAccountData,String customerId, String losLeadId) {...}private LoanAccountCreationRequest getLoanAccountCreationCallerRequest ( WrapperLoanAccountCreationRequest loanAccountData,String customerId, String losLeadId) throws DataNotFoundException, InternalServerError{...}private static LoanAccountCreation getSkippedLoanAccountCreationResponse ( WrapperLoanAccountCreationRequest loanAccountData){...}private static LoanAccountCreation getFailedRecoverableLoanAccountCreationResponse( WrapperLoanAccountCreationRequest loanAccountData,String message) {...}private static LoanAccountCreation getFailedIrrecoverableLoanAccountCreationResponse( WrapperLoanAccountCreationRequest loanAccountData, String message) {...}private static LoanAccountCreation getSuccessLoanAccountCreationResponse( WrapperLoanAccountCreationRequest loanAccountData, LoanAccountCreationSuccessResponse loanAccountCreationServiceResponse){...}private MandateDetails getMandateDetails( WrapperMandateDetails mandateDetails) {...}private SanctionLimit getSanctionLimit( WrapperSanctionLimit sanctionLimit) {...}private ClientCreditInformation getClientCreditInformation ( WrapperClientCreditInformation clientCreditInformation) throws DataNotFoundException, InternalServerError {...}private CreditBureauMilesData fetchCreditBureauData ( WrapperClientCreditInformation clientCreditInformation) throws DataNotFoundException, InternalServerError{...}private LoanAccountDetailsRequest getLoanAccountDetails ( WrapperLoanAccountDetails loanAccountDetails,String customerId, String losLeadId) throws DataNotFoundException, InternalServerError{...}private PaymentModeMilesData fetchPaymentModeData ( String paymentMode) throws DataNotFoundException, InternalServerError {...}private ClientCreation createClient (WrapperClientCreationRequest clientData) {...}private static ClientCreation getFailedRecoverableClientCreationResponse ( WrapperClientCreationRequest clientData, String message) {...}private static ClientCreation getSkippedClientCreationResponse ( WrapperClientCreationRequest clientData){...}private static ClientCreation getSkippedClientCreationResponse ( WrapperClientCreationRequest clientData){...}private static ClientCreation getSuccessClientCreationResponse ( WrapperClientCreationRequest clientData,ClientCreationSuccessResponse clientCreationServiceResponse){...}private static ClientCreation getFailedIrrecoverableClientCreationResponse ( WrapperClientCreationRequest clientData,String message) {...}private ClientCreationCallerRequest getClientCreationCallerRequest ( WrapperClientCreationRequest clientData) throws InternalServerError, JsonProcessingException, DataNotFoundException {...}private String fetchOccupationCode ( WrapperClientCreationRequest clientData) throws DataNotFoundException, InternalServerError{...}private CountryMilesData fetchCountryData ( WrapperClientCreationRequest clientData) throws DataNotFoundException, InternalServerError {...}private String fetchProfessionData ( WrapperClientCreationRequest clientData) throws DataNotFoundException, InternalServerError {...}private StateMilesData fetchStateData ( String stateAbbreviation) throws DataNotFoundException, InternalServerError {...}private List&lt;Bank&gt; getBankDetails ( List&lt;WrapperClientBankDetails&gt; clientBankDetails) throws JsonProcessingException,InternalServerError,DataNotFoundException {...}private Branch getBranchDetails ( WrapperClientBranchDetails clientBranchDetails) throws DataNotFoundException, InternalServerError{...}private BranchData fetchBranchData ( WrapperClientBranchDetails clientBranchDetails) throws DataNotFoundException, InternalServerError {...}private SubBroker getSubBrokerDetails ( WrapperClientSubBrokerDetails clientSubBrokerDetails) throws JsonProcessingException, InternalServerError, DataNotFoundException{...}private PrimaryRelationshipManager getPrimaryRMDetails ( WrapperClientPrimaryRM clientPrimarvRM) throws JsonProcessingException, InternalServerError, DataNotFoundException{...}private GST getGSTDetails (WrapperClientGSTDetails clientGSTDetails) throws DataNotFoundException. InternalServerError{...}As we can observe from the above code snippet, numerous responsibilities are assigned to the above file. Above file knows too many things about the dependency objects. If we see there is some grouping created with space separation to visually indicate that they are not related and serves different purpose based on functionality.Refactoring of the above code took more than half a day to understand and decide how to segregate the large class, which is knowing too much about the internal of 3 APIs in the smaller classes. Here methods are moved to LoanContractService, LoanAccountService, and ClientService respectively.Post refactoring above implementation looks like the below,file name: LoanCreationWrapper.class@Autowired ClientService clientService;@Autowired LoanAccountService loanAccountService;@Autowired LoanContractService loanContractService;public WrapperResponse createLoan (WrapperRequest wrapperRequest) {...}private void prepareWrapperResponse (WrapperResponse response, WrapperResponseBody responseBody) {...}file name: ClientService.class...private ClientCreation createClient (WrapperClientCreationRequest clientData) {...}private static ClientCreation getFailedRecoverableClientCreationResponse ( WrapperClientCreationRequest clientData, String message) {...}private static ClientCreation getSkippedClientCreationResponse ( WrapperClientCreationRequest clientData){...}private static ClientCreation getSkippedClientCreationResponse ( WrapperClientCreationRequest clientData){...}private static ClientCreation getSuccessClientCreationResponse ( WrapperClientCreationRequest clientData,ClientCreationSuccessResponse clientCreationServiceResponse){...}private static ClientCreation getFailedIrrecoverableClientCreationResponse ( WrapperClientCreationRequest clientData,String message) {...}private ClientCreationCallerRequest getClientCreationCallerRequest ( WrapperClientCreationRequest clientData) throws InternalServerError, JsonProcessingException, DataNotFoundException {...}private String fetchOccupationCode ( WrapperClientCreationRequest clientData) throws DataNotFoundException, InternalServerError{...}private CountryMilesData fetchCountryData ( WrapperClientCreationRequest clientData) throws DataNotFoundException, InternalServerError {...}private String fetchProfessionData ( WrapperClientCreationRequest clientData) throws DataNotFoundException, InternalServerError {...}private StateMilesData fetchStateData ( String stateAbbreviation) throws DataNotFoundException, InternalServerError {...}private List&lt;Bank&gt; getBankDetails ( List&lt;WrapperClientBankDetails&gt; clientBankDetails) throws JsonProcessingException,InternalServerError,DataNotFoundException {...}private Branch getBranchDetails ( WrapperClientBranchDetails clientBranchDetails) throws DataNotFoundException, InternalServerError{...}private BranchData fetchBranchData ( WrapperClientBranchDetails clientBranchDetails) throws DataNotFoundException, InternalServerError {...}private SubBroker getSubBrokerDetails ( WrapperClientSubBrokerDetails clientSubBrokerDetails) throws JsonProcessingException, InternalServerError, DataNotFoundException{...}private PrimaryRelationshipManager getPrimaryRMDetails ( WrapperClientPrimaryRM clientPrimarvRM) throws JsonProcessingException, InternalServerError, DataNotFoundException{...}private GST getGSTDetails (WrapperClientGSTDetails clientGSTDetails) throws DataNotFoundException. InternalServerError{...}file name: LoanAccountService.class...private LoanAccountCreation createLoanAccount ( WrapperLoanAccountCreationRequest loanAccountData,String customerId, String losLeadId) {...}private LoanAccountCreationRequest getLoanAccountCreationCallerRequest ( WrapperLoanAccountCreationRequest loanAccountData,String customerId, String losLeadId) throws DataNotFoundException, InternalServerError{...}private static LoanAccountCreation getSkippedLoanAccountCreationResponse ( WrapperLoanAccountCreationRequest loanAccountData){...}private static LoanAccountCreation getFailedRecoverableLoanAccountCreationResponse( WrapperLoanAccountCreationRequest loanAccountData,String message) {...}private static LoanAccountCreation getFailedIrrecoverableLoanAccountCreationResponse( WrapperLoanAccountCreationRequest loanAccountData, String message) {...}private static LoanAccountCreation getSuccessLoanAccountCreationResponse( WrapperLoanAccountCreationRequest loanAccountData, LoanAccountCreationSuccessResponse loanAccountCreationServiceResponse){...}private MandateDetails getMandateDetails( WrapperMandateDetails mandateDetails) {...}private SanctionLimit getSanctionLimit( WrapperSanctionLimit sanctionLimit) {...}private ClientCreditInformation getClientCreditInformation ( WrapperClientCreditInformation clientCreditInformation) throws DataNotFoundException, InternalServerError {...}private CreditBureauMilesData fetchCreditBureauData ( WrapperClientCreditInformation clientCreditInformation) throws DataNotFoundException, InternalServerError{...}private LoanAccountDetailsRequest getLoanAccountDetails ( WrapperLoanAccountDetails loanAccountDetails,String customerId, String losLeadId) throws DataNotFoundException, InternalServerError{...}private PaymentModeMilesData fetchPaymentModeData ( String paymentMode) throws DataNotFoundException, InternalServerError {...}private static LoanAccountCreation getSkippedLoanAccountResponse ( WrapperLoanAccountCreationRequest loanAccountData){...}file name: LoanContractService.class...private LoanContractCreation createLoanContract ( WrapperLoanContractCreationRequest loanContractData,String loanAccountId){...}private static LoanContractCreation getFailedIrrecoverableLoanContractCreationResponse( WrapperLoanContractCreationRequest loanContractData, String message) {...}private static LoanContractCreation getFailedRecoverableLoanContractCreationResponse ( WrapperLoanContractCreationRequest loanContractData, String message) {...}private static LoanContractCreation getSkippedLoanContractCreationResponse ( WrapperLoanContractCreationRequest loanContractData){...}private static LoanContractCreation getSuccessLoanContractCreationResponse ( WrapperLoanContractCreationRequest loanContractData,LoanContractCreationSuccessResponse loanContractCreationServiceResponse){...}private LoanContractCreationCallerRequest getLoanContractCreationCallerRequest ( WrapperLoanContractCreationRequest loanContractData,String loanAccountId ) throws DataNotFoundException,JsonProcessingException, InternalServerError{...}private static LoanContractCreation getSkippedLoanContractResponse ( WrapperLoanContractCreationRequest loanContractData){...}Structure of Wrapper response is,file name: WrapperResponse.classpublic class WrapperResponse {// other fields\tprivate WrapperResponseBody wrapperResponseBody;}public class WrapperResponseBody {\tprivate ClientResponse clientResponse;\tprivate LoanAccountResponse loanAccountResponse;\tprivate LoanContractResponse loanContractResponse;}In short, the wrapper should only invoke the API and collect the responses and return.Different refactoring techniques used to achieve the above results, Extract Method Move Method Remove Middle Man Push Down MethodConclusionWhen we dirty our hands while working with such code in a real project, it will not be that easy to identify patterns or grouping. It took me time to understand the code and later perform the refactoring. Refactoring could take additional time if the test of the target code is complex. I have experienced one instance, where I had to understand code and tests to perform the refactoring. Due to its complex implementation and testing, it took more time than expected." }, { "title": "Understaning RAFT - distributed consensus protocol", "url": "/posts/understanding-raft-distributed-consensus-protocol/", "categories": "Presentation, system-design", "tags": "design, backenddevelopment, softwareengineering", "date": "2023-05-07 00:00:00 +0530", "snippet": "RAFT - Distributed Consensus ProtocolIn this presentation, I talked about the RAFT consensus algorithm which is commonly used in large different systems to achieve consensus in a distributed environment. Many systems uses a custom implementation of the RAFT algorithm that is understandable and easy to implement compared to the Paxos algorithm for distributed consensus protocol.We will start from the basic understanding of the algorithm and later see an issue with this algorithm. We will also talk on recent incident that caused an internet outage for many people and businesses.Deck Diagramdownload and view svg file in separate viewer.Further readingsBlogs A Byzantine failure in the real world Consistent Core Generation Clock Emergent leader HeartbeatVideo: “Raft - The Understandable Distributed Protocol” by Ben Johnson (2013)" }, { "title": "Code Smell Series: Unit Test", "url": "/posts/code-smell-in-unit-test/", "categories": "Blogging, CodeSmellSeries", "tags": "coding, smells", "date": "2023-05-01 20:00:00 +0530", "snippet": " The secret to high-quality code is following the path of clean code.A code will remain at its highest quality if it has an understandable and meaningful test. Robert C. Martin in his book, “Clean Code: A Handbook of Agile Software Craftsmanship” mentioned a very nice acronym for clean code in unit testing.F.I.R.S.T.As per clean code practice, all tests follow the five rules below that form the F.I.R.S.T. acronym. The below explanation is taken from the Clean Code Book by Robert C. Martin,Fast: Tests should be fast. They should run quickly. When tests run slow, you won’t want to run them frequently. If you don’t run them frequently, you won’t find problems early enough to fix them easily. You won’t feel as free to clean up the code. Eventually, the code will begin to rot.Independent: Tests should not depend on each other. One test should not set up the conditions for the next test. You should be able to run each test independently and run the tests in any order you like. When tests depend on each other, then the first one to fail causes a cascade of downstream failures, making diagnosis difficult and hiding downstream defects.Repeatable: Tests should be repeatable in any environment. You should be able to run the tests in the production environment, in the QA environment, and on your local development machine without a network. If your tests aren’t repeatable in any environment, then you’ll always have an excuse for why they fail. You’ll also find yourself unable to run the tests when the environment isn’t available.Self-Validating: The tests should have a boolean output. Either they pass or fail. You should not have to read through a log file to tell whether the tests pass. You should not have to manually compare two different text files to see whether the tests pass. If the tests aren’t self-validating, then failure can become subjective, and running the tests can require a long manual evaluation.Timely: The tests need to be written in a timely fashion. Unit tests should be written just before the production code that makes them pass. If you write tests after the production code, then you may find the production code to be hard to test. You may decide that some production code is too hard to test. You may not design the production code to be testable.Rule 1: Single Concept per TestAs per this rule, a single test function should test a single thing/concept. We don’t want long test functions that go testing one miscellaneous thing after another. Below code snippet is an example of such a test. This test function should be segregated into three independent tests because it tests three independent things. Merging them all together into the same function forces the reader to figure out why each section is there and what is being tested by that section.Example of bad test,/*** tests for the addMonths() method.*/@Testpublic void should_add_Months() { var d1 = CustomDate.createInstance(31, 5, 2004); var d2 = CustomDate.addMonths(1, d1); assertEquals(30, d2.getDayOfMonth()); assertEquals(6, d2.getMonth()); assertEquals(2004, d2.getYYYY()); var d3 = CustomDate.addMonths(2, d1); assertEquals(31, d3.getDayOfMonth()); assertEquals(7, d3.getMonth()); assertEquals(2004, d3.getYYYY()); var d4 = CustomDate.addMonths(1, CustomDate.addMonths(1, d1)); assertEquals(30, d4.getDayOfMonth()); assertEquals(7, d4.getMonth()); assertEquals(2004, d4.getYYYY());}Rule 2: One Assert per TestThis rule says that every test function in a JUnit test should have one and only one assert statement. This rule may seem harsh, but the advantage can be seen in the below code snippet. Those tests come to a single conclusion that is quick and easy to understand.Let’s see one code example to understand this rule.Before:@Testpublic void should_return_page_hierarchy_as_json() throws Exception { makePages(\"PageOne\", \"PageOne.ChildOne\", \"PageTwo\"); submitRequest(\"root\", \"type:pages\"); assertResponseIsJson(); assertResponseContains( \"{ {\\\"page\\\": \\\"PageOne\\\"}, {\\\"page\\\": \\\"PageTwo\\\"}, {\\\"page\\\": \\\"ChildOne\\\"} }\" );}After:@Testpublic void should_return_page_hierarchy_as_json() throws Exception { givenPages(\"PageOne\", \"PageOne.ChildOne\", \"PageTwo\"); whenRequestIsIssued(\"root\", \"type:pages\"); thenResponseShouldBeJson();}@Testpublic void should_return_page_hierarchy_with_right_tags() throws Exception { givenPages(\"PageOne\", \"PageOne.ChildOne\", \"PageTwo\"); whenRequestIsIssued(\"root\", \"type:pages\"); thenResponseShouldContain( \"{ {\\\"page\\\": \\\"PageOne\\\"}, {\\\"page\\\": \\\"PageTwo\\\"}, {\\\"page\\\": \\\"ChildOne\\\"} }\" );}Do we have any real examples?Now, let’s see another example that is taken from the production code. Below is the code snippet of the unit test.@Testvoid test_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); assertEquals (Status.SUCCESS, wrapperResponse.getStatus()); assertEquals (\"ID0001\", wrapperResponse.getLosLeadId()); assertEquals (Status.SUCCESS.getCode(), wrapperResponse.getCode()); assertNull (wrapperResponse.getFailureType()); var clientCreationResponse = response.getClientCreation(); assertEquals (Status.SKIPPED, clientCreationResponse.getStatus()); assertEquals (\"10001\", clientCreationResponse.getCustomerId()); assertEquals (\"LOS0001\", clientCreationResponse.getUniqueRecordId()); assertEquals (Status.SKIPPED.getCode(),clientCreationResponse.getCode()); assertNull (clientCreationResponse.getFailureType()); var loanAccountCreationResponse =response.getLoanAccountCreation(); assertEquals (Status.SKIPPED, loanAccountCreationResponse.getStatus()); assertEquals (\"10001\", loanAccountreationResponse.getLoanAccountId()); assertEquals (\"1234\", loanAccountCreationResponse.getUniqueRecordId()); assertEquals (Status.SKIPPED.getCode(), loanAccountCreationResponse.getCode()); assertNul1 (loanAccountCreationResponse.getFailureType()); var loanContractCreationResponse = response.getLoanContractCreation(); assertEquals (Status.SUCCESS, loanContractCreationResponse.getStatus()); assertEquals (\"10001\", loanContractCreationResponse.getLoanContractId()); assertEquals (\"XXX123GYU\", loanContractCreationResponse.getUniqueRecordId()); assertEquals (Status.SUCCESS.getCode(),loanContractCreationResponse.getCode()); assertNull (loanContractCreationResponse.getFailureType());}As you can see, the above test code is very complex and has too many assertions. This test is hard to understand. As a matter of fact, the code that this test is written for is also very complex. As we know test is a documentation for the project/service, so the above test will confuse more developers instead of helping when things go bad. Let’s apply the earlier 2 rules discussed in the above test code....@Testvoid test_wrapper_status_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert assertEquals (Status.SUCCESS, wrapperResponse.getStatus()); assertEquals (Status.SUCCESS.getCode(), wrapperResponse.getCode()); assertEquals (\"ID0001\", wrapperResponse.getLosLeadId()); assertNull (wrapperResponse.getFailureType()); } @Test void test_client_creation_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var clientCreationResponse = response.getClientCreation() ; assertEquals (Status.SKIPPED, clientCreationResponse.getStatus()); assertEquals (Status.SKIPPED.getCode(),clientCreationResponse.getCode()); assertEquals (\"10001\", clientCreationResponse.getCustomerId()); assertEquals (\"LOS0001\", clientCreationResponse.getUniqueRecordId()); assertNull (clientCreationResponse.getFailureType());}@Testvoid test_loan_account_creation_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var loanAccountCreationResponse =response.getLoanAccountCreation(); assertEquals (Status.SKIPPED, loanAccountCreationResponse.getStatus()); assertEquals (Status.SKIPPED.getCode(), loanAccountCreationResponse.getCode()); assertEquals (\"10001\", loanAccountreationResponse.getLoanAccountId()); assertEquals (\"1234\", loanAccountCreationResponse.getUniqueRecordId()); assertNul1 (loanAccountCreationResponse.getFailureType());}@Testvoid test_loan_contract_Creation_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var loanContractCreationResponse = response.getLoanContractCreation(); assertEquals (Status.SUCCESS, loanContractCreationResponse.getStatus()); assertEquals (Status.SUCCESS.getCode(),loanContractCreationResponse.getCode()); assertEquals (\"10001\", loanContractCreationResponse.getLoanContractId()); assertEquals (\"XXX123GYU\", loanContractCreationResponse.getUniqueRecordId()); assertNull (loanContractCreationResponse.getFailureType());}...The above-refactored code looks pretty nice compared to the earlier test, but still, there are many asserts. A test should have at max 5 assertions per function but as a best practice, it should have a single assertion per test. if assertions count &gt; 5, it means you are doing something different and it is complex.Following a single assertion per test rule, we can further decompose our test. Let’s take test_something_for_client_creation test in the last step and further break it down into smaller tests.@Testvoid test_client_creation_that_has_skipped_status_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var clientCreationResponse = response.getClientCreation() ; assertEquals (Status.SKIPPED, clientCreationResponse.getStatus());}@Testvoid test_client_creation_that_has_skipped_status_code_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var clientCreationResponse = response.getClientCreation() ; assertEquals (Status.SKIPPED.getCode(),clientCreationResponse.getCode());}@Testvoid test_client_creation_that_has_customerId_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var clientCreationResponse = response.getClientCreation() ; assertEquals (\"10001\", clientCreationResponse.getCustomerId());}@Testvoid test_client_creation_that_has_uniqueRecordId_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var clientCreationResponse = response.getClientCreation() ; assertEquals (\"LOS0001\", clientCreationResponse.getUniqueRecordId());}@Testvoid test_client_creation_that_has_success_failure_type_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var clientCreationResponse = response.getClientCreation() ; assertNull (clientCreationResponse.getFailureType());}... // adding similarly for other testRule 3: Tests should not depend on each otherAs a general rule, each test function should contain all the code and resources that it requires to test the piece of code. A test function is a mini-universe that contains all things it needs to test the piece of code. For a particular class under test, there will be many mini-universe. A failure in one test shouldn’t affect the other test directly or indirectly.Example: In the below code snippet, we are using a static resource called Security context. It is used to handle user authentication and extract user details from the token. If we see carefully, we are creating a static mock of the SecurityContext.class and using the object to mock the method response.@Testvoid test_something() { ... var securityContext = Mockito.mockStatic(SecurityContext.class);\tsecurityContext.when(SecurityContext::getUserId).thenReturn(CUSTOMER_ID); ... securityContext.close();}If the above test fails and fails to release the securityContext static mock object, It will cause failure in other tests. And debugging will be tough if we are not aware of this problem. If you are not following proper development practices, there is a good chance that your code will be having similar issues. One way to solve the above problem is to use try-with-resources statement to handle this failure. Refer to the code snippet for the above code refactored to use try-with-resources statement.@Testvoid test_something() {\ttry (var securityContext = Mockito.mockStatic(SecurityContext.class)) {\tsecurityContext.when(SecurityContext::getUserId).thenReturn(CUSTOMER_ID);\t...\t}}The common reason for occurring this issue: Not following correct testing practices. Not releasing the static mock resource after completion of the test. Not handling commonly shared resources in testing.ConclusionBased on my observation, many developers generally don’t follow proper development practices. They either write production code before test code or don’t give importance to testing and put less effort into writing such tests. I have witnessed such a project, where the developer after a few years of development of the feature, hesitates to add new modifications with the excuse that code is complex, and he doesn’t have the context to add changes to that feature. It is a disaster for the project.It would be a lot better if proper development practices were followed in the project to minimize such an impact. Practices such as Pair Programming, Mob Review, Test Driven Development, Code Review, and Code Documentation are some practices to ensure high-quality code and less time to deploy new changes in production. You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Do not trust others' code", "url": "/posts/do-not-trust-others-code/", "categories": "Blogging, Article", "tags": "design, backenddevelopment", "date": "2022-10-31 20:00:00 +0530", "snippet": "In this article, I shared one instance of poor API implementation with numerous bugs and my experience with it.Background ContextAs per the current business process, If someone wants a loan against security, they have to physically visit one of the branches and wait in a long queue to start the process for the loan.The person who handles that request manually checks documents and later creates customers in the system once a loan is approved. Now, business wants to go digital and automate this manual process. Once the loan is approved digitally, the customer gets created in the same backend system (Legacy). This legacy system is core for most of the process and is difficult to change in a few years. So now you understand what problem we are going to discuss here.Client creation API is used to add a new customer once the loan against security is approved. This API already exists. In the orchestrated loan application journey, I have to consume that API once the loan is approved.The ProblemI am not able to create new clients in the legacy system. I read the documentation and reviewed the sample request numerous times but failed to create a new client via API.Let’s begin,Part 1: Status code misuse and misleading error messageMy initial feeling by looking at the API documentation was scary as it has more than 130 fields for the request in an excel sheet. Whenever I send a request for client creation, I get the below error,Status code: 500 INTERNAL SERVER ERROR{ \"Message\": \"An error has occurred.\"}After looking at the above response, I thought it did authentication successfully, and when it hits the service, something has gone wrong with the request, and because of that it is giving the above error.But, API requests have more than 130 fields in sample requests, and It is unclear from the response what is the actual problem. Even after trying different approaches , I failed to create a client, so I contacted a person from the integration team who successfully added a new client to the system, but that person used UI to create the client. Gauche, this is a pain. It’s like hitting the wall.One and half days have been wasted mailing, waiting, calling, reading documentation, and trying different approaches. I still have no clue why I am getting this error.Based on a hunch, I thought to check the request header and found the issue. In the request header, username and password field were added in the wrong way. After resolving, I was able to get a different error but yeah some progress. Instead of 401, they used 500 (INTERNAL SERVER ERROR ) for authentication failure. Is it the ignorance or laziness of the developer? This issue took me nearly two days. And, it is not the end.Part 2: API is badly implemented and poorly documentedNow, I can see different errors and resolve most of the issues by referring to the documentation and the database(I got access via UI to check).After resolving most of the validation errors, I got the below error.{ … \"status\": { \"Status\": \"Fail\", … \"Remarks\": \"Exception: Column ‘customerid’ does not belong to table Table.\" }}I may be sending customerId as an extra field, so I checked the payload again, but to my surprise, there is no field like customerId. As per the documentation, I do not see any problem with the request, so what is wrong here? 😫Hold on, why is the error saying the customer id column in the table is not present? Something strange is about the implementation of the API. It is revealing something which should not be. I wonder what database query is used to insert new client details.Since I can view the database, based on some trial and error with different combinations of values, I realized that the branch code (varchar) field with integer value works, but for string, it fails with the above error. But why is it even accepting that value if it should be an integer? why is it not part of the validation?It is not the only problem with this field. When I used values from the branch code column(Integer) already in the database, as per my understanding, it should work, but it is failing with the above error again. I do not understand this behavior of the API implementation.After some trial and error, I used the value from branchID instead of the branch code column in the database. It worked, but to my surprise, the documentation says branch code, but it is working with branch ID. I lost my trust in the API implementation.Part 3: I somehow managed to make the API work, but the default value, as per the documentation, is breaking the API call.It cost me approx. 2.5 days for making the first successful API call. It gave me mixed feelings of enjoyment and sadness. Next, I thought of playing with this API with some different values, which I will send to this API once I start the story implementation. I had some default values to use for my use case and thought to send that to the API and check whether I can successfully create a new client or not.The response I got looks like below,{ … \"status\": { \"Status\": \"Fail\", … \"Remarks\": \"Exception: Error converting data type varchar to numeric.\" }}From the above error, I thought maybe I edited the wrong field, but no, all field values look fine. So what could have gone wrong? The sad part about this error message is I do not know which among the multiple edited fields (10 fields I edited) has the issue. I had to undo the changes and try to send a new call after each field change. What a great API design and developer experience!, I was laughing with my pair.Later I found the problem I was giving groupName as NA (before groupName was Diamond), ok it is the name, so I should be receiving an invalid name or something like that, but why is the API internally trying to convert that to the numeric value for NA?Seriously, I have no idea what to say. Internally I was crying and thinking about running away.groupName field in the request now reverted to the previous value. Let’s see if I can send the request. Yes, I can create a new client in the system again. This time I felt true happiness in my last four days.Part 4: frugally removed unnecessary/optional fields from the request payload but now a weird validation error.Since API requests have numerous optional fields, I tried to remove which are unnecessary, as per my use case. I removed most of the fields except for one field(pool account). If I remove it and send the request, here is the response,{ … \"status\": { \"Status\": \"Fail\", … \"Remarks\": \" TaxStatus is required\" }}Maybe I accidentally deleted the tax status field, so I checked the payload, but it is present. Why am I getting the above error? So I tried to add back the pool account and sent the same request again, and it worked. What is happening here? Is this another problem in the API? Big Yes.The documentation says It is an optional field and can be blank. So I tried to remove it like other optional fields. I understand that sometimes we developers miss updating the documentation, which is ok, but why is it giving me the wrong error message? It should have said pool account required and not misleading by saying tax status.Part 5: Finally, I can send the request as per my use case, but my happiness did not last for more than 10 mins.I got the feedback to update the data via UI after creating a new client in the system as branch folks will be using that same system to update the data. It is fair, and I thought it would be a piece of cake as I only have to update one field in the UI and try to save the data as branch folks do.As I clicked the save button, I got numerous validation error messages. Yikes, what is going on here?…I was hoping the API would save the data in the system with all the proper validation. Is it too much to expect?Jokes apart, I manually had to resolve all those validation errors in the UI by adding the missing values and later adding the same in the API.During fixing those validation issues, I encountered one mandatory field called department, which is required but was missing. So I checked API documentation for where to pass in the request, but I did not find anything about it. So I checked sample requests but no luck. Only one question I had at that time, Why did the system even allow the saving of the new client data? If validation was missing, shouldn’t the API be responsible for validation? Well, it is easy to criticize others’ code. But maybe there could be some unknown reason for which this happened. What could you have done differently if you were in that developer’s place?What could be improved? Use proper documentation tool: The documentation which I got is nothing but excel sheets and some sample request payload in the text file. Seriously, we have a lot of good tools like swagger UI, Postman, Redoc, etc.; but still, why are we ignoring them and using word files, excel files, confluence, etc.? I think this decision is made by the tech team, not by business, so I feel this is the ignorance of one who is leading the team. Use proper testing: Everyone knows about unit testing, integration testing, and end-to-end testing, but sometimes we do not write a valid test. We get the feeling of 80%+ test coverage but does that even matter if your code cannot do one thing properly? In my case, if testing was fairly done, I might not have spent three days sending one successful request to your API. Validate content: If you are making any code changes, ensure to validate the documentation content. In my case, it was an excel sheet with 130+ fields for request. Give meaningful error response: It is better to focus on the error handling part of the API. Sadly, This is something I have observed developers give less importance to. If validation fails, return a proper reason for failure instead of one generic error. I do not have access to your code, and you should not expect me to find time to read your code before accessing your API. Do perform other business operations on data: In my case, client creation API by name suggests I should create and save in a database. But as a developer, I also need to ensure that other business operations on the stored data are working. For here, after saving, when I tried to update the data via UI, which branch folk will do, I got many validation errors that got missed in the API implementation. When bypassing the manual process, try to validate other business flows on data. ConclusionClient creation API is core in journey automation. From five short stories about the same API, you could have realized the pain of using a poorly implemented API. I have experienced a lot of pain with poorly implemented APIs and poor documentation, but this experience is different from others and unique.Want to read more? Presentation: How not to document your API? Blog: What makes API documentation terrible?" }, { "title": "How (Not) To Document Your APIs", "url": "/posts/how-not-to-document-your-apis/", "categories": "Presentation, Documentation", "tags": "design, backenddevelopment, softwareengineering", "date": "2022-10-23 00:00:00 +0530", "snippet": "How (Not) To document Your APIsIn this presentation, I talked about the troubles I faced due to poor API design. It happened due to less emphasis on documenting changes. Developers who do not have access to the internal code of the API will only have to rely on provided document.I shared four instances where poor documentation caused more pain than helping me to make a sound judgment about the effort required to integrate that API into my change.Image source: https://giphy.comVideo Deck " }, { "title": "State in reactive system - Orchestration-based Saga Pattern", "url": "/posts/saga-pattern-state-in-reactive-system/", "categories": "Presentation, system-design", "tags": "design, backenddevelopment, softwareengineering", "date": "2022-08-31 00:00:00 +0530", "snippet": "State in Reactive SystemIn this presentation, I talked about something we are already familiar. For some, it will be a refreshing session; for others, it will be new learning.We see how our design evolves as we progress with the presentation. We initially start with a simple application, later identify problems and see how to fix those problems.Video Deck Further readingsBlogs Modern day service orchestration using DSLs Service per team Microservices The Reactive Manifesto Saga PatternVideo: Spring I/O 2022: Be proactive about state in a reactive system by Nele Lea UhlemannBook: Practical Process Automation: Orchestration and Integration in Microservices and Cloud Native Architectures by Bernd Ruecker" }, { "title": "What makes API documentation terrible?", "url": "/posts/what-makes-api-documentation-terrible/", "categories": "Blogging, Article", "tags": "design, backenddevelopment, softwareengineering", "date": "2022-08-13 00:00:00 +0530", "snippet": "Is this another article on How to write documentation?No, this article is not about How to write API documentation but about what things everyone should avoid while writing API documentation. This documentation is meant to be used by other developers, so it is better to avoid the below problems that I tried to articulate.What do we mean by API in simple terms? An interface through which clients interact with the services.Why am I asking about API? There is a reason I brought up this topic. I have read a lot of API documentation internal and external(vendor API) to the projects, and the majority of them are poorly maintained. Sometimes, it is hard to understand what data to send for the working of the API. Why don’t developers give importance to writing API documentation? Why is it still hard to understand the API even after reading the documentation provided by the team who developed that API? Let me share the problems I faced while working with other APIs. This problem is something I have experienced in many projects internally and with vendor-provided solutions.Instance 1: Account Details APIThis API is used to get account details based on customer ID (Required) and account id (Optional). API says the customer ID field is mandatory, but if not provided still works. 😑 If only an account ID was provided, which is optional, it can still return details based on the optional field. 😑 Secondly, it also returns other resources which have matching account IDs but different customer IDs. It was not documented for the API. Even if it returns details for multiple resources, there is no way to identify which resource belongs to which customer ID as there is no field ID in returned resource details. Seriously, Which to use? How am I going to identify it? It makes no sense to me. 😔 Instance 2: Suspension Details APILegacy API is used to get the suspension details for customer id. API is named GET BASE_URL/suspension-details, but it returns account details. Due to bad naming, it created more confusion during migration. It should have been named GET \"BASE_URL/account-details. Why did this thing get missed? 😕 Instance 3: Money Transfer APIAn API used to send money from an internal account to an external account (customer account) API has some fields marked as mandatory in the documentation, but it can accepts empty/null values. It doesn’t make sense. What is the point of mandatory fields? Details for debtor and creditor accounts have all the fields marked as optional. In reality, as per the business, that API needs mandatory details, but technical implementation says different things. I had to make a few calls with the client team and check the usage in other products to understand the API. In the name of adding encryption features in the API, many fields are renamed, new fields were added, and some were removed. I don’t understand why this is not communicated to the other team/business, as this leads to the wrong information communicated with developers. Later, wrong estimations are given by developers.This is a serious issue. Instance 4: Product Rate APIAn API used to update the product rate. This rates get updated multiple times in a day. The data model name used for serializing the request payload looks like the TOCard, but what does TO mean here? No documentation was available, and When I checked the Jira story, there were also TO terms used. So, what does “TO” mean? I was clueless. In business terms, there was not any TO-related jargon used. The team that built that API is no longer available. During migration, new developers picked it up, but they had no full context. Going directly to the product owner could be a good approach, but I wanted to know why that developer thought TOCard name is sufficient to understand the payload data. Once data is updated, it returns the response which has a message like, Successfully updated TOCard details Later I realized after talking with a few old developers that “TO” in TOCard may mean Treasury Officer, as there is one such department. But, it is again a guess by that developer. I do not understand what that developer achieved by saving a few characters in the model name. It is a useless name as it is not directly helping me understand the API. Instance 5: Event ProducerInterface does not have to be REST API. It could be a Pub-Sub type interface, where the requests in the form of a message are consumed, and a response is sent in the form of a message/event. Here, Incomplete implementation of producer pushed in production that wasn’t consumed by any product. So, it did not affect any product. What is the meaning of that code? Why was that code not reverted? It gave me the understanding that since it is in prod, It must be working. After I integrated my changes to consume that message, my code won't get it. After debugging the framework, I found that the header in the message had one mandatory field missing. After fixing it worked. Seriously, for someone new to the framework, this is a nightmare as he could not know framework level understanding in a short time.Better to implement it correctly or avoid writing it in the first place. Instance 6: 2 Factor Authentication (2FA) APIAn API is used to send OTP messages to the customers, and another API is used to validate OTP messages. Validating OTP requires a sent OTP pin and function ID(used to identify message format specific to the product).The ProblemOTP Validation API suddenly stopped working for the product I was working on, which caused smoke failure in the lower environment. It blocked us from pushing any change to the higher environment like QA.BeforeThe default value used for function ID in OTP generation and validation. This change was working fine. The front-end pulls the function ID from Database(DB) based on product code.NowAfter eight months, the frontend code had two different values for the product code (for example, FOREX and FOREX_CARD). The frontend code was quite complex to check which config was used while making the OTP generation and validation call, as two different configs were present with different product codes. Later checking the DB, I found that the product code got updated from FOREX to FOREX_CARD a few days ago. In the OTP validation API, I found that if the function ID is missing in the request body, it takes the default value. Next, talking with the integration team, I learned that the function ID has to be the same in OTP generation and validation calls for a product code which was a different value in the API call. Later after 3-4 hr debugging, I realized that the frontend was using an older name (FOREX), not the new name(FOREX_CARD) to get the function ID. By looking at the frontend code, it was difficult for me to understand those things, but with the help of frontend folks, I found the config. Questions in my mind, Why were there two different names for product code in the frontend code? Why is older code not removed? Anyway, front-end code was refactored and removed code related to older names. Afterward, It started working. It is the code and unclear understanding of the API usage due to poor documentation that caused the issue. The function ID has to be the same in OTP generation, and validation calls for a product code This was not documented in tools like Jira, confluence, etc. Later talking with the integration team, I came to know.Conclusion I will not blame the team, but the practice that was followed by the developer who implemented that API in the product should have mentioned necessary details in the story so that others could quickly come to know. It is better to have a clear understanding of API before making any changes to the product. Small changes could go unnoticed, and no one will come to know the actual cause of failure unless the one who implemented that change is debugging the issue, which in my case was not available. Documentation is there to help other developers transition to your API smoothly. It is also better to review your documentation with other peers if you are not clear about what to include and add. This documentation saves a lot of hours and unnecessary meetings and calls for small things. It is better to add implicit details in the documentation not directly mentioned in the code but is required to use the API correctly. Avoid using meaningless small names or names like TOCard, which won’t help anyone understand your API. If those terms are business defined better to add a description explaining in 1-2 lines. Double check the details regarding what fields are optional and mandatory, as miss information could cause the story to spill over.   You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Code Smell Series: Shh…something is happening there", "url": "/posts/code-smell-series-ssh-something-is-happening/", "categories": "Blogging, CodeSmellSeries", "tags": "coding, smells", "date": "2022-07-02 00:00:00 +0530", "snippet": " …Shh…let me tell you…something is happening in that code.Last month, I encountered one problem in the code. That code act as a decision-maker to decide the next step in the application, based on the decision from the human(Team Dispatcher), which is an async task and takes time.The ProblemThe problem was that decision-maker implementation was complex and was doing many things. Things were happening behind every line of code, Which is difficult to understand even if you read that code multiple times.Let’s jump to the code,class DispatcherDecisionMaker( private val onApprove: Step, private val onReject: Step, private val onPendingSettlement: Step) { fun onExecute( dispatcherHumanTaskResponse: DispatcherHumanTaskResponse? = null, fetchPaymentResponse: FetchPaymentResponse? = null ): Step { return when (fetchPaymentResponse == null) { true -&gt; getDispatcherResponse(dispatcherHumanTaskResponse) false -&gt; when (fetchPaymentResponse.isPaymentSettled) { false -&gt; onPendingSettlement else -&gt; getDispatcherResponse(dispatcherHumanTaskResponse) } } } private fun getDispatcherResponse(dispatcherHumanTaskResponse: DispatcherHumanTaskResponse?) = when (dispatcherHumanTaskResponse == null) { true -&gt; onApprove false -&gt; when (dispatcherHumanTaskResponse.decision) { CaseDecision.APPROVED -&gt; onApprove else -&gt; onReject } }} Even today, After reading the above code and knowing the flow, I still need to check steps and other implementations to understand the code. I encourage you to once read the above code and then think about what you understood?UnderstandingIf you are unable to understand the above code, you are not alone. Let me brief you on what checks the above code does? Is payment settlement toggle/flow enabled? Was there any response from the fetch payment status API call? If there was any response, check for payment settlement status and decide whether to wait for settlement to complete or move on to dispatcher(Team) decision? Is dispatcher human task toggle/flow enabled? If it is not enabled, default to as approved. Otherwise, check the decision made by the Team Dispatcher and decide on the next step.I was surprised that too many things were happening and have been taken care of by the above piece of code. But reading the code again now, I still feel confused as it is implicit knowledge, and the other developer might not have that proper context, so he/she again needs to read the last step from the code itself.From the code, we see the class is handling two responsibilities Payment decision and Dispatcher decision. Also, there is naming smell problem in getDispatcherResponse(). getDispatcherResponse() method doesn’t return the dispatcher response but returns the next step to execute based on the human decision from Dispatcher Team.fetchPaymentResponse == null, this code check whether payment settlement flow/toggle is enabled or not. Seriously, I can’t interpret this by reading this class alone. That developer will have a hard time understanding his code after a few years.What is the use of else in line number 15?, Why not true? it makes more sense than writing else there.- else -&gt; getDispatcherResponse(dispatcherHumanTaskResponse)+ true -&gt; getDispatcherResponse(dispatcherHumanTaskResponse)dispatcherHumanTaskResponse == null, this code checks whether Dispatcher Human Task flow/toggle is enabled or not. Again, this is hard to understand from the code. I am scared of the day if that change got pushed into production, and after a few months, some strange bug appears. I can’t imagine that scenario for the developer who will be debugging it as everything is happening as an event.RefactoringNow, Let’s see refactored version of the above implementation.class DispatcherDecisionMaker( private val onApprove: Step, private val onReject: Step, private val onPendingSettlement: Step) { fun onExecute( dispatcherHumanTaskResponse: DispatcherHumanTaskResponse? = null, fetchPaymentResponse: FetchPaymentResponse? = null ): Step { return when (isPaymentSettlementEnabled(fetchPaymentResponse)) { false -&gt; nextStep(dispatcherHumanTaskResponse) true -&gt; when (isPaymentSettled(fetchPaymentResponse)) { false -&gt; onPendingSettlement true -&gt; nextStep(dispatcherHumanTaskResponse) } } } private fun isPaymentSettlementEnabled(fetchPaymentResponse: FetchPaymentResponse?): Boolean { return fetchPaymentResponse != null } private fun isPaymentSettled(fetchPaymentResponse: FetchPaymentResponse?): Boolean { return fetchPaymentResponse!!.isPaymentSettled } private fun nextStep(dispatcherHumanTaskResponse: DispatcherHumanTaskResponse?) = when (isDispatcherFlowDisabled(dispatcherHumanTaskResponse)) { true -&gt; onApprove false -&gt; when (dispatcherDecision(dispatcherHumanTaskResponse)) { CaseDecision.APPROVED -&gt; onApprove else -&gt; onReject } } private fun isDispatcherFlowDisabled(dispatcherHumanTaskResponse: DispatcherHumanTaskResponse?): Boolean { return dispatcherHumanTaskResponse == null } private fun dispatcherDecision(dispatcherHumanTaskResponse: DispatcherHumanTaskResponse?): CaseDecision { return dispatcherHumanTaskResponse!!.decision }}Further RefactoringAbove code is still doing multiple things. It can be further simplified into 2 separate class,Implementation of Dispatcher Decision Maker:class DispatcherDecisionMaker ( private val onApprove: Step, private val onReject: Step) { fun onExecute(dispatcherHumanTaskResponse: DispatcherHumanTaskResponse? = null): Step { return when (isDispatcherFlowDisabled (dispatcherHumanTaskResponse)) { true -&gt; onApprove false -&gt; when (dispatcherDecision (dispatcherHumanTaskResponse)) { CaseDecision.APPROVED -&gt; onApprove else -&gt; onReject } } } private fun isDispatcherFlowDisabled (dispatcherHumanTaskResponse: DispatcherHumanTaskResponse?): Boolean { return dispatcherHumanTaskResponse == null } private fun dispatcherDecision (dispatcherHumanTaskResponse: DispatcherHumanTaskResponse?): CaseDecision { return dispatcherHumanTaskResponse!!.decision }}Implementation of Payment Status Decision Maker:class PaymentStatusDecisionMaker( private val onPendingSettlementFlow: Step, private val onSuccess: Step, private val onSkip: Step){ fun onExecute( fetchPaymentResponse: FetchPaymentResponse? = null ): Step { return when (isPaymentSettlementEnabled (fetchPaymentResponse)) { true -&gt; when (isPaymentSettled(fetchPaymentResponse)) { true -&gt; onSuccess false -&gt; onPendingSettlementFlow } false -&gt; onSkip } } private fun isPaymentSettlementEnabled (fetchPaymentResponse: FetchPaymentResponse?): Boolean { return fetchPaymentResponse != null } private fun isPaymentSettled(fetchPaymentResponse: FetchPaymentResponse?): Boolean { return fetchPaymentResponse!!.isPaymentSettled }}It looks much better as each class is doing one thing. Even though it required to create another class but at the cost of clean responsibility and better readability, Which has more advantages than disadvantages. Also, it increases reusability.Conclusion:Initially, we saw that code was doing more things that can’t be easily interpreted by just reading it. To get the full context of the whole code, I needed to read the previous flow, and then I had to figure out what was happening, which was a clear signal of code smell. Be careful next time.  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Code Smell Series: Naming smells - Part 2", "url": "/posts/code-smell-series-file-name/", "categories": "Blogging, CodeSmellSeries", "tags": "coding, smells", "date": "2022-06-19 00:00:00 +0530", "snippet": " …Ahem…Why I still see non-sense naming?I recall encountering a few naming smells in the legacy system, during migration activity. There I saw three files with similar name in the production infrastructure code.ContextAs per new business requirement, I had to make some enhancements to the existing service to use different authorization tokens based on environments. Config/Code level changes were pretty straight forward.I encountered an actual problem when I wanted to deploy my changes to a non-dev environment (like release assurance and Prod). There, I needed to add some additional changes to Jenkins credentials, deployment pipeline code, and helm chart.The ProblemProd.yamlProd-new.yamlProd-new-new.yamlI saw the above three files for both release assurance and production environment. In that legacy system, no active development was happening. When I checked git logs for the latest changes, I failed to figure out which file I needed to change. Only one of those configurations is in use in the deployment pipeline.Which one is it?After checking the commit, I saw all three was updated same time. It is weird. Why does someone updated all three files when only one is in used?You may say, It could be to have a backup of the configuration. As git is been used for a long time, I don’t see any reason for the backup. If you want to revert to 1 month older configurations, you can easily do it.Ok, I already spent more than 20 mins, and I was thinking prod.yaml should be the one, but to double confirm, I checked with the infra folks for what file I have to update, and there I came to know I have to update prod-new-new.yaml.What? Why this weird naming format? I don’t understand this naming convention. This naming is adding confusion instead of bringing clarity.I asked the same question, but they were saying, Pravin that been there for a long time, and what can we do? I don’t understand How come that change at that time got approved.  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Code Smell Series: Naming smells - Part 1", "url": "/posts/code-smell-series-variable-name/", "categories": "Blogging, CodeSmellSeries", "tags": "coding, smells", "date": "2022-06-15 00:00:00 +0530", "snippet": " Are you sure your code is free from naming smell?You may think about ignoring it and adding to backlog due to week smell, but if that tech debt is not fixed and delayed, it will create more headache then peace of mind while working. It is the responsibility of developers to make sure that the code they are writing should not include naming Jargon, whether it is the application name, API endpoint, module, class, method, variables, etc. Until now, I have encountered this problem many times, but majority of them were small, had less impact and were fixed within a week. I recall 2 different instances which troubled me more.RecentlyI encountered naming smell, while performing application migration from one platform (in-house old service platform) to another platform (in-house new advanced service platform). There was one API that was exposed to the back-office team for pushing the latest data multiple times a day. The API accepts a list of currency exchange details that is further processed (aggregation, validation, transformation, and more) and then stored in the DB to be used by other applications.Problem 1:Here, naming of a few fields in currency exchange was not clear, something like, fxdCrncyCode and varCrncyCode.Someone who is reading this name for the first time will not come to know what it could mean. Is it some business term? We could guess Crncy could roughly mean currency given we are accepting currency exchange detail. But, what could fxd and var mean here?I tried asking few developers who joined project before me, but they don’t know as that service was last time updated a year ago, and folks who developed is no longer working in the project. So next, I thought to check the API contract testing code where I was expecting to find the test for that field and get actual meaning behind that, but sadly, I didn’t get anything there.At last, I had to read the complete API implementation to understand what was happening?, Where data was saved?, and there I saw it. Fxd mean fixed and var mean Variable.Now you will say, Pravin You could have guessed, it does roughly means same variable name.But hold on, Why do I have to guess the name? Why not written clearly in the first place? What did they achieved by saving two char from fixed and five char from Variable? One thing is for sure I had to spend two days just searching, asking, reading the code, and also confirm the same in the downstream application. The sad part is, the developer who implemented is no longer in the project, and no developer on the team had the full context about it. You could have checked the API documentation and come to know everything about it.That’s true. But, I was not able to find the documentation page in confluence. You could have checked details from the source that pushes the data to the API. Yes, I could have, but that might have cost me another 2-3 days as the team who uses the API is not actively aware much about the fields.API integrated with them three years ago, so not everyone in that team had full context. Also, the team was busy with many tasks at that time. Given the tight deadline, that could be the last option for me.Now What?If that developer had written the proper name, which was a simple task, It might have saved my precious time.It is easy to criticise others code. What would you have done different in that developers place? Lets discuss the possible solution to above problem. Some more context: After some time, I realised that developer used the same name coming from the source system in Request Payload.You may say, from the source, we are getting data in that format, so what could be done? Are you sure there is no option other than using fxdCrncyCode and varCrncyCode that is unclear?I think that is just an excuse. Below are the few approaches that, You could follow to avoid above problem.Approach 1:Create DTO object which accept the source data and translate to domain specific name like,data class ExchangeRateDTO ( val fxdCrncyCode: String, val varCrncyCode: String ) { fun toDomain(): ExchangeRate { return ExchangeRate( fixedCurrencyCode = this.fxdCrncyCode, variableCurrencyCode = this.varCrncyCode ) }} But this creates an extra class to manage.Approach 2:Much simpler way, if you are using Jackson like library for deserializing the data back to object,Use @JsonProperty in the data class field name to map the name to different name.data class ExchangeRate( @JsonProperty(\"fxdCrncyCode\") val fixedCurrencyCode: String, @JsonProperty(\"varCrncyCode\") val VariableCurrencyCode: String) In the API contract, Jackson will see that field name and set that against the proper name.Approach 3:Ask the source to send the correct field name. That could have been a good approach if done initially. But in my case, that contract was already in use, and the team sending the data was lazy to pick this tech debt.Problem 2:This API had the wrong endpoint. It was like ../card-rate, but in reality, it was not accepting card rate but currency exchange rate. So, Why it was not fixed later?Due to these everywhere in the code base, wrong names was used, and people who read the code thought, in a business context, it could mean card rate.But, the business people are not referring that name. so it is another naming smell in the API enpoint. Naming smell - Part 2, is on second instance which troubled me.Link to a page You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Tips to test (end to end) library changes - Gradle based Project", "url": "/posts/tips-to-test-library-changes/", "categories": "Blogging, Article", "tags": "gradle, softwareengineering, backenddevelopment, testing", "date": "2022-06-03 11:40:00 +0530", "snippet": "The ProblemI had made multiple changes in 2 dependencies of the spring application. These changes are on the local machine, and I need to test the functionality of the change end to end with the dependent application before pushing the artifact to a remote artifact repository like Artifactory.I know this is a simple task, but the reason is that some people are still struggling with this thing(including me), and now even popular IDE like IntelliJ also support that. But, which approach to choose is stillpeople get confused.&nbsp;SetupHere, I am using Kotlin, Gradle, and IntelliJ development IDE. For the sake of simplicity, I am going touse a simple project, but the challenges are still the same.Let’s startI ask about above problem every time to myself, whenever I am working on a project that has some changes in one of the dependencies, and I know there are many ways to handle it. Let’s see some of them.&nbsp;Approach 1: Upload the changes to a remote artifact repository (Artifactory) and use that to testThis is straightforward. Here, we simply publish our change to remote Artifactory and update the version of the dependency in build.gradle file,For example,-implementation(\"dev.pravin:projectb:1.0.1\")+implementation(\"dev.pravin:projectb:1.0.2\")I do not suggest this approach. If you are unsure about your change, it is better to avoid this approach. You could follow either of the below approaches to test locally. Later go ahead and follow this approach. Mostly this is taken care of by CI build pipeline.Other things to know: Once the build is available in remote Artifactory, we have to update the library version in the build.gradle file and use that to test the changes. It takes a lot of time as now we have to update the build version push those changes to the version control system (GitHub, Bitbucket, etc.), create a new build with CI pipeline, and later push it to Artifactory. Update the build.gradle file in the dependent app to test it. As we add changes, we have to follow the same steps again. It is easy to accomplish. The artifact repository becomes messy as there is no way to tell stable and unstable build. It creates more confusion for other folks who will be using it.&nbsp;Approach 2: Add changes as external jar dependencyIt is another way to add a dependency to the project, but it requires a few extra steps to configure the dependency. If you update the dependency, you need to remove the older one, then add the latest one.These steps keep repeating for each change.steps to add external jarOther things to know: We have to uncomment the current dependency for the build.gradle file and import the jar as an external dependency. We must create a jar of the library, and later follow steps to manually remove the old and add the updated jar. This approach can be frustrating if you have to test changes multiple times for the same library. It is easy to recall those steps, and anyone can easily do it.&nbsp;Approach 3: publish changes to the local maven repository(~/.m2) and using that to test changes Let’s say, Project A dependents on Project B.Changes in Project B (Assuming Project B already has required changes): Open the build.gradle file and add the following changes: Increment project version We can locally use it to identify artifacts by incrementing the version (like from 1.0.1 to 1.0.2). Add maven-publish plugin plugins { ... id \"maven-publish\" } .. Add below config for publishing ... publishing { publications { maven(MavenPublication) { from components.java } } } Screenshot of build.gradle in project B open the terminal in the root folder of the project and run gradle publishToMavenLocal. This command, creates a new build of the project with the updated version, and publishes the jar artifact in the local repository of the maven. This repository is used by other projects to resolve the dependency. Screenshot of terminal in project B Changes in Project A: we have to make small changes in build.gradle file. check for repositories and add “mavenLocal()” as the first entry. Next, we must update the dependency version to the one we published in the previous step. - implementation(\"dev.pravin:projectb:1.0.1\") + implementation(\"dev.pravin:projectb:1.0.2\") Screenshot of build.gradle in project A Reload the build.gradle changes and run the project. This time Gradle will check the maven local repository (~/.m2) and use that to resolve dependencies. We should now be able to run the application with the latest changes. Screenshot of project A execution output Here is the complete step animation: complete steps for both project changes It is much simpler and avoids unnecessary artifact upload to a remote artifact repository for each change.Other things to know: It has some code changes like adding the maven Local plugin, updating the version, and config related to publishing to maven local in build.gradle of the application. It comparatively takes less time, and it will be simple as we make changes.We can reuse the same version while publishing to maven local. In the end, add code changes, publish to maven local with the same version, and reload build.gradle in the dependent application. There are small code changes, and it is easy to follow.&nbsp;Approach 4: Add the library as a Module dependency in the application and test itThis pattern is followed by most Gradle projects where we add projects (Project A, Project B) as a subproject in the multi-project setup. We have to create top-level settings.gradle which has details of both subprojects. In our problem, this method doesn’t provide much benefit as it requires more changes in gradle files to test the code changes. It feels like using a bazooka to kill a fly. For step-by-step details, follow this linkOther things to know: We have to make a few changes in the Gradle file (build.gradle and settings.gradle) to achieve this. It takes time to set up for beginners, and even for non-beginner, it still requires following some documentation (In short, it requires effort to remember steps). Understanding how this works takes some time. It is done as a one-time setup. Due to more steps involved if wrongly followed, it may cause other problems like dependency not being visible, and there could be more. It is not simple, and checking changes with more applications could be time-consuming.&nbsp;ConclusionAfter going through all the above approaches, it is clear that Approaches 1 and 4 are not a good choice. Both approaches 2 and 3 are better for this problem statement.If there are small changes that we need to test, we can go with Approach 2, and it is ok as that involves fewer gradle file changes compared to approach 3.If there are many changes that you need to test, it would be better to go with Approach 3. Even though approach 3 has few extra changes compared to approach 2, we know that there is a good possibility that we again have to make changes and redo the same process. So, approach 3 reduces the hassle of manual importing that approach 2 does.For this problem statement, approach 3 is the better option as I need to make further changes and update the code. You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Understanding Nodejs(V8) and Spring Webflux(Netty) — based on Event Loop design", "url": "/posts/understanding-nodejs-v8-and-spring-webflux-netty-based-on-event-loop-design/", "categories": "Blogging, Article", "tags": "nodejs, springwebflux, backenddevelopment, eventloop", "date": "2022-04-14 00:00:00 +0530", "snippet": "For the last five months, I have been using Spring Webflux in my project for building asynchronous reactive Apps. In the past, I have also worked on the Nodejs project.In the below article, I tried to explain Nodejs and Webflux based on Event Loop, which gave me good insight into internal components and request handling. Full article was published on Medium, link to Medium blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Approaches for Start and Resume Journey for User Onboarding to Platform", "url": "/posts/approaches-for-start-and-resume-journey-for-user-onboarding-to-platform-part-i/", "categories": "Blogging, Article", "tags": "design, softwareengineering, backenddevelopment, onboardingprocess", "date": "2022-03-20 00:00:00 +0530", "snippet": "User onboarding to platform is very important process before they access services offered by the platform. The most important part is code sharing which becomes problem at some point if right design, data structure is not used. It is not an easy process, and in the below article (2 Parts), I have shared few approaches which can be used to handle this complex process. Part 1: Full article was published on Medium, link to Medium blog article Part 2: Full article was published on Medium, link to Medium blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Challenges in Migration of Application to Another Platform", "url": "/posts/challenges-discovered-in-migration-of-application-to-another-platform/", "categories": "Blogging, Article", "tags": "backenddevelopment, softwareengineering, migration", "date": "2022-02-12 00:00:00 +0530", "snippet": "I am working on the migration of an application from one platform to another platform, both were developed in-house, and managed by internal teams. The purpose of migration is to decommission the current platform, switch to another platform that has better features available to use, and have good integration with external services.Here is my another article on migration. In this article, I have discussed some of the common challenges.Let me know your thoughts 😉 Full article was published on Medium, link to Medium blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "AWS API Gateway — Ways to handle Request Timeout", "url": "/posts/aws-api-gateway-ways-to-handle-request-timeout/", "categories": "Blogging, Article", "tags": "aws, softwarearchitecture, backenddevelopment", "date": "2021-10-25 00:00:00 +0530", "snippet": "AWS cloud services are improving with time, and I am enjoying using their services. We all know that using 3rd part services in our application adds additional constraints to our services. Compared to benefits, those restrictions may sometimes be manageable.The Problem:I have come across a problem where API request timeout is restricted due to vendor (AWS in my case) and can’t increase after max timeout value (30 sec). Daily multiple batches ran by different API consumers that experienced timeout for some requests amounting to 3–4% of the total request sent.I have discussed above issue in depth in below article 😉. Full article was published on Medium, link to Medium blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Understanding Legacy System Migration using Strangler Fig Pattern", "url": "/posts/understanding-legacy-system-migration-using-strangler/", "categories": "Blogging, Article", "tags": "softwaredevelopment, systemdesign, strategy", "date": "2021-09-10 00:00:00 +0530", "snippet": "Strangler Fig Pattern appears to be straight forward strategy for migration of Legacy System to Modern Application. But in reality, it is not that easy process, and there are many variations of the same pattern. In this article, Strangler Fig Pattern has been discussed and steps to migrate legacy systems with a low risk of failure Full article was published on LinkedIn, link to LinkedIn blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Things I learned from analysis for migration activity", "url": "/posts/things-i-learned-from-analysis-migration-activity/", "categories": "Blogging, Article", "tags": "design, softwaredevelopment, nodejs, backenddevelopment", "date": "2021-08-16 00:00:00 +0530", "snippet": "I have been performing some analysis for nodejs application migration for the past few months. This activity has helped me identify problems and challenges that we usually face due to the poor design of some of the legacy app/libraries. For detailed insight, refer to the below article. Full article was published on LinkedIn, link to LinkedIn blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "External Traffic handling in AWS Kubernetes Services", "url": "/posts/external-traffic-handling-aws-kubernetes-services/", "categories": "Blogging, Article", "tags": "awscloud, kubernetes, networkarchitecture, scalability, resiliency", "date": "2021-06-27 00:00:00 +0530", "snippet": "I have been wondering, How AWS handles the traffic from external services to services/pods running inside the Kubernetes cluster? 🤨 In this article, we discuss what happens behind the scene in AWS Kubernetes and other options that can help for scalability and resiliency. Full article was published on LinkedIn, link to LinkedIn blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Understanding Class Loading in Java Virtual Machine", "url": "/posts/understanding-class-loading-java-virtual-machine/", "categories": "Blogging, Article", "tags": "java, security, backenddevelopment", "date": "2021-05-24 00:00:00 +0530", "snippet": "I had a use case where I wanted to update the implementation of the algorithm based on the trigger without restarting the app. This app is written using java(spring) and it exposes API to be consumed. Here, whenever the configuration is changed in the repository, the app is notified to reload the code. Java allows us to use our implementation of the class loader and using this, I implemented the required functionality.This article is created to explain the class loader and how we can use it to create our custom implementation and perform hot deployment without any app downtime. Full article was published on LinkedIn, link to LinkedIn blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Understanding Java VM for Memory Management", "url": "/posts/understanding-java-vm-memory-management/", "categories": "Blogging, Article", "tags": "java, docker-compose, kafka, performance-tuning, jvm, backend", "date": "2021-05-15 00:00:00 +0530", "snippet": "I had created a demo application in my free time which uses pub/sub for sharing messages between 2 java microservice application using Kafka as middleware. Later on, I starter experimenting on effect of different garbage collection algorithm using some popular tools.After trying out different algorithm, I am surprised that same code could get faster if right garbage collection algorithm is configured for the application.I have created an article to document same thing. Full article was published on LinkedIn, link to LinkedIn blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" } ]
